{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT: Part 2\n",
    "\n",
    "## The Architecture - How BERT Actually Works\n",
    "\n",
    "---\n",
    "\n",
    "**Paper:** [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "---\n",
    "\n",
    "In Part 1, we learned why BERT was created. Now let's look at how it's built.\n",
    "\n",
    "If you followed the Transformer series, you already know most of this. BERT is just the **encoder** part of the Transformer. But there are some important details about input representation that we need to cover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Rectangle\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## BERT = Encoder-Only Transformer\n",
    "\n",
    "Remember the original Transformer had two parts:\n",
    "- **Encoder**: Processes input, can see all positions\n",
    "- **Decoder**: Generates output, can only see past positions\n",
    "\n",
    "BERT uses **only the encoder**. No decoder.\n",
    "\n",
    "Why? Because BERT is for **understanding**, not generation. It needs to see the full input to understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT vs Original Transformer\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 8))\n",
    "\n",
    "# Original Transformer\n",
    "ax = axes[0]\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "ax.set_title('Original Transformer\\n(Translation)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Encoder\n",
    "enc = FancyBboxPatch((1, 3), 3, 5, boxstyle=\"round,pad=0.1\",\n",
    "                      facecolor='#3498db', edgecolor='#2980b9', linewidth=2, alpha=0.8)\n",
    "ax.add_patch(enc)\n",
    "ax.text(2.5, 5.5, 'ENCODER', fontsize=11, ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "# Decoder\n",
    "dec = FancyBboxPatch((6, 3), 3, 5, boxstyle=\"round,pad=0.1\",\n",
    "                      facecolor='#e74c3c', edgecolor='#c0392b', linewidth=2, alpha=0.8)\n",
    "ax.add_patch(dec)\n",
    "ax.text(7.5, 5.5, 'DECODER', fontsize=11, ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "# Arrow\n",
    "ax.annotate('', xy=(6, 5.5), xytext=(4, 5.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='#333', lw=2))\n",
    "\n",
    "ax.text(2.5, 2.3, 'Input:\\n\"The cat sat\"', fontsize=9, ha='center')\n",
    "ax.text(7.5, 2.3, 'Output:\\n\"Le chat assis\"', fontsize=9, ha='center')\n",
    "\n",
    "# BERT\n",
    "ax = axes[1]\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "ax.set_title('BERT\\n(Understanding)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Just encoder\n",
    "enc = FancyBboxPatch((2.5, 3), 5, 5, boxstyle=\"round,pad=0.1\",\n",
    "                      facecolor='#3498db', edgecolor='#2980b9', linewidth=2, alpha=0.8)\n",
    "ax.add_patch(enc)\n",
    "ax.text(5, 5.5, 'ENCODER\\nONLY', fontsize=12, ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "ax.text(5, 2.3, 'Input:\\n\"The [MASK] sat\"', fontsize=9, ha='center')\n",
    "ax.text(5, 8.7, 'Output:\\nContextualized embeddings\\n(one per token)', fontsize=9, ha='center')\n",
    "\n",
    "# Strikethrough decoder\n",
    "ax.text(8, 5.5, 'No decoder!', fontsize=10, color='#e74c3c', fontweight='bold', rotation=-20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Two Model Sizes\n",
    "\n",
    "BERT comes in two sizes:\n",
    "\n",
    "| | BERT-Base | BERT-Large |\n",
    "|---|---|---|\n",
    "| Layers (L) | 12 | 24 |\n",
    "| Hidden size (H) | 768 | 1024 |\n",
    "| Attention heads (A) | 12 | 16 |\n",
    "| Total parameters | 110M | 340M |\n",
    "\n",
    "BERT-Base was designed to match GPT's size (for fair comparison).\n",
    "\n",
    "BERT-Large was made bigger to see if scale helps (it does).\n",
    "\n",
    "The architecture is the same - just different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model size comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "models = ['GPT-1', 'BERT-Base', 'BERT-Large']\n",
    "params = [117, 110, 340]\n",
    "colors = ['#95a5a6', '#3498db', '#e74c3c']\n",
    "\n",
    "bars = ax.bar(models, params, color=colors, edgecolor='white', linewidth=2)\n",
    "\n",
    "ax.set_ylabel('Parameters (Millions)', fontsize=11)\n",
    "ax.set_title('Model Size Comparison', fontsize=12, fontweight='bold')\n",
    "ax.set_ylim(0, 400)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, param in zip(bars, params):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n",
    "            f'{param}M', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.text(1, 50, 'Same size as GPT-1\\n(fair comparison)', fontsize=9, ha='center', \n",
    "        bbox=dict(boxstyle='round', facecolor='#fef9e7'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Input Representation: The Key Details\n",
    "\n",
    "This is where BERT differs from a standard Transformer encoder. BERT has specific input formatting.\n",
    "\n",
    "### The Input Format\n",
    "\n",
    "Every BERT input looks like this:\n",
    "\n",
    "```\n",
    "[CLS] tokens of sentence A [SEP] tokens of sentence B [SEP]\n",
    "```\n",
    "\n",
    "Or for single sentence:\n",
    "\n",
    "```\n",
    "[CLS] tokens of sentence [SEP]\n",
    "```\n",
    "\n",
    "Let's break this down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Token: [CLS]\n",
    "\n",
    "**[CLS]** stands for \"classification\".\n",
    "\n",
    "It's always the first token. The final hidden state of [CLS] is used as the \"sentence representation\" for classification tasks.\n",
    "\n",
    "Why? Because self-attention lets [CLS] gather information from all other tokens. After 12 (or 24) layers, [CLS] has \"seen\" the entire input.\n",
    "\n",
    "```\n",
    "Input:  [CLS] The movie was great [SEP]\n",
    "                  ↓ self-attention ↓\n",
    "Output: [CLS'] contains info about entire sentence\n",
    "                  ↓\n",
    "        Classify: POSITIVE\n",
    "```\n",
    "\n",
    "### Special Token: [SEP]\n",
    "\n",
    "**[SEP]** stands for \"separator\".\n",
    "\n",
    "It marks the end of a sentence. For tasks with two sentences (like question answering or entailment), it separates them:\n",
    "\n",
    "```\n",
    "[CLS] Do cats meow ? [SEP] Yes , cats make meowing sounds . [SEP]\n",
    "      ← Question →        ← Answer →\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input format visualization\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(7, 5.5, 'BERT Input Format (Two Sentences)', fontsize=13, ha='center', fontweight='bold')\n",
    "\n",
    "# Tokens\n",
    "tokens = ['[CLS]', 'The', 'cat', 'sat', '[SEP]', 'It', 'was', 'tired', '[SEP]']\n",
    "colors = ['#9b59b6', '#3498db', '#3498db', '#3498db', '#9b59b6', \n",
    "          '#e74c3c', '#e74c3c', '#e74c3c', '#9b59b6']\n",
    "\n",
    "for i, (token, color) in enumerate(zip(tokens, colors)):\n",
    "    x = 1 + i * 1.4\n",
    "    box = FancyBboxPatch((x-0.5, 3.5), 1.1, 0.8, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor=color, edgecolor='none', alpha=0.8)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x, 3.9, token, fontsize=10, ha='center', va='center', \n",
    "            color='white', fontweight='bold')\n",
    "\n",
    "# Segment labels\n",
    "ax.plot([0.5, 5.5], [2.8, 2.8], color='#3498db', lw=3)\n",
    "ax.text(3, 2.5, 'Segment A', fontsize=10, ha='center', color='#3498db', fontweight='bold')\n",
    "\n",
    "ax.plot([6.5, 12.5], [2.8, 2.8], color='#e74c3c', lw=3)\n",
    "ax.text(9.5, 2.5, 'Segment B', fontsize=10, ha='center', color='#e74c3c', fontweight='bold')\n",
    "\n",
    "# Legend\n",
    "ax.text(7, 1.5, '[CLS] = Classification token (used for sentence-level tasks)', fontsize=9, ha='center')\n",
    "ax.text(7, 1, '[SEP] = Separator (marks sentence boundaries)', fontsize=9, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Three Embeddings\n",
    "\n",
    "For each token, BERT combines three embeddings:\n",
    "\n",
    "1. **Token Embedding**: What word/subword is this?\n",
    "2. **Segment Embedding**: Is this sentence A or sentence B?\n",
    "3. **Position Embedding**: What position in the sequence?\n",
    "\n",
    "These three are added together to create the input representation.\n",
    "\n",
    "```\n",
    "Input = Token_Embedding + Segment_Embedding + Position_Embedding\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three embeddings visualization (BERT paper Figure 2 style)\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(7, 9.5, 'BERT Input Representation', fontsize=14, ha='center', fontweight='bold')\n",
    "\n",
    "# Tokens\n",
    "tokens = ['[CLS]', 'my', 'dog', 'is', 'cute', '[SEP]', 'he', 'likes', 'play', '##ing', '[SEP]']\n",
    "n = len(tokens)\n",
    "x_positions = np.linspace(1, 13, n)\n",
    "\n",
    "# Input tokens row\n",
    "ax.text(0.3, 7.5, 'Input', fontsize=10, ha='right', fontweight='bold')\n",
    "for i, (x, token) in enumerate(zip(x_positions, tokens)):\n",
    "    ax.text(x, 7.5, token, fontsize=9, ha='center', \n",
    "            bbox=dict(boxstyle='round,pad=0.2', facecolor='#ecf0f1', edgecolor='#bdc3c7'))\n",
    "\n",
    "# Token embeddings row\n",
    "ax.text(0.3, 6, 'Token\\nEmbed', fontsize=9, ha='right', fontweight='bold')\n",
    "for i, x in enumerate(x_positions):\n",
    "    color = '#9b59b6' if tokens[i] in ['[CLS]', '[SEP]'] else '#3498db'\n",
    "    box = FancyBboxPatch((x-0.45, 5.6), 0.9, 0.8, boxstyle=\"round,pad=0.02\",\n",
    "                          facecolor=color, edgecolor='none', alpha=0.7)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x, 6, f'E_{tokens[i][:3]}', fontsize=7, ha='center', va='center', color='white')\n",
    "\n",
    "# Segment embeddings row\n",
    "ax.text(0.3, 4.5, 'Segment\\nEmbed', fontsize=9, ha='right', fontweight='bold')\n",
    "segments = ['A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B']\n",
    "for i, (x, seg) in enumerate(zip(x_positions, segments)):\n",
    "    color = '#27ae60' if seg == 'A' else '#e74c3c'\n",
    "    box = FancyBboxPatch((x-0.45, 4.1), 0.9, 0.8, boxstyle=\"round,pad=0.02\",\n",
    "                          facecolor=color, edgecolor='none', alpha=0.7)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x, 4.5, f'E_{seg}', fontsize=8, ha='center', va='center', color='white')\n",
    "\n",
    "# Position embeddings row\n",
    "ax.text(0.3, 3, 'Position\\nEmbed', fontsize=9, ha='right', fontweight='bold')\n",
    "for i, x in enumerate(x_positions):\n",
    "    box = FancyBboxPatch((x-0.45, 2.6), 0.9, 0.8, boxstyle=\"round,pad=0.02\",\n",
    "                          facecolor='#f39c12', edgecolor='none', alpha=0.7)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x, 3, f'E_{i}', fontsize=8, ha='center', va='center', color='white')\n",
    "\n",
    "# Plus signs\n",
    "for y in [5.2, 3.7]:\n",
    "    for x in x_positions:\n",
    "        ax.text(x, y, '+', fontsize=12, ha='center', va='center', fontweight='bold')\n",
    "\n",
    "# Equals\n",
    "ax.text(0.3, 1.5, 'Input\\nRepresent.', fontsize=9, ha='right', fontweight='bold')\n",
    "for i, x in enumerate(x_positions):\n",
    "    box = FancyBboxPatch((x-0.45, 1.1), 0.9, 0.8, boxstyle=\"round,pad=0.02\",\n",
    "                          facecolor='#2c3e50', edgecolor='none', alpha=0.9)\n",
    "    ax.add_patch(box)\n",
    "\n",
    "ax.text(7, 2.2, '=', fontsize=20, ha='center', va='center', fontweight='bold')\n",
    "\n",
    "# Legend\n",
    "ax.text(7, 0.3, 'Final input = Token Embedding + Segment Embedding + Position Embedding', \n",
    "        fontsize=10, ha='center', style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Position Embeddings\n",
    "\n",
    "Unlike the original Transformer (which used sinusoidal functions), BERT uses **learned** position embeddings.\n",
    "\n",
    "- Maximum sequence length: 512 tokens\n",
    "- Position embedding matrix: 512 × 768 (for BERT-Base)\n",
    "\n",
    "The authors found no significant difference between learned and sinusoidal positions, so they just used learned ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## WordPiece Tokenization\n",
    "\n",
    "BERT doesn't use word-level tokenization. It uses **WordPiece** - a subword tokenization method.\n",
    "\n",
    "### Why Subwords?\n",
    "\n",
    "Word-level tokenization has problems:\n",
    "- Vocabulary gets huge (every word needs an entry)\n",
    "- Rare words are poorly represented\n",
    "- Out-of-vocabulary words can't be handled\n",
    "\n",
    "Subword tokenization splits rare words into smaller pieces:\n",
    "\n",
    "```\n",
    "\"playing\"    → [\"play\", \"##ing\"]\n",
    "\"unhappiness\" → [\"un\", \"##hap\", \"##pi\", \"##ness\"]\n",
    "\"TensorFlow\" → [\"Ten\", \"##sor\", \"##Fl\", \"##ow\"]\n",
    "```\n",
    "\n",
    "The `##` prefix means \"this continues the previous token\".\n",
    "\n",
    "Common words stay whole. Rare words get split into recognizable pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordPiece tokenization examples\n",
    "print(\"WordPiece Tokenization Examples\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "examples = [\n",
    "    (\"The cat sat\", [\"The\", \"cat\", \"sat\"]),\n",
    "    (\"playing\", [\"play\", \"##ing\"]),\n",
    "    (\"unhappiness\", [\"un\", \"##hap\", \"##pi\", \"##ness\"]),\n",
    "    (\"embeddings\", [\"em\", \"##bed\", \"##ding\", \"##s\"]),\n",
    "    (\"TensorFlow\", [\"Tensor\", \"##Fl\", \"##ow\"]),\n",
    "    (\"transformer\", [\"transform\", \"##er\"]),\n",
    "]\n",
    "\n",
    "for text, tokens in examples:\n",
    "    print(f\"\\n'{text}'\")\n",
    "    print(f\"  → {tokens}\")\n",
    "    print(f\"  ({len(tokens)} tokens)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 6))\n",
    "\n",
    "# Word-level\n",
    "ax = axes[0]\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 2)\n",
    "ax.axis('off')\n",
    "ax.set_title('Word-Level Tokenization', fontsize=11, fontweight='bold', loc='left')\n",
    "\n",
    "words = ['I', 'love', 'playing', 'basketball', 'unhappily']\n",
    "for i, word in enumerate(words):\n",
    "    ax.text(1 + i*2.2, 1, word, fontsize=11, ha='center',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='#e74c3c', edgecolor='none', alpha=0.7),\n",
    "            color='white')\n",
    "\n",
    "ax.text(11.5, 1, '5 tokens', fontsize=10, ha='center', color='#666')\n",
    "ax.text(11.5, 0.4, 'Problem: \"unhappily\"\\nmight be OOV', fontsize=8, ha='center', color='#e74c3c')\n",
    "\n",
    "# WordPiece\n",
    "ax = axes[1]\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 2)\n",
    "ax.axis('off')\n",
    "ax.set_title('WordPiece Tokenization', fontsize=11, fontweight='bold', loc='left')\n",
    "\n",
    "pieces = ['I', 'love', 'play', '##ing', 'basket', '##ball', 'un', '##hap', '##pi', '##ly']\n",
    "positions = [0.5, 1.5, 2.5, 3.3, 4.3, 5.2, 6.2, 7, 7.8, 8.6]\n",
    "for pos, piece in zip(positions, pieces):\n",
    "    color = '#27ae60' if not piece.startswith('##') else '#3498db'\n",
    "    ax.text(pos + 0.5, 1, piece, fontsize=9, ha='center',\n",
    "            bbox=dict(boxstyle='round,pad=0.2', facecolor=color, edgecolor='none', alpha=0.7),\n",
    "            color='white')\n",
    "\n",
    "ax.text(11.5, 1, '10 tokens', fontsize=10, ha='center', color='#666')\n",
    "ax.text(11.5, 0.4, 'All pieces are\\nin vocabulary!', fontsize=8, ha='center', color='#27ae60')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT's Vocabulary\n",
    "\n",
    "- Size: ~30,000 tokens\n",
    "- Includes whole words, subwords, and characters\n",
    "- Special tokens: [CLS], [SEP], [MASK], [PAD], [UNK]\n",
    "\n",
    "With 30K subwords, BERT can represent virtually any text - even words it's never seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Architecture Diagram\n",
    "\n",
    "Let's draw the complete BERT architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete BERT architecture\n",
    "fig, ax = plt.subplots(figsize=(12, 14))\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 14)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(6, 13.5, 'BERT Architecture', fontsize=16, ha='center', fontweight='bold')\n",
    "\n",
    "# Input tokens\n",
    "tokens = ['[CLS]', 'The', 'cat', 'sat', '[SEP]']\n",
    "x_positions = [2, 4, 6, 8, 10]\n",
    "\n",
    "for x, token in zip(x_positions, tokens):\n",
    "    ax.text(x, 1, token, fontsize=10, ha='center',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='#ecf0f1', edgecolor='#bdc3c7'))\n",
    "\n",
    "ax.text(6, 0.3, 'Input Tokens', fontsize=10, ha='center', color='#666')\n",
    "\n",
    "# Embedding layer\n",
    "embed_box = FancyBboxPatch((1, 1.8), 10, 1.2, boxstyle=\"round,pad=0.05\",\n",
    "                            facecolor='#f39c12', edgecolor='#e67e22', linewidth=2, alpha=0.8)\n",
    "ax.add_patch(embed_box)\n",
    "ax.text(6, 2.4, 'Token + Segment + Position Embeddings', fontsize=10, \n",
    "        ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "# Arrows from input to embedding\n",
    "for x in x_positions:\n",
    "    ax.annotate('', xy=(x, 1.8), xytext=(x, 1.4),\n",
    "                arrowprops=dict(arrowstyle='->', color='#333', lw=1))\n",
    "\n",
    "# Transformer layers\n",
    "layer_colors = plt.cm.Blues(np.linspace(0.3, 0.8, 6))\n",
    "for i in range(6):  # Show 6 layers (representing 12)\n",
    "    y = 3.5 + i * 1.4\n",
    "    layer_box = FancyBboxPatch((1, y), 10, 1.2, boxstyle=\"round,pad=0.05\",\n",
    "                                facecolor=layer_colors[i], edgecolor='#2980b9', linewidth=1)\n",
    "    ax.add_patch(layer_box)\n",
    "    \n",
    "    if i == 2:\n",
    "        ax.text(6, y + 0.6, 'Transformer Encoder Layer\\n(Self-Attention + FFN)', \n",
    "                fontsize=9, ha='center', va='center', color='white', fontweight='bold')\n",
    "    if i == 5:\n",
    "        ax.text(6, y + 0.6, '× 12 (Base) or × 24 (Large)', \n",
    "                fontsize=9, ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "# Output representations\n",
    "output_y = 12\n",
    "for i, x in enumerate(x_positions):\n",
    "    box = FancyBboxPatch((x-0.5, output_y), 1, 0.8, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor='#27ae60', edgecolor='none', alpha=0.8)\n",
    "    ax.add_patch(box)\n",
    "    label = 'C' if i == 0 else f'T{i}'\n",
    "    ax.text(x, output_y + 0.4, label, fontsize=10, ha='center', va='center', \n",
    "            color='white', fontweight='bold')\n",
    "\n",
    "ax.text(6, 13, 'Output: Contextualized Representations', fontsize=10, ha='center', color='#666')\n",
    "\n",
    "# Labels\n",
    "ax.text(0.5, output_y + 0.4, 'C = [CLS]\\nrepresentation', fontsize=8, ha='center', color='#27ae60')\n",
    "\n",
    "# Arrows\n",
    "for x in x_positions:\n",
    "    ax.annotate('', xy=(x, 12), xytext=(x, 11.7),\n",
    "                arrowprops=dict(arrowstyle='->', color='#333', lw=1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What Each Layer Does\n",
    "\n",
    "Each Transformer encoder layer (from Part 3 of our Transformer series) contains:\n",
    "\n",
    "1. **Multi-Head Self-Attention**\n",
    "   - Each token attends to all other tokens\n",
    "   - 12 attention heads (BERT-Base) or 16 heads (BERT-Large)\n",
    "   - Captures different types of relationships\n",
    "\n",
    "2. **Feed-Forward Network**\n",
    "   - Two linear layers with GELU activation\n",
    "   - Hidden size: 3072 (4× the hidden dimension)\n",
    "   - Applied to each position independently\n",
    "\n",
    "3. **Residual Connections + Layer Normalization**\n",
    "   - After each sub-layer\n",
    "   - Helps with training deep networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer detail\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(5, 9.5, 'Inside One Transformer Encoder Layer', fontsize=13, ha='center', fontweight='bold')\n",
    "\n",
    "# Input\n",
    "ax.text(5, 8.8, 'Input from previous layer', fontsize=9, ha='center', color='#666')\n",
    "ax.annotate('', xy=(5, 8.2), xytext=(5, 8.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='#333', lw=1.5))\n",
    "\n",
    "# Multi-head attention\n",
    "attn_box = FancyBboxPatch((2, 6.5), 6, 1.5, boxstyle=\"round,pad=0.05\",\n",
    "                           facecolor='#e74c3c', edgecolor='#c0392b', linewidth=2, alpha=0.8)\n",
    "ax.add_patch(attn_box)\n",
    "ax.text(5, 7.25, 'Multi-Head Self-Attention', fontsize=11, ha='center', va='center',\n",
    "        color='white', fontweight='bold')\n",
    "\n",
    "# Add & Norm 1\n",
    "an1_box = FancyBboxPatch((2, 5), 6, 0.8, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor='#f39c12', edgecolor='none', alpha=0.8)\n",
    "ax.add_patch(an1_box)\n",
    "ax.text(5, 5.4, 'Add & Layer Norm', fontsize=10, ha='center', va='center',\n",
    "        color='white', fontweight='bold')\n",
    "\n",
    "# Feed-forward\n",
    "ff_box = FancyBboxPatch((2, 3), 6, 1.5, boxstyle=\"round,pad=0.05\",\n",
    "                         facecolor='#9b59b6', edgecolor='#8e44ad', linewidth=2, alpha=0.8)\n",
    "ax.add_patch(ff_box)\n",
    "ax.text(5, 3.75, 'Feed-Forward Network\\n(768 → 3072 → 768)', fontsize=10, ha='center', va='center',\n",
    "        color='white', fontweight='bold')\n",
    "\n",
    "# Add & Norm 2\n",
    "an2_box = FancyBboxPatch((2, 1.5), 6, 0.8, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor='#f39c12', edgecolor='none', alpha=0.8)\n",
    "ax.add_patch(an2_box)\n",
    "ax.text(5, 1.9, 'Add & Layer Norm', fontsize=10, ha='center', va='center',\n",
    "        color='white', fontweight='bold')\n",
    "\n",
    "# Residual connections\n",
    "ax.annotate('', xy=(1.5, 5.4), xytext=(1.5, 8),\n",
    "            arrowprops=dict(arrowstyle='-', color='#3498db', lw=2))\n",
    "ax.annotate('', xy=(2, 5.4), xytext=(1.5, 5.4),\n",
    "            arrowprops=dict(arrowstyle='->', color='#3498db', lw=2))\n",
    "\n",
    "ax.annotate('', xy=(1.5, 1.9), xytext=(1.5, 4.5),\n",
    "            arrowprops=dict(arrowstyle='-', color='#3498db', lw=2))\n",
    "ax.annotate('', xy=(2, 1.9), xytext=(1.5, 1.9),\n",
    "            arrowprops=dict(arrowstyle='->', color='#3498db', lw=2))\n",
    "\n",
    "# Output\n",
    "ax.annotate('', xy=(5, 0.8), xytext=(5, 1.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='#333', lw=1.5))\n",
    "ax.text(5, 0.5, 'Output to next layer', fontsize=9, ha='center', color='#666')\n",
    "\n",
    "# Vertical arrows\n",
    "ax.annotate('', xy=(5, 8), xytext=(5, 8.2),\n",
    "            arrowprops=dict(arrowstyle='->', color='#333', lw=1.5))\n",
    "ax.annotate('', xy=(5, 6.5), xytext=(5, 5.8),\n",
    "            arrowprops=dict(arrowstyle='->', color='#333', lw=1.5))\n",
    "ax.annotate('', xy=(5, 5), xytext=(5, 4.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='#333', lw=1.5))\n",
    "ax.annotate('', xy=(5, 3), xytext=(5, 2.3),\n",
    "            arrowprops=dict(arrowstyle='->', color='#333', lw=1.5))\n",
    "\n",
    "ax.text(0.8, 6.5, 'Residual\\nconnection', fontsize=8, ha='center', color='#3498db')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## GELU Activation\n",
    "\n",
    "BERT uses **GELU** (Gaussian Error Linear Unit) instead of ReLU in the feed-forward layers.\n",
    "\n",
    "$$GELU(x) = x \\cdot \\Phi(x)$$\n",
    "\n",
    "Where Φ(x) is the cumulative distribution function of the standard normal distribution.\n",
    "\n",
    "In practice, it's approximated as:\n",
    "\n",
    "$$GELU(x) \\approx 0.5x(1 + \\tanh(\\sqrt{2/\\pi}(x + 0.044715x^3)))$$\n",
    "\n",
    "GELU is smoother than ReLU and has been shown to work better for Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GELU vs ReLU\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "x = np.linspace(-4, 4, 200)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(x, relu(x), label='ReLU', lw=2, color='#e74c3c')\n",
    "ax.plot(x, gelu(x), label='GELU', lw=2, color='#3498db')\n",
    "ax.axhline(y=0, color='#333', lw=0.5)\n",
    "ax.axvline(x=0, color='#333', lw=0.5)\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('f(x)', fontsize=11)\n",
    "ax.set_title('GELU vs ReLU Activation Functions', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(-1, 4)\n",
    "\n",
    "ax.annotate('GELU is smooth\\n(no sharp corner)', xy=(-0.5, gelu(-0.5)), \n",
    "            xytext=(-2.5, 1.5), fontsize=9,\n",
    "            arrowprops=dict(arrowstyle='->', color='#3498db'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: BERT Architecture\n",
    "\n",
    "| Component | BERT-Base | BERT-Large |\n",
    "|-----------|-----------|------------|\n",
    "| Layers | 12 | 24 |\n",
    "| Hidden size | 768 | 1024 |\n",
    "| Attention heads | 12 | 16 |\n",
    "| Feed-forward size | 3072 | 4096 |\n",
    "| Max sequence length | 512 | 512 |\n",
    "| Vocabulary size | ~30,000 | ~30,000 |\n",
    "| Total parameters | 110M | 340M |\n",
    "\n",
    "### Input Format\n",
    "- [CLS] + tokens + [SEP] (+ more tokens + [SEP] for pairs)\n",
    "- WordPiece tokenization\n",
    "- Three embeddings summed: token + segment + position\n",
    "\n",
    "### Architecture\n",
    "- Encoder-only Transformer\n",
    "- GELU activation in feed-forward\n",
    "- Learned position embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What's Next: Part 3\n",
    "\n",
    "Now you know BERT's architecture. In Part 3, we'll cover:\n",
    "\n",
    "- **Pre-training in detail**: Masked LM and Next Sentence Prediction\n",
    "- **The masking strategy**: Why 15%? Why not always [MASK]?\n",
    "- **Training data and compute**\n",
    "\n",
    "---\n",
    "\n",
    "*Paper:* [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
