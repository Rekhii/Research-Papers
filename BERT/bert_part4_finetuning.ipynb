{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT: Part 4\n",
    "\n",
    "## Fine-tuning - Adapting BERT to Your Task\n",
    "\n",
    "---\n",
    "\n",
    "**Paper:** [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "---\n",
    "\n",
    "Pre-training gives us a model that \"understands\" language. Fine-tuning teaches it to do something useful with that understanding.\n",
    "\n",
    "The beauty of BERT: fine-tuning is simple. Same architecture, just add a small output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Fine-tuning Recipe\n",
    "\n",
    "1. **Start** with pre-trained BERT weights\n",
    "2. **Add** a task-specific output layer (usually just one linear layer)\n",
    "3. **Train** on your labeled data for a few epochs\n",
    "4. **Done**\n",
    "\n",
    "That's it. The whole BERT model gets updated during fine-tuning, but because it starts from good weights, you don't need much data or time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning overview\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(7, 7.5, 'Fine-tuning: Add Output Layer + Train', fontsize=14, ha='center', fontweight='bold')\n",
    "\n",
    "# Pre-trained BERT\n",
    "bert_box = FancyBboxPatch((3, 2.5), 8, 3.5, boxstyle=\"round,pad=0.1\",\n",
    "                           facecolor='#3498db', edgecolor='#2980b9', linewidth=3, alpha=0.8)\n",
    "ax.add_patch(bert_box)\n",
    "ax.text(7, 4.25, 'Pre-trained BERT\\n(110M or 340M parameters)\\n\\nAlready knows language!', \n",
    "        fontsize=11, ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "# Task-specific head\n",
    "head_box = FancyBboxPatch((5, 6.2), 4, 0.8, boxstyle=\"round,pad=0.05\",\n",
    "                           facecolor='#e74c3c', edgecolor='none', alpha=0.9)\n",
    "ax.add_patch(head_box)\n",
    "ax.text(7, 6.6, 'Task Head (new)', fontsize=10, ha='center', va='center', \n",
    "        color='white', fontweight='bold')\n",
    "\n",
    "# Arrow\n",
    "ax.annotate('', xy=(7, 6.2), xytext=(7, 6),\n",
    "            arrowprops=dict(arrowstyle='->', color='#333', lw=2))\n",
    "\n",
    "# Input\n",
    "ax.text(7, 1.8, 'Your task data', fontsize=10, ha='center')\n",
    "ax.annotate('', xy=(7, 2.5), xytext=(7, 2.1),\n",
    "            arrowprops=dict(arrowstyle='->', color='#333', lw=2))\n",
    "\n",
    "# Details on the side\n",
    "details = [\n",
    "    'Fine-tuning:',\n",
    "    '• Learning rate: 2e-5 to 5e-5',\n",
    "    '• Epochs: 2-4',\n",
    "    '• Batch size: 16-32',\n",
    "    '• Time: minutes to hours',\n",
    "]\n",
    "for i, line in enumerate(details):\n",
    "    weight = 'bold' if i == 0 else 'normal'\n",
    "    ax.text(0.5, 5 - i*0.5, line, fontsize=9, fontweight=weight)\n",
    "\n",
    "ax.text(7, 0.8, 'Everything gets updated - both BERT and the new head', \n",
    "        fontsize=10, ha='center', style='italic', color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task Type 1: Sentence Classification\n",
    "\n",
    "**Examples:** Sentiment analysis, spam detection, topic classification\n",
    "\n",
    "**How it works:**\n",
    "1. Feed sentence through BERT\n",
    "2. Take the [CLS] hidden state\n",
    "3. Pass through a linear layer → class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence classification\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(7, 7.5, 'Sentence Classification (e.g., Sentiment)', fontsize=13, ha='center', fontweight='bold')\n",
    "\n",
    "# Input\n",
    "tokens = ['[CLS]', 'This', 'movie', 'was', 'great', '!', '[SEP]']\n",
    "for i, token in enumerate(tokens):\n",
    "    color = '#9b59b6' if token in ['[CLS]', '[SEP]'] else '#3498db'\n",
    "    ax.text(2 + i*1.5, 6, token, fontsize=10, ha='center',\n",
    "            bbox=dict(boxstyle='round,pad=0.2', facecolor=color, edgecolor='none', alpha=0.7),\n",
    "            color='white')\n",
    "\n",
    "# BERT\n",
    "bert_box = FancyBboxPatch((1, 3.5), 11, 1.5, boxstyle=\"round,pad=0.05\",\n",
    "                           facecolor='#2c3e50', edgecolor='none', alpha=0.9)\n",
    "ax.add_patch(bert_box)\n",
    "ax.text(6.5, 4.25, 'BERT', fontsize=12, ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "# Arrows down\n",
    "for i in range(7):\n",
    "    ax.annotate('', xy=(2 + i*1.5, 5), xytext=(2 + i*1.5, 5.6),\n",
    "                arrowprops=dict(arrowstyle='->', color='#333', lw=1))\n",
    "\n",
    "# Only [CLS] output used\n",
    "ax.annotate('', xy=(2, 2.8), xytext=(2, 3.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='#9b59b6', lw=3))\n",
    "\n",
    "# Classifier\n",
    "clf_box = FancyBboxPatch((0.5, 1.8), 3, 0.8, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor='#e74c3c', edgecolor='none', alpha=0.9)\n",
    "ax.add_patch(clf_box)\n",
    "ax.text(2, 2.2, 'Linear (768→2)', fontsize=10, ha='center', va='center',\n",
    "        color='white', fontweight='bold')\n",
    "\n",
    "# Output\n",
    "ax.annotate('', xy=(4.5, 2.2), xytext=(3.5, 2.2),\n",
    "            arrowprops=dict(arrowstyle='->', color='#333', lw=2))\n",
    "\n",
    "ax.text(6, 2.5, 'Positive: 0.94', fontsize=11, ha='center', color='#27ae60', fontweight='bold')\n",
    "ax.text(6, 1.8, 'Negative: 0.06', fontsize=11, ha='center', color='#e74c3c')\n",
    "\n",
    "# Note\n",
    "ax.text(9, 3, 'Other token outputs\\nare ignored', fontsize=9, ha='center', color='#999')\n",
    "\n",
    "ax.text(7, 0.8, '[CLS] token aggregates sentence meaning through self-attention', \n",
    "        fontsize=10, ha='center', style='italic', color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task Type 2: Sentence Pair Classification\n",
    "\n",
    "**Examples:** Natural language inference (NLI), paraphrase detection, textual similarity\n",
    "\n",
    "**How it works:**\n",
    "1. Concatenate both sentences with [SEP] between them\n",
    "2. Feed through BERT\n",
    "3. Take [CLS] → linear layer → class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence pair classification (NLI example)\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(7, 7.5, 'Sentence Pair Classification (NLI)', fontsize=13, ha='center', fontweight='bold')\n",
    "\n",
    "# Premise and hypothesis\n",
    "ax.text(3.5, 6.5, 'Premise: \"A man is playing guitar\"', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round,pad=0.3', facecolor='#3498db', edgecolor='none', alpha=0.6))\n",
    "ax.text(10.5, 6.5, 'Hypothesis: \"Music is being played\"', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round,pad=0.3', facecolor='#27ae60', edgecolor='none', alpha=0.6))\n",
    "\n",
    "# Combined input\n",
    "ax.text(7, 5.5, '[CLS] A man is playing guitar [SEP] Music is being played [SEP]', \n",
    "        fontsize=9, ha='center',\n",
    "        bbox=dict(boxstyle='round,pad=0.3', facecolor='#ecf0f1', edgecolor='#bdc3c7'))\n",
    "\n",
    "# BERT\n",
    "bert_box = FancyBboxPatch((2, 3), 10, 1.5, boxstyle=\"round,pad=0.05\",\n",
    "                           facecolor='#2c3e50', edgecolor='none', alpha=0.9)\n",
    "ax.add_patch(bert_box)\n",
    "ax.text(7, 3.75, 'BERT', fontsize=12, ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "ax.annotate('', xy=(7, 4.5), xytext=(7, 5.1),\n",
    "            arrowprops=dict(arrowstyle='->', color='#333', lw=2))\n",
    "\n",
    "# [CLS] output\n",
    "ax.annotate('', xy=(3, 2.2), xytext=(3, 3),\n",
    "            arrowprops=dict(arrowstyle='->', color='#9b59b6', lw=2))\n",
    "\n",
    "# Classifier\n",
    "clf_box = FancyBboxPatch((1.5, 1.3), 3, 0.8, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor='#e74c3c', edgecolor='none', alpha=0.9)\n",
    "ax.add_patch(clf_box)\n",
    "ax.text(3, 1.7, 'Linear (768→3)', fontsize=10, ha='center', va='center',\n",
    "        color='white', fontweight='bold')\n",
    "\n",
    "# Output classes\n",
    "ax.annotate('', xy=(5.5, 1.7), xytext=(4.5, 1.7),\n",
    "            arrowprops=dict(arrowstyle='->', color='#333', lw=2))\n",
    "\n",
    "ax.text(8, 2.1, 'Entailment: 0.89', fontsize=10, ha='left', color='#27ae60', fontweight='bold')\n",
    "ax.text(8, 1.6, 'Neutral: 0.08', fontsize=10, ha='left', color='#f39c12')\n",
    "ax.text(8, 1.1, 'Contradiction: 0.03', fontsize=10, ha='left', color='#e74c3c')\n",
    "\n",
    "ax.text(7, 0.4, 'NLI: Does hypothesis follow from premise?', fontsize=10, ha='center', style='italic', color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task Type 3: Token Classification (NER, POS Tagging)\n",
    "\n",
    "**Examples:** Named Entity Recognition, Part-of-Speech tagging\n",
    "\n",
    "**How it works:**\n",
    "1. Feed sentence through BERT\n",
    "2. Take hidden state for **each** token\n",
    "3. Each token → linear layer → tag\n",
    "\n",
    "Unlike classification, we use ALL token outputs, not just [CLS]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token classification (NER)\n",
    "fig, ax = plt.subplots(figsize=(14, 9))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 9)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(7, 8.5, 'Token Classification (Named Entity Recognition)', fontsize=13, ha='center', fontweight='bold')\n",
    "\n",
    "# Input tokens\n",
    "tokens = ['[CLS]', 'John', 'works', 'at', 'Google', 'in', 'NYC', '[SEP]']\n",
    "labels = ['', 'B-PER', 'O', 'O', 'B-ORG', 'O', 'B-LOC', '']\n",
    "x_positions = [1, 2.5, 4, 5.5, 7, 8.5, 10, 11.5]\n",
    "\n",
    "for x, token in zip(x_positions, tokens):\n",
    "    color = '#9b59b6' if token in ['[CLS]', '[SEP]'] else '#3498db'\n",
    "    ax.text(x, 7, token, fontsize=10, ha='center',\n",
    "            bbox=dict(boxstyle='round,pad=0.2', facecolor=color, edgecolor='none', alpha=0.7),\n",
    "            color='white')\n",
    "\n",
    "# BERT\n",
    "bert_box = FancyBboxPatch((0.5, 4.5), 12, 1.5, boxstyle=\"round,pad=0.05\",\n",
    "                           facecolor='#2c3e50', edgecolor='none', alpha=0.9)\n",
    "ax.add_patch(bert_box)\n",
    "ax.text(6.5, 5.25, 'BERT', fontsize=12, ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "# Arrows down to BERT\n",
    "for x in x_positions:\n",
    "    ax.annotate('', xy=(x, 6), xytext=(x, 6.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='#333', lw=1))\n",
    "\n",
    "# Hidden states\n",
    "for x in x_positions:\n",
    "    ax.annotate('', xy=(x, 3.7), xytext=(x, 4.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='#333', lw=1))\n",
    "\n",
    "# Linear layers (one per token, but same weights)\n",
    "for x in x_positions[1:-1]:  # Skip [CLS] and [SEP]\n",
    "    clf_box = FancyBboxPatch((x-0.5, 3), 1, 0.6, boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor='#e74c3c', edgecolor='none', alpha=0.8)\n",
    "    ax.add_patch(clf_box)\n",
    "\n",
    "ax.text(6.5, 3.3, 'Linear (768 → num_tags)', fontsize=9, ha='center', color='white', fontweight='bold')\n",
    "\n",
    "# Output labels\n",
    "colors_map = {'B-PER': '#e74c3c', 'B-ORG': '#3498db', 'B-LOC': '#27ae60', 'O': '#95a5a6'}\n",
    "for x, label in zip(x_positions[1:-1], labels[1:-1]):\n",
    "    color = colors_map.get(label, '#95a5a6')\n",
    "    ax.text(x, 2, label, fontsize=10, ha='center', fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.2', facecolor=color, edgecolor='none', alpha=0.8),\n",
    "            color='white')\n",
    "    ax.annotate('', xy=(x, 2.4), xytext=(x, 3),\n",
    "                arrowprops=dict(arrowstyle='->', color='#333', lw=1))\n",
    "\n",
    "# Legend\n",
    "ax.text(7, 1, 'B-PER = Person, B-ORG = Organization, B-LOC = Location, O = Other', \n",
    "        fontsize=9, ha='center', color='#666')\n",
    "\n",
    "ax.text(7, 0.4, 'Same linear layer applied to each token (shared weights)', \n",
    "        fontsize=9, ha='center', style='italic', color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task Type 4: Question Answering (Extractive)\n",
    "\n",
    "**Examples:** SQuAD, reading comprehension\n",
    "\n",
    "**How it works:**\n",
    "1. Input: [CLS] question [SEP] context [SEP]\n",
    "2. Predict **start** and **end** positions of answer in context\n",
    "3. Two linear layers: one for start, one for end\n",
    "\n",
    "This is extractive QA - the answer is a span from the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question answering\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(7, 9.5, 'Extractive Question Answering (SQuAD)', fontsize=13, ha='center', fontweight='bold')\n",
    "\n",
    "# Question and context\n",
    "ax.text(7, 8.5, 'Question: \"Where is the Eiffel Tower?\"', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round,pad=0.3', facecolor='#3498db', edgecolor='none', alpha=0.6),\n",
    "        color='white')\n",
    "ax.text(7, 7.7, 'Context: \"The Eiffel Tower is located in Paris, France.\"', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round,pad=0.3', facecolor='#27ae60', edgecolor='none', alpha=0.6),\n",
    "        color='white')\n",
    "\n",
    "# Input format\n",
    "ax.text(7, 6.7, '[CLS] Where is the Eiffel Tower ? [SEP] The Eiffel Tower is located in Paris , France . [SEP]', \n",
    "        fontsize=8, ha='center',\n",
    "        bbox=dict(boxstyle='round,pad=0.2', facecolor='#ecf0f1', edgecolor='#bdc3c7'))\n",
    "\n",
    "# BERT\n",
    "bert_box = FancyBboxPatch((1, 4), 12, 1.8, boxstyle=\"round,pad=0.05\",\n",
    "                           facecolor='#2c3e50', edgecolor='none', alpha=0.9)\n",
    "ax.add_patch(bert_box)\n",
    "ax.text(7, 4.9, 'BERT', fontsize=12, ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "ax.annotate('', xy=(7, 5.8), xytext=(7, 6.3),\n",
    "            arrowprops=dict(arrowstyle='->', color='#333', lw=2))\n",
    "\n",
    "# Token outputs\n",
    "context_tokens = ['The', 'Eiffel', 'Tower', 'is', 'located', 'in', 'Paris', ',', 'France', '.']\n",
    "x_positions = np.linspace(2, 12, len(context_tokens))\n",
    "\n",
    "for i, (x, token) in enumerate(zip(x_positions, context_tokens)):\n",
    "    # Highlight answer span\n",
    "    if token in ['Paris', ',', 'France']:\n",
    "        color = '#f39c12'\n",
    "    else:\n",
    "        color = '#95a5a6'\n",
    "    ax.text(x, 3.3, token, fontsize=9, ha='center',\n",
    "            bbox=dict(boxstyle='round,pad=0.15', facecolor=color, edgecolor='none', alpha=0.7))\n",
    "\n",
    "# Start and end predictions\n",
    "start_box = FancyBboxPatch((1.5, 1.5), 4, 1, boxstyle=\"round,pad=0.05\",\n",
    "                            facecolor='#e74c3c', edgecolor='none', alpha=0.8)\n",
    "ax.add_patch(start_box)\n",
    "ax.text(3.5, 2, 'Start prediction\\nPosition 6 (\"Paris\")', fontsize=9, ha='center', va='center',\n",
    "        color='white', fontweight='bold')\n",
    "\n",
    "end_box = FancyBboxPatch((8.5, 1.5), 4, 1, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor='#9b59b6', edgecolor='none', alpha=0.8)\n",
    "ax.add_patch(end_box)\n",
    "ax.text(10.5, 2, 'End prediction\\nPosition 8 (\"France\")', fontsize=9, ha='center', va='center',\n",
    "        color='white', fontweight='bold')\n",
    "\n",
    "# Arrows\n",
    "ax.annotate('', xy=(3.5, 2.5), xytext=(x_positions[6], 3),\n",
    "            arrowprops=dict(arrowstyle='->', color='#e74c3c', lw=2))\n",
    "ax.annotate('', xy=(10.5, 2.5), xytext=(x_positions[8], 3),\n",
    "            arrowprops=dict(arrowstyle='->', color='#9b59b6', lw=2))\n",
    "\n",
    "# Answer\n",
    "ax.text(7, 0.7, 'Answer: \"Paris, France\" (span from start to end)', fontsize=11, ha='center',\n",
    "        fontweight='bold', bbox=dict(boxstyle='round', facecolor='#f39c12', edgecolor='none'),\n",
    "        color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fine-tuning Hyperparameters\n",
    "\n",
    "The paper recommends:\n",
    "\n",
    "| Hyperparameter | Recommended Values |\n",
    "|----------------|--------------------|\n",
    "| Batch size | 16, 32 |\n",
    "| Learning rate | 2e-5, 3e-5, 5e-5 |\n",
    "| Epochs | 2, 3, 4 |\n",
    "\n",
    "### Why Such Small Learning Rates?\n",
    "\n",
    "BERT is already well-trained. We want to **adapt** it, not **destroy** what it learned.\n",
    "\n",
    "Large learning rates would overwrite the pre-trained knowledge. Small learning rates make small adjustments.\n",
    "\n",
    "### Why So Few Epochs?\n",
    "\n",
    "With pre-trained weights, the model converges quickly. Too many epochs leads to overfitting on small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning code example\n",
    "print(\"Fine-tuning Example (PyTorch-style pseudocode)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "code = '''\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "# Load pre-trained BERT with classification head\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2  # Binary classification\n",
    ")\n",
    "\n",
    "# Optimizer with small learning rate\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(3):  # Just 3 epochs!\n",
    "    for batch in train_dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            labels=batch['labels']\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "'''\n",
    "\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Results: Fine-tuning Performance\n",
    "\n",
    "The paper showed fine-tuning works incredibly well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "tasks = ['MNLI\\n(NLI)', 'SST-2\\n(Sentiment)', 'CoLA\\n(Grammar)', 'SQuAD\\n(QA)']\n",
    "bert_scores = [86.7, 94.9, 60.5, 90.9]\n",
    "previous_sota = [80.6, 93.2, 35.0, 84.1]\n",
    "\n",
    "x = np.arange(len(tasks))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, previous_sota, width, label='Previous SOTA', color='#bdc3c7')\n",
    "bars2 = ax.bar(x + width/2, bert_scores, width, label='BERT-Large', color='#e74c3c')\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=11)\n",
    "ax.set_title('Fine-tuning Results: BERT vs Previous State-of-the-Art', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tasks, fontsize=10)\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add improvement annotations\n",
    "improvements = ['+6.1', '+1.7', '+25.5', '+6.8']\n",
    "for i, (imp, b1, b2) in enumerate(zip(improvements, previous_sota, bert_scores)):\n",
    "    ax.annotate(imp, xy=(i + width/2, b2 + 2), ha='center', fontsize=9, \n",
    "                fontweight='bold', color='#27ae60')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improvements are massive:\n",
    "- **CoLA**: +25.5 points (grammaticality judgment)\n",
    "- **SQuAD**: +6.8 points (question answering)\n",
    "- **MNLI**: +6.1 points (natural language inference)\n",
    "\n",
    "All from the same pre-trained model, just different output heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Fine-tuning Patterns\n",
    "\n",
    "| Task Type | Output Used | Output Layer |\n",
    "|-----------|-------------|-------------|\n",
    "| Sentence classification | [CLS] | Linear → num_classes |\n",
    "| Sentence pair classification | [CLS] | Linear → num_classes |\n",
    "| Token classification | All tokens | Linear → num_tags |\n",
    "| Question answering | Context tokens | Linear → 2 (start/end) |\n",
    "\n",
    "The pattern is always the same:\n",
    "1. Feed input through BERT\n",
    "2. Take relevant hidden states\n",
    "3. Simple linear layer(s)\n",
    "4. Train for a few epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What's Next: Part 5\n",
    "\n",
    "Now we understand the theory. In Part 5, we'll:\n",
    "\n",
    "- **Implement BERT from scratch**\n",
    "- Load pre-trained weights\n",
    "- Fine-tune on a real task\n",
    "- See it work\n",
    "\n",
    "---\n",
    "\n",
    "*Paper:* [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
