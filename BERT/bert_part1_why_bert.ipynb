{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT: Part 1\n",
    "\n",
    "## Why BERT Changed Everything\n",
    "\n",
    "---\n",
    "\n",
    "**Paper:** [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "**Authors:** Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova (Google AI Language)\n",
    "\n",
    "**Published:** October 2018\n",
    "\n",
    "---\n",
    "\n",
    "Before BERT, getting state-of-the-art results on NLP tasks required task-specific architectures. You needed different models for sentiment analysis, question answering, named entity recognition.\n",
    "\n",
    "BERT changed that. One pre-trained model, fine-tune it for 30 minutes, beat everything.\n",
    "\n",
    "Let me show you why this was such a big deal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The State of NLP Before BERT\n",
    "\n",
    "To understand why BERT mattered, you need to know what we were doing before.\n",
    "\n",
    "### The Word Embedding Era (2013-2017)\n",
    "\n",
    "In 2013, Word2Vec showed us something remarkable: you could represent words as vectors, and similar words would have similar vectors.\n",
    "\n",
    "```\n",
    "king - man + woman ≈ queen\n",
    "```\n",
    "\n",
    "This was genuinely exciting. We went from treating words as arbitrary symbols to having meaningful representations.\n",
    "\n",
    "**Word2Vec, GloVe, FastText** - these became the standard. You'd download pre-trained embeddings and use them as the first layer of your model.\n",
    "\n",
    "But there was a fundamental problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem: One Word, One Vector\n",
    "\n",
    "Consider the word **\"bank\"**:\n",
    "\n",
    "- \"I deposited money in the **bank**\"\n",
    "- \"I sat by the river **bank**\"\n",
    "\n",
    "With Word2Vec, both sentences use the exact same vector for \"bank\". The embedding is **static** - it doesn't change based on context.\n",
    "\n",
    "This is obviously wrong. The word means completely different things in these sentences.\n",
    "\n",
    "Same problem with \"apple\":\n",
    "- \"I ate an **apple**\" (fruit)\n",
    "- \"I bought an **Apple**\" (company)\n",
    "\n",
    "Or \"play\":\n",
    "- \"Let's **play** basketball\" (verb - activity)\n",
    "- \"We watched a **play**\" (noun - theater)\n",
    "\n",
    "Static embeddings can't handle this. They give you one vector per word, regardless of how it's used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize this problem\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Static embeddings (Word2Vec style)\n",
    "ax = axes[0]\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-2, 2)\n",
    "ax.set_title('Static Embeddings (Word2Vec)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# \"bank\" has ONE position regardless of context\n",
    "ax.scatter([0.5], [0.8], s=200, c='#e74c3c', zorder=5)\n",
    "ax.annotate('\"bank\"\\n(one vector for all uses)', (0.5, 0.8), \n",
    "            xytext=(0.5, 0.2), fontsize=10, ha='center',\n",
    "            arrowprops=dict(arrowstyle='->', color='gray'))\n",
    "\n",
    "# Context sentences\n",
    "ax.text(-1.8, 1.7, '\"river bank\"', fontsize=9, color='#3498db')\n",
    "ax.text(-1.8, 1.4, '\"bank account\"', fontsize=9, color='#3498db')\n",
    "ax.text(-1.8, 1.1, '\"bank robbery\"', fontsize=9, color='#3498db')\n",
    "ax.text(-1.8, 0.8, '\"steep bank\"', fontsize=9, color='#3498db')\n",
    "\n",
    "# Arrow showing they all map to same point\n",
    "for y in [1.7, 1.4, 1.1, 0.8]:\n",
    "    ax.annotate('', xy=(0.3, 0.8), xytext=(-0.5, y),\n",
    "                arrowprops=dict(arrowstyle='->', color='#bdc3c7', lw=0.8))\n",
    "\n",
    "ax.text(0, -1.5, 'Problem: Same vector for\\ncompletely different meanings!', \n",
    "        fontsize=10, ha='center', color='#e74c3c', fontweight='bold')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "# Right: Contextualized embeddings (BERT style)\n",
    "ax = axes[1]\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-2, 2)\n",
    "ax.set_title('Contextualized Embeddings (BERT)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Different positions based on context\n",
    "positions = [(1.2, 1.5), (1.0, 0.3), (0.8, 0.1), (-0.8, 1.2)]\n",
    "contexts = ['\"river bank\"', '\"bank account\"', '\"bank robbery\"', '\"steep bank\"']\n",
    "colors = ['#27ae60', '#e74c3c', '#e74c3c', '#27ae60']\n",
    "\n",
    "for (x, y), ctx, c in zip(positions, contexts, colors):\n",
    "    ax.scatter([x], [y], s=150, c=c, zorder=5)\n",
    "    ax.annotate(ctx, (x, y), xytext=(x-0.3, y+0.3), fontsize=9)\n",
    "\n",
    "ax.text(0, -1.5, 'Solution: Different vectors\\nbased on context!', \n",
    "        fontsize=10, ha='center', color='#27ae60', fontweight='bold')\n",
    "\n",
    "# Add legend\n",
    "ax.scatter([], [], c='#27ae60', s=100, label='Geography meaning')\n",
    "ax.scatter([], [], c='#e74c3c', s=100, label='Financial meaning')\n",
    "ax.legend(loc='lower right', fontsize=9)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Enter ELMo: The First Contextualized Embeddings (2018)\n",
    "\n",
    "A few months before BERT, a paper called **ELMo** (Embeddings from Language Models) tackled this problem.\n",
    "\n",
    "The idea: instead of one fixed embedding per word, run the sentence through a bidirectional LSTM and use the hidden states as embeddings.\n",
    "\n",
    "```\n",
    "Forward LSTM:  The → cat → sat → on → the → bank\n",
    "Backward LSTM: bank ← the ← on ← sat ← cat ← The\n",
    "```\n",
    "\n",
    "Combine both directions → contextualized embedding for each word.\n",
    "\n",
    "This worked! ELMo improved results across many tasks.\n",
    "\n",
    "**But there was still a problem...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELMo's Limitation: Shallow Bidirectionality\n",
    "\n",
    "ELMo runs two separate LSTMs:\n",
    "- Forward: reads left-to-right\n",
    "- Backward: reads right-to-left\n",
    "\n",
    "Then it **concatenates** them:\n",
    "\n",
    "```\n",
    "ELMo(\"bank\") = [forward_hidden; backward_hidden]\n",
    "```\n",
    "\n",
    "The problem? Each direction is computed **independently**. The forward LSTM doesn't know what the backward LSTM learned, and vice versa.\n",
    "\n",
    "It's like having two people read a sentence from opposite ends and then combining their notes. They never actually discussed what they found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELMo vs BERT comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# ELMo\n",
    "ax = axes[0]\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "ax.set_title('ELMo: Concatenate Two Directions', fontsize=12, fontweight='bold')\n",
    "\n",
    "words = ['The', 'river', 'bank', 'was', 'steep']\n",
    "for i, word in enumerate(words):\n",
    "    ax.text(1 + i*1.7, 1, word, fontsize=10, ha='center', \n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='#ecf0f1', edgecolor='#bdc3c7'))\n",
    "\n",
    "# Forward arrows\n",
    "for i in range(4):\n",
    "    ax.annotate('', xy=(2.2 + i*1.7, 3.5), xytext=(1.3 + i*1.7, 3.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='#3498db', lw=2))\n",
    "ax.text(5, 4, 'Forward LSTM', fontsize=10, ha='center', color='#3498db', fontweight='bold')\n",
    "\n",
    "# Backward arrows\n",
    "for i in range(4):\n",
    "    ax.annotate('', xy=(1.3 + i*1.7, 5.5), xytext=(2.2 + i*1.7, 5.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='#e74c3c', lw=2))\n",
    "ax.text(5, 6.2, 'Backward LSTM', fontsize=10, ha='center', color='#e74c3c', fontweight='bold')\n",
    "\n",
    "# Concatenation\n",
    "ax.text(5, 7.2, 'Concatenate (no interaction)', fontsize=9, ha='center', \n",
    "        bbox=dict(boxstyle='round', facecolor='#f9e79f'))\n",
    "\n",
    "# BERT\n",
    "ax = axes[1]\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "ax.set_title('BERT: True Bidirectional Attention', fontsize=12, fontweight='bold')\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    ax.text(1 + i*1.7, 1, word, fontsize=10, ha='center', \n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='#ecf0f1', edgecolor='#bdc3c7'))\n",
    "\n",
    "# Bidirectional attention (all-to-all)\n",
    "# Show attention from \"bank\" to all other words\n",
    "bank_x = 1 + 2*1.7  # position of \"bank\"\n",
    "for i, word in enumerate(words):\n",
    "    other_x = 1 + i*1.7\n",
    "    if word != 'bank':\n",
    "        ax.annotate('', xy=(other_x, 2.5), xytext=(bank_x, 2.5),\n",
    "                    arrowprops=dict(arrowstyle='<->', color='#27ae60', lw=1.5,\n",
    "                                   connectionstyle='arc3,rad=0.3'))\n",
    "\n",
    "ax.text(5, 4.5, 'Self-Attention:\\nEvery word sees every other word\\nAT THE SAME TIME', \n",
    "        fontsize=10, ha='center', color='#27ae60', fontweight='bold')\n",
    "\n",
    "ax.text(5, 7.2, 'Deep bidirectional (true fusion)', fontsize=9, ha='center', \n",
    "        bbox=dict(boxstyle='round', facecolor='#d5f5e3'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Meanwhile: GPT (June 2018)\n",
    "\n",
    "OpenAI released GPT a few months before BERT. GPT used the Transformer architecture (which we covered in the previous series), but only the **decoder** part.\n",
    "\n",
    "GPT was trained to predict the next word:\n",
    "\n",
    "```\n",
    "Input:  \"The cat sat on the\"\n",
    "Target: \"mat\"\n",
    "```\n",
    "\n",
    "This worked great for text generation. But there's a limitation for understanding tasks.\n",
    "\n",
    "### GPT's Problem: Left-to-Right Only\n",
    "\n",
    "Because GPT predicts the next word, it can only look at previous words. It's **unidirectional**.\n",
    "\n",
    "```\n",
    "\"The cat sat on the [MASK]\"\n",
    "                     ↑\n",
    "    Can only see: \"The cat sat on the\"\n",
    "    Cannot see: anything after [MASK]\n",
    "```\n",
    "\n",
    "For many tasks (sentiment analysis, question answering), you want to look at the **entire** sentence, not just what came before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The BERT Insight: Masked Language Modeling\n",
    "\n",
    "The Google team asked a simple question:\n",
    "\n",
    "> \"What if we could train a Transformer to look at the whole sentence?\"\n",
    "\n",
    "The problem: if you train a model to predict the next word, it can't see future words (that would be cheating). But if you just show it the whole sentence... what do you train it to predict?\n",
    "\n",
    "**Their solution: Mask some words and predict them.**\n",
    "\n",
    "```\n",
    "Original:  \"The cat sat on the mat\"\n",
    "Masked:    \"The [MASK] sat on the mat\"\n",
    "Task:      Predict that [MASK] = \"cat\"\n",
    "```\n",
    "\n",
    "Now the model can see words on **both sides** of the masked word. It has full bidirectional context.\n",
    "\n",
    "This is the key insight of BERT: **Masked Language Modeling (MLM)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM visualization\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 7)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(7, 6.5, 'Masked Language Modeling: The Key Innovation', fontsize=14, \n",
    "        ha='center', fontweight='bold')\n",
    "\n",
    "# Original sentence\n",
    "ax.text(1, 5, 'Original:', fontsize=11, fontweight='bold')\n",
    "words = ['The', 'cat', 'sat', 'on', 'the', 'mat']\n",
    "for i, word in enumerate(words):\n",
    "    ax.text(2.5 + i*1.5, 5, word, fontsize=12, ha='center',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='#3498db', edgecolor='none'),\n",
    "            color='white')\n",
    "\n",
    "# Masked sentence\n",
    "ax.text(1, 3.5, 'Masked:', fontsize=11, fontweight='bold')\n",
    "masked_words = ['The', '[MASK]', 'sat', 'on', 'the', 'mat']\n",
    "for i, word in enumerate(masked_words):\n",
    "    color = '#e74c3c' if word == '[MASK]' else '#3498db'\n",
    "    ax.text(2.5 + i*1.5, 3.5, word, fontsize=12, ha='center',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor=color, edgecolor='none'),\n",
    "            color='white')\n",
    "\n",
    "# Arrows showing context\n",
    "mask_x = 2.5 + 1*1.5  # position of [MASK]\n",
    "ax.annotate('', xy=(mask_x - 0.5, 2.8), xytext=(2.5, 3.2),\n",
    "            arrowprops=dict(arrowstyle='->', color='#27ae60', lw=2))\n",
    "ax.annotate('', xy=(mask_x + 0.5, 2.8), xytext=(2.5 + 2*1.5, 3.2),\n",
    "            arrowprops=dict(arrowstyle='->', color='#27ae60', lw=2))\n",
    "ax.annotate('', xy=(mask_x + 0.5, 2.8), xytext=(2.5 + 3*1.5, 3.2),\n",
    "            arrowprops=dict(arrowstyle='->', color='#27ae60', lw=2))\n",
    "ax.annotate('', xy=(mask_x + 0.5, 2.8), xytext=(2.5 + 4*1.5, 3.2),\n",
    "            arrowprops=dict(arrowstyle='->', color='#27ae60', lw=2))\n",
    "ax.annotate('', xy=(mask_x + 0.5, 2.8), xytext=(2.5 + 5*1.5, 3.2),\n",
    "            arrowprops=dict(arrowstyle='->', color='#27ae60', lw=2))\n",
    "\n",
    "# Prediction\n",
    "ax.text(mask_x, 2.3, 'Predict: \"cat\"', fontsize=12, ha='center', fontweight='bold',\n",
    "        bbox=dict(boxstyle='round', facecolor='#f39c12', edgecolor='none'),\n",
    "        color='white')\n",
    "\n",
    "# Key point\n",
    "ax.text(7, 1, 'The model sees BOTH \"The\" (left) AND \"sat on the mat\" (right)\\nto predict the masked word!', \n",
    "        fontsize=11, ha='center', \n",
    "        bbox=dict(boxstyle='round', facecolor='#d5f5e3', edgecolor='#27ae60'))\n",
    "\n",
    "# Comparison\n",
    "ax.text(12, 4.5, 'GPT:', fontsize=10, fontweight='bold')\n",
    "ax.text(12, 4, 'Can only see left', fontsize=9, color='#e74c3c')\n",
    "ax.text(12, 3.3, 'BERT:', fontsize=10, fontweight='bold')\n",
    "ax.text(12, 2.8, 'Sees both sides!', fontsize=9, color='#27ae60')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Why This Matters: Real Examples\n",
    "\n",
    "Let me show you why bidirectional context is so important.\n",
    "\n",
    "### Example 1: Pronoun Resolution\n",
    "\n",
    "```\n",
    "\"The trophy didn't fit in the suitcase because it was too big.\"\n",
    "```\n",
    "\n",
    "What does \"it\" refer to? The trophy or the suitcase?\n",
    "\n",
    "- If \"it was too **big**\" → \"it\" = trophy\n",
    "- If \"it was too **small**\" → \"it\" = suitcase\n",
    "\n",
    "You need to see \"big\" (which comes **after** \"it\") to understand what \"it\" means.\n",
    "\n",
    "GPT, reading left-to-right, hasn't seen \"big\" yet when it processes \"it\".\n",
    "\n",
    "BERT sees everything at once.\n",
    "\n",
    "### Example 2: Sentiment Analysis\n",
    "\n",
    "```\n",
    "\"I thought the movie would be terrible, but it was actually amazing.\"\n",
    "```\n",
    "\n",
    "Is this positive or negative? You need to read the **whole sentence**. The beginning sounds negative, but the ending flips it.\n",
    "\n",
    "### Example 3: Question Answering\n",
    "\n",
    "```\n",
    "Context: \"The Eiffel Tower is located in Paris, France.\"\n",
    "Question: \"Where is the Eiffel Tower?\"\n",
    "```\n",
    "\n",
    "The model needs to match \"Where\" in the question to \"located in\" in the context. This requires looking at both directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Second Pre-training Task: Next Sentence Prediction\n",
    "\n",
    "Masked LM teaches the model about words. But many tasks (like question answering) involve understanding **relationships between sentences**.\n",
    "\n",
    "BERT adds a second pre-training task: **Next Sentence Prediction (NSP)**.\n",
    "\n",
    "Given two sentences, predict if the second one actually follows the first:\n",
    "\n",
    "```\n",
    "Sentence A: \"The man went to the store.\"\n",
    "Sentence B: \"He bought some milk.\"\n",
    "Label: IsNext (yes, B follows A)\n",
    "\n",
    "Sentence A: \"The man went to the store.\"\n",
    "Sentence B: \"Penguins are flightless birds.\"\n",
    "Label: NotNext (random sentence, doesn't follow)\n",
    "```\n",
    "\n",
    "50% of training examples are real consecutive sentences (IsNext).\n",
    "50% are random pairs (NotNext).\n",
    "\n",
    "This teaches the model to understand discourse and sentence relationships.\n",
    "\n",
    "*(Note: Later research showed NSP isn't that important. Models like RoBERTa dropped it. But it was part of the original BERT.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pre-training + Fine-tuning: The New Paradigm\n",
    "\n",
    "Here's the workflow BERT introduced:\n",
    "\n",
    "### Step 1: Pre-train (done once, by Google)\n",
    "\n",
    "- Train on massive unlabeled text (Wikipedia + Books)\n",
    "- Tasks: Masked LM + Next Sentence Prediction\n",
    "- Takes days on many TPUs\n",
    "- Results in a general-purpose language understanding model\n",
    "\n",
    "### Step 2: Fine-tune (done by you, for your task)\n",
    "\n",
    "- Start from pre-trained weights\n",
    "- Add a simple output layer for your task\n",
    "- Train on your labeled data\n",
    "- Takes minutes to hours on one GPU\n",
    "\n",
    "This is **transfer learning** for NLP. Similar to how ImageNet pre-training revolutionized computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-train + Fine-tune visualization\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(7, 7.5, 'The Pre-train + Fine-tune Paradigm', fontsize=14, \n",
    "        ha='center', fontweight='bold')\n",
    "\n",
    "# Pre-training phase\n",
    "pretrain_box = FancyBboxPatch((0.5, 3), 5.5, 3.5, boxstyle=\"round,pad=0.1\",\n",
    "                                facecolor='#ebf5fb', edgecolor='#3498db', linewidth=2)\n",
    "ax.add_patch(pretrain_box)\n",
    "ax.text(3.25, 6, 'PRE-TRAINING', fontsize=12, ha='center', fontweight='bold', color='#3498db')\n",
    "\n",
    "ax.text(3.25, 5.2, 'Data: Wikipedia + Books\\n(3.3 billion words)', fontsize=9, ha='center')\n",
    "ax.text(3.25, 4.2, 'Tasks:\\n- Masked LM (15% of words)\\n- Next Sentence Prediction', fontsize=9, ha='center')\n",
    "ax.text(3.25, 3.3, 'Time: Days on 64 TPUs', fontsize=9, ha='center', color='#e74c3c')\n",
    "\n",
    "# Arrow\n",
    "ax.annotate('', xy=(7.5, 4.75), xytext=(6, 4.75),\n",
    "            arrowprops=dict(arrowstyle='->', color='#333', lw=2))\n",
    "ax.text(6.75, 5.2, 'Pre-trained\\nweights', fontsize=9, ha='center')\n",
    "\n",
    "# Fine-tuning phase\n",
    "finetune_box = FancyBboxPatch((7.5, 3), 6, 3.5, boxstyle=\"round,pad=0.1\",\n",
    "                                facecolor='#fef9e7', edgecolor='#f39c12', linewidth=2)\n",
    "ax.add_patch(finetune_box)\n",
    "ax.text(10.5, 6, 'FINE-TUNING', fontsize=12, ha='center', fontweight='bold', color='#f39c12')\n",
    "\n",
    "ax.text(10.5, 5.2, 'Data: Your task-specific dataset\\n(could be just 1000 examples!)', fontsize=9, ha='center')\n",
    "ax.text(10.5, 4.2, 'Task: Classification, NER, QA,\\nwhatever you need', fontsize=9, ha='center')\n",
    "ax.text(10.5, 3.3, 'Time: Minutes to hours on 1 GPU', fontsize=9, ha='center', color='#27ae60')\n",
    "\n",
    "# Output tasks\n",
    "tasks = ['Sentiment', 'NER', 'QA', 'Similarity']\n",
    "for i, task in enumerate(tasks):\n",
    "    ax.text(8 + i*1.5, 2.3, task, fontsize=8, ha='center',\n",
    "            bbox=dict(boxstyle='round,pad=0.2', facecolor='#d5f5e3', edgecolor='#27ae60'))\n",
    "\n",
    "# Key insight\n",
    "ax.text(7, 1, 'Key: Pre-training captures general language knowledge.\\nFine-tuning adapts it to your specific task.', \n",
    "        fontsize=10, ha='center', style='italic',\n",
    "        bbox=dict(boxstyle='round', facecolor='#fadbd8', edgecolor='#e74c3c'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Results: BERT Destroyed Everything\n",
    "\n",
    "When BERT was released, it immediately set new state-of-the-art results on 11 NLP tasks.\n",
    "\n",
    "Not by a little bit. By a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results comparison\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# GLUE benchmark\n",
    "ax = axes[0]\n",
    "tasks = ['MNLI', 'QQP', 'QNLI', 'SST-2', 'CoLA', 'MRPC', 'RTE']\n",
    "previous_sota = [80.6, 66.1, 82.3, 93.2, 35.0, 86.0, 61.7]\n",
    "bert_base = [84.6, 71.2, 90.5, 93.5, 52.1, 88.9, 66.4]\n",
    "bert_large = [86.7, 72.1, 92.7, 94.9, 60.5, 89.3, 70.1]\n",
    "\n",
    "x = np.arange(len(tasks))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, previous_sota, width, label='Previous SOTA', color='#bdc3c7')\n",
    "ax.bar(x, bert_base, width, label='BERT-Base', color='#3498db')\n",
    "ax.bar(x + width, bert_large, width, label='BERT-Large', color='#e74c3c')\n",
    "\n",
    "ax.set_ylabel('Accuracy / Score')\n",
    "ax.set_title('GLUE Benchmark Results', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tasks, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(30, 100)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# SQuAD\n",
    "ax = axes[1]\n",
    "models = ['Previous\\nSOTA', 'BERT-Base', 'BERT-Large', 'Human']\n",
    "squad_f1 = [84.1, 88.5, 90.9, 91.2]\n",
    "colors = ['#bdc3c7', '#3498db', '#e74c3c', '#27ae60']\n",
    "\n",
    "bars = ax.bar(models, squad_f1, color=colors)\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('SQuAD 1.1 (Question Answering)', fontweight='bold')\n",
    "ax.set_ylim(80, 95)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, score in zip(bars, squad_f1):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3, \n",
    "            f'{score}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.text(2.5, 81.5, 'BERT-Large nearly matches\\nhuman performance!', \n",
    "        fontsize=9, ha='center', color='#e74c3c', style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Key Numbers\n",
    "\n",
    "| Benchmark | Metric | Previous SOTA | BERT-Large | Improvement |\n",
    "|-----------|--------|---------------|------------|-------------|\n",
    "| GLUE | Average | 75.5 | 82.1 | **+6.6 points** |\n",
    "| SQuAD 1.1 | F1 | 84.1 | 90.9 | **+6.8 points** |\n",
    "| SQuAD 2.0 | F1 | 66.3 | 83.1 | **+16.8 points** |\n",
    "| SWAG | Accuracy | 66.7 | 86.6 | **+19.9 points** |\n",
    "\n",
    "On SQuAD 2.0, BERT improved by almost **17 points**. That's not incremental progress - that's a paradigm shift.\n",
    "\n",
    "On SWAG (commonsense inference), the improvement was nearly **20 points**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Why BERT Worked So Well\n",
    "\n",
    "Let me summarize the key innovations:\n",
    "\n",
    "### 1. True Bidirectionality\n",
    "\n",
    "Unlike GPT (left-to-right) or ELMo (separate directions), BERT sees the full context at every layer through self-attention.\n",
    "\n",
    "### 2. Masked Language Modeling\n",
    "\n",
    "Clever pre-training task that forces the model to understand context deeply.\n",
    "\n",
    "### 3. Transformer Architecture\n",
    "\n",
    "The encoder-only Transformer (which we studied in the previous series) is perfect for understanding tasks.\n",
    "\n",
    "### 4. Scale\n",
    "\n",
    "BERT was trained on a lot of data:\n",
    "- BooksCorpus: 800 million words\n",
    "- English Wikipedia: 2.5 billion words\n",
    "- Total: 3.3 billion words\n",
    "\n",
    "And the model was large:\n",
    "- BERT-Base: 110 million parameters\n",
    "- BERT-Large: 340 million parameters\n",
    "\n",
    "### 5. The Pre-train + Fine-tune Paradigm\n",
    "\n",
    "Training once on lots of data, then adapting to specific tasks with minimal effort.\n",
    "\n",
    "This combination was incredibly powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## BERT's Impact\n",
    "\n",
    "BERT changed how we do NLP:\n",
    "\n",
    "**Before BERT:** Design task-specific architectures, train from scratch.\n",
    "\n",
    "**After BERT:** Download pre-trained model, add output layer, fine-tune.\n",
    "\n",
    "This made state-of-the-art NLP accessible. You didn't need massive compute or clever architectures anymore. Just fine-tune BERT.\n",
    "\n",
    "### The Family Tree\n",
    "\n",
    "BERT spawned an entire family of models:\n",
    "\n",
    "```\n",
    "BERT (Oct 2018)\n",
    "    ├── RoBERTa (July 2019) - Better training recipe\n",
    "    ├── ALBERT (Sept 2019) - Parameter efficient\n",
    "    ├── DistilBERT (Oct 2019) - Smaller, faster\n",
    "    ├── ELECTRA (March 2020) - More efficient pre-training\n",
    "    └── Many more...\n",
    "```\n",
    "\n",
    "Even GPT-2 and GPT-3 were influenced by showing that pre-training at scale works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: The Timeline\n",
    "\n",
    "| Year | Development | Limitation |\n",
    "|------|------------|------------|\n",
    "| 2013 | Word2Vec | Static embeddings |\n",
    "| 2014 | GloVe | Static embeddings |\n",
    "| 2018 Feb | ELMo | Shallow bidirectional |\n",
    "| 2018 June | GPT | Unidirectional only |\n",
    "| **2018 Oct** | **BERT** | **Deep bidirectional** |\n",
    "\n",
    "BERT combined the best ideas:\n",
    "- Transformer architecture (from \"Attention is All You Need\")\n",
    "- Contextualized embeddings (from ELMo)\n",
    "- Pre-training at scale (from GPT)\n",
    "- True bidirectionality (new!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What's Next: Part 2\n",
    "\n",
    "Now you understand WHY BERT was created and what problems it solved.\n",
    "\n",
    "In Part 2, we'll look at HOW it works:\n",
    "\n",
    "- The architecture (encoder-only Transformer)\n",
    "- Input representation ([CLS], [SEP], segments)\n",
    "- WordPiece tokenization\n",
    "- BERT-Base vs BERT-Large\n",
    "\n",
    "---\n",
    "\n",
    "*Paper:* [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "*Original Transformer paper:* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
