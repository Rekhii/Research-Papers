{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT: Part 3\n",
    "\n",
    "## Pre-training - Masked Language Modeling and Next Sentence Prediction\n",
    "\n",
    "---\n",
    "\n",
    "**Paper:** [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "---\n",
    "\n",
    "This is where BERT gets its power. The pre-training tasks are what teach BERT to understand language before it ever sees your specific task.\n",
    "\n",
    "Let's look at exactly how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1: Masked Language Modeling (MLM)\n",
    "\n",
    "The core idea: randomly hide some words, ask the model to predict them.\n",
    "\n",
    "```\n",
    "Original:  \"The cat sat on the mat\"\n",
    "Masked:    \"The [MASK] sat on the mat\"\n",
    "Target:    Predict \"cat\"\n",
    "```\n",
    "\n",
    "This forces the model to understand context from both directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Masking Strategy: 15% with a Twist\n",
    "\n",
    "BERT masks **15%** of tokens in each sequence. But here's the clever part - it doesn't always replace with [MASK].\n",
    "\n",
    "For the selected 15%:\n",
    "- **80%** are replaced with [MASK]\n",
    "- **10%** are replaced with a random word\n",
    "- **10%** are kept unchanged\n",
    "\n",
    "**Why this weird split?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking strategy visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(6, 7.5, 'BERT Masking Strategy', fontsize=14, ha='center', fontweight='bold')\n",
    "\n",
    "# Original sentence\n",
    "ax.text(1, 6.5, 'Original:', fontsize=11, fontweight='bold')\n",
    "ax.text(3.5, 6.5, '\"The cat sat on the fluffy mat\"', fontsize=11)\n",
    "\n",
    "# Selected for masking\n",
    "ax.text(1, 5.5, 'Selected (15%):', fontsize=11, fontweight='bold')\n",
    "ax.text(3.5, 5.5, '\"cat\" and \"fluffy\" chosen randomly', fontsize=11, color='#e74c3c')\n",
    "\n",
    "# Three branches\n",
    "branches = [\n",
    "    (2, 4, '80% → [MASK]', '\"The [MASK] sat on the [MASK] mat\"', '#e74c3c'),\n",
    "    (6, 4, '10% → Random', '\"The dog sat on the purple mat\"', '#f39c12'),\n",
    "    (10, 4, '10% → Keep', '\"The cat sat on the fluffy mat\"', '#27ae60'),\n",
    "]\n",
    "\n",
    "for x, y, label, example, color in branches:\n",
    "    box = FancyBboxPatch((x-1.5, y-0.8), 3, 1.2, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor=color, edgecolor='none', alpha=0.8)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x, y, label, fontsize=10, ha='center', va='center', \n",
    "            color='white', fontweight='bold')\n",
    "    ax.text(x, y-1.5, example, fontsize=8, ha='center', color=color)\n",
    "\n",
    "# Arrows\n",
    "ax.annotate('', xy=(2, 4.7), xytext=(4, 5.3), arrowprops=dict(arrowstyle='->', color='#333'))\n",
    "ax.annotate('', xy=(6, 4.7), xytext=(6, 5.3), arrowprops=dict(arrowstyle='->', color='#333'))\n",
    "ax.annotate('', xy=(10, 4.7), xytext=(8, 5.3), arrowprops=dict(arrowstyle='->', color='#333'))\n",
    "\n",
    "# Explanation\n",
    "ax.text(6, 1.5, 'Why this split?', fontsize=11, ha='center', fontweight='bold')\n",
    "explanations = [\n",
    "    '• 80% [MASK]: Main signal - learn to predict from context',\n",
    "    '• 10% Random: Teaches model that ANY position might need correction',\n",
    "    '• 10% Keep: Prevents model from only learning when it sees [MASK]',\n",
    "]\n",
    "for i, exp in enumerate(explanations):\n",
    "    ax.text(2, 1 - i*0.4, exp, fontsize=9, ha='left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Not Just Use [MASK]?\n",
    "\n",
    "The problem: during fine-tuning and inference, there's no [MASK] token. The model never sees [MASK] in real data.\n",
    "\n",
    "If we only trained with [MASK], the model might:\n",
    "- Only pay attention when it sees [MASK]\n",
    "- Not learn to produce good representations for normal tokens\n",
    "\n",
    "The 10% random and 10% unchanged cases force the model to:\n",
    "- Maintain good representations for ALL positions\n",
    "- Not rely on the presence of [MASK] as a signal\n",
    "\n",
    "This is a small but important detail that makes BERT work well at inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the masking procedure\n",
    "def create_mlm_examples(tokens, vocab_size=30000, mask_token_id=103, \n",
    "                         mask_prob=0.15, mask_token_prob=0.8, random_prob=0.1):\n",
    "    \"\"\"\n",
    "    Create masked language modeling examples.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of token IDs\n",
    "        vocab_size: Size of vocabulary (for random replacement)\n",
    "        mask_token_id: ID of [MASK] token\n",
    "        mask_prob: Probability of selecting a token for masking\n",
    "        mask_token_prob: Of selected, probability of replacing with [MASK]\n",
    "        random_prob: Of selected, probability of replacing with random token\n",
    "    \n",
    "    Returns:\n",
    "        masked_tokens: Tokens with masking applied\n",
    "        labels: Original tokens for masked positions, -100 for others\n",
    "        mask_positions: Which positions were selected\n",
    "    \"\"\"\n",
    "    tokens = np.array(tokens)\n",
    "    masked_tokens = tokens.copy()\n",
    "    labels = np.full_like(tokens, -100)  # -100 = ignore in loss\n",
    "    \n",
    "    # Randomly select 15% of positions\n",
    "    mask_positions = np.random.random(len(tokens)) < mask_prob\n",
    "    \n",
    "    # Don't mask special tokens (positions 0 and last are usually [CLS] and [SEP])\n",
    "    mask_positions[0] = False\n",
    "    mask_positions[-1] = False\n",
    "    \n",
    "    for i in np.where(mask_positions)[0]:\n",
    "        labels[i] = tokens[i]  # Save original for prediction target\n",
    "        \n",
    "        rand = np.random.random()\n",
    "        if rand < mask_token_prob:  # 80% -> [MASK]\n",
    "            masked_tokens[i] = mask_token_id\n",
    "        elif rand < mask_token_prob + random_prob:  # 10% -> random\n",
    "            masked_tokens[i] = np.random.randint(1000, vocab_size)  # Avoid special tokens\n",
    "        # else: 10% -> keep original (do nothing)\n",
    "    \n",
    "    return masked_tokens, labels, mask_positions\n",
    "\n",
    "# Demo\n",
    "# Simulated token IDs for \"[CLS] The cat sat on the mat [SEP]\"\n",
    "# (In reality, these would come from the tokenizer)\n",
    "token_ids = [101, 1996, 4937, 2068, 2006, 1996, 13523, 102]  # Example IDs\n",
    "token_names = ['[CLS]', 'The', 'cat', 'sat', 'on', 'the', 'mat', '[SEP]']\n",
    "\n",
    "print(\"Original tokens:\", token_names)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print()\n",
    "\n",
    "# Run masking multiple times to show variation\n",
    "print(\"Masking examples (15% chance per token):\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(5):\n",
    "    masked, labels, positions = create_mlm_examples(token_ids)\n",
    "    \n",
    "    result = []\n",
    "    for j, (orig, mask, label) in enumerate(zip(token_names, masked, labels)):\n",
    "        if label != -100:  # This position was selected\n",
    "            if mask == 103:\n",
    "                result.append('[MASK]')\n",
    "            elif mask != token_ids[j]:\n",
    "                result.append(f'[RAND:{mask}]')\n",
    "            else:\n",
    "                result.append(f'{orig}*')  # Kept same\n",
    "        else:\n",
    "            result.append(orig)\n",
    "    \n",
    "    print(f\"Example {i+1}: {' '.join(result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How MLM Prediction Works\n",
    "\n",
    "For each masked position, BERT predicts the original token:\n",
    "\n",
    "1. Get the hidden state at that position (768 dimensions for BERT-Base)\n",
    "2. Pass through a prediction head (linear layer + softmax)\n",
    "3. Output: probability distribution over vocabulary\n",
    "4. Loss: cross-entropy between prediction and actual token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM prediction visualization\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(7, 7.5, 'MLM Prediction at Masked Position', fontsize=13, ha='center', fontweight='bold')\n",
    "\n",
    "# Input with mask\n",
    "tokens = ['[CLS]', 'The', '[MASK]', 'sat', 'on', 'mat', '[SEP]']\n",
    "for i, token in enumerate(tokens):\n",
    "    color = '#e74c3c' if token == '[MASK]' else '#3498db'\n",
    "    ax.text(2 + i*1.5, 6, token, fontsize=10, ha='center',\n",
    "            bbox=dict(boxstyle='round,pad=0.2', facecolor=color, edgecolor='none', alpha=0.7),\n",
    "            color='white')\n",
    "\n",
    "# BERT\n",
    "bert_box = FancyBboxPatch((1, 4), 11, 1.2, boxstyle=\"round,pad=0.05\",\n",
    "                           facecolor='#2c3e50', edgecolor='none', alpha=0.9)\n",
    "ax.add_patch(bert_box)\n",
    "ax.text(6.5, 4.6, 'BERT (12 layers)', fontsize=11, ha='center', va='center',\n",
    "        color='white', fontweight='bold')\n",
    "\n",
    "# Arrows to BERT\n",
    "for i in range(7):\n",
    "    ax.annotate('', xy=(2 + i*1.5, 5.2), xytext=(2 + i*1.5, 5.6),\n",
    "                arrowprops=dict(arrowstyle='->', color='#333', lw=1))\n",
    "\n",
    "# Hidden state for masked position\n",
    "ax.annotate('', xy=(5, 3.2), xytext=(5, 4),\n",
    "            arrowprops=dict(arrowstyle='->', color='#e74c3c', lw=2))\n",
    "\n",
    "hidden_box = FancyBboxPatch((3.5, 2.2), 3, 0.8, boxstyle=\"round,pad=0.05\",\n",
    "                             facecolor='#e74c3c', edgecolor='none', alpha=0.8)\n",
    "ax.add_patch(hidden_box)\n",
    "ax.text(5, 2.6, 'Hidden state (768 dims)', fontsize=9, ha='center', va='center',\n",
    "        color='white', fontweight='bold')\n",
    "\n",
    "# Prediction head\n",
    "ax.annotate('', xy=(5, 1.4), xytext=(5, 2.2),\n",
    "            arrowprops=dict(arrowstyle='->', color='#333', lw=1.5))\n",
    "\n",
    "pred_box = FancyBboxPatch((3, 0.4), 4, 0.8, boxstyle=\"round,pad=0.05\",\n",
    "                           facecolor='#27ae60', edgecolor='none', alpha=0.8)\n",
    "ax.add_patch(pred_box)\n",
    "ax.text(5, 0.8, 'Linear + Softmax → P(cat) = 0.87', fontsize=9, ha='center', va='center',\n",
    "        color='white', fontweight='bold')\n",
    "\n",
    "# Other positions (no prediction needed)\n",
    "ax.text(10, 2.6, 'Other positions:\\nno prediction\\n(labels = -100)', fontsize=9, \n",
    "        ha='center', color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2: Next Sentence Prediction (NSP)\n",
    "\n",
    "MLM teaches word-level understanding. NSP teaches sentence-level understanding.\n",
    "\n",
    "### The Task\n",
    "\n",
    "Given two sentences A and B, predict: does B actually follow A in the original text?\n",
    "\n",
    "```\n",
    "Positive example (IsNext):\n",
    "  A: \"The man went to the store.\"\n",
    "  B: \"He bought some milk.\"\n",
    "  Label: IsNext\n",
    "\n",
    "Negative example (NotNext):\n",
    "  A: \"The man went to the store.\"\n",
    "  B: \"Penguins live in Antarctica.\"\n",
    "  Label: NotNext\n",
    "```\n",
    "\n",
    "Training data: 50% real consecutive sentences, 50% random pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSP visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Positive example\n",
    "ax = axes[0]\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.axis('off')\n",
    "ax.set_title('Positive: IsNext (50%)', fontsize=12, fontweight='bold', color='#27ae60')\n",
    "\n",
    "ax.text(5, 5, 'Sentence A:', fontsize=10, ha='center', fontweight='bold')\n",
    "ax.text(5, 4.3, '\"The man went to the store.\"', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round,pad=0.3', facecolor='#3498db', edgecolor='none', alpha=0.7),\n",
    "        color='white')\n",
    "\n",
    "ax.text(5, 3.2, 'Sentence B:', fontsize=10, ha='center', fontweight='bold')\n",
    "ax.text(5, 2.5, '\"He bought some milk.\"', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round,pad=0.3', facecolor='#27ae60', edgecolor='none', alpha=0.7),\n",
    "        color='white')\n",
    "\n",
    "ax.text(5, 1.2, 'B actually follows A in original text', fontsize=9, ha='center', style='italic')\n",
    "ax.text(5, 0.6, 'Label: IsNext', fontsize=11, ha='center', fontweight='bold', color='#27ae60')\n",
    "\n",
    "# Negative example\n",
    "ax = axes[1]\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.axis('off')\n",
    "ax.set_title('Negative: NotNext (50%)', fontsize=12, fontweight='bold', color='#e74c3c')\n",
    "\n",
    "ax.text(5, 5, 'Sentence A:', fontsize=10, ha='center', fontweight='bold')\n",
    "ax.text(5, 4.3, '\"The man went to the store.\"', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round,pad=0.3', facecolor='#3498db', edgecolor='none', alpha=0.7),\n",
    "        color='white')\n",
    "\n",
    "ax.text(5, 3.2, 'Sentence B:', fontsize=10, ha='center', fontweight='bold')\n",
    "ax.text(5, 2.5, '\"Penguins live in Antarctica.\"', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round,pad=0.3', facecolor='#e74c3c', edgecolor='none', alpha=0.7),\n",
    "        color='white')\n",
    "\n",
    "ax.text(5, 1.2, 'B is a random sentence (not related)', fontsize=9, ha='center', style='italic')\n",
    "ax.text(5, 0.6, 'Label: NotNext', fontsize=11, ha='center', fontweight='bold', color='#e74c3c')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How NSP Prediction Works\n",
    "\n",
    "NSP uses the [CLS] token representation:\n",
    "\n",
    "1. Take the hidden state of [CLS] (which has \"seen\" both sentences)\n",
    "2. Pass through a linear layer → 2 classes\n",
    "3. Softmax → probability of IsNext vs NotNext\n",
    "4. Binary cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSP prediction visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 7)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(6, 6.5, 'NSP Uses [CLS] Token', fontsize=13, ha='center', fontweight='bold')\n",
    "\n",
    "# Input\n",
    "tokens = ['[CLS]', 'Sent', 'A', '...', '[SEP]', 'Sent', 'B', '...', '[SEP]']\n",
    "colors = ['#9b59b6', '#3498db', '#3498db', '#3498db', '#9b59b6', \n",
    "          '#27ae60', '#27ae60', '#27ae60', '#9b59b6']\n",
    "for i, (token, color) in enumerate(zip(tokens, colors)):\n",
    "    ax.text(1.5 + i*1.1, 5.5, token, fontsize=9, ha='center',\n",
    "            bbox=dict(boxstyle='round,pad=0.15', facecolor=color, edgecolor='none', alpha=0.7),\n",
    "            color='white')\n",
    "\n",
    "# BERT\n",
    "bert_box = FancyBboxPatch((1, 3.5), 10, 1.2, boxstyle=\"round,pad=0.05\",\n",
    "                           facecolor='#2c3e50', edgecolor='none', alpha=0.9)\n",
    "ax.add_patch(bert_box)\n",
    "ax.text(6, 4.1, 'BERT', fontsize=11, ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "# Arrows down\n",
    "for i in range(9):\n",
    "    ax.annotate('', xy=(1.5 + i*1.1, 4.7), xytext=(1.5 + i*1.1, 5.2),\n",
    "                arrowprops=dict(arrowstyle='->', color='#333', lw=0.8))\n",
    "\n",
    "# Only [CLS] output is used for NSP\n",
    "ax.annotate('', xy=(1.5, 2.7), xytext=(1.5, 3.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='#9b59b6', lw=2))\n",
    "\n",
    "cls_hidden = FancyBboxPatch((0.5, 1.8), 2, 0.7, boxstyle=\"round,pad=0.05\",\n",
    "                             facecolor='#9b59b6', edgecolor='none', alpha=0.8)\n",
    "ax.add_patch(cls_hidden)\n",
    "ax.text(1.5, 2.15, '[CLS] hidden', fontsize=9, ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "# Classifier\n",
    "ax.annotate('', xy=(3.5, 2.15), xytext=(2.5, 2.15),\n",
    "            arrowprops=dict(arrowstyle='->', color='#333', lw=1.5))\n",
    "\n",
    "clf_box = FancyBboxPatch((3.5, 1.8), 2.5, 0.7, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor='#f39c12', edgecolor='none', alpha=0.8)\n",
    "ax.add_patch(clf_box)\n",
    "ax.text(4.75, 2.15, 'Linear (768→2)', fontsize=9, ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "# Output\n",
    "ax.annotate('', xy=(7, 2.15), xytext=(6, 2.15),\n",
    "            arrowprops=dict(arrowstyle='->', color='#333', lw=1.5))\n",
    "\n",
    "ax.text(8.5, 2.5, 'IsNext: 0.92', fontsize=10, ha='center', color='#27ae60', fontweight='bold')\n",
    "ax.text(8.5, 1.8, 'NotNext: 0.08', fontsize=10, ha='center', color='#e74c3c')\n",
    "\n",
    "# Note\n",
    "ax.text(6, 0.7, '[CLS] aggregates information from entire sequence via self-attention', \n",
    "        fontsize=9, ha='center', style='italic', color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is NSP Actually Useful?\n",
    "\n",
    "This is interesting: later work (RoBERTa, 2019) found that NSP doesn't help much. In some cases, removing it actually improves results.\n",
    "\n",
    "Why did BERT include it?\n",
    "- The authors believed sentence relationships mattered\n",
    "- Tasks like QA and NLI involve sentence pairs\n",
    "\n",
    "Why might it not help?\n",
    "- The task is too easy (topic difference is obvious)\n",
    "- MLM already captures enough\n",
    "\n",
    "Modern BERT variants often drop NSP. But for understanding the original paper, it's important to know it was there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training Data\n",
    "\n",
    "BERT was pre-trained on:\n",
    "\n",
    "| Dataset | Size |\n",
    "|---------|------|\n",
    "| BooksCorpus | 800M words |\n",
    "| English Wikipedia | 2,500M words |\n",
    "| **Total** | **3,300M words** |\n",
    "\n",
    "### Why These Datasets?\n",
    "\n",
    "**BooksCorpus:** Long, coherent text. Good for learning sentence relationships.\n",
    "\n",
    "**Wikipedia:** Factual, diverse topics. Good for learning world knowledge.\n",
    "\n",
    "Both are relatively clean text - no need for heavy preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "datasets = ['BooksCorpus', 'Wikipedia']\n",
    "sizes = [800, 2500]\n",
    "colors = ['#3498db', '#27ae60']\n",
    "\n",
    "bars = ax.barh(datasets, sizes, color=colors, edgecolor='white', linewidth=2)\n",
    "ax.set_xlabel('Words (millions)', fontsize=11)\n",
    "ax.set_title('BERT Pre-training Data', fontsize=12, fontweight='bold')\n",
    "ax.set_xlim(0, 3500)\n",
    "\n",
    "for bar, size in zip(bars, sizes):\n",
    "    ax.text(size + 50, bar.get_y() + bar.get_height()/2, \n",
    "            f'{size}M', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.axvline(x=3300, color='#e74c3c', linestyle='--', lw=2)\n",
    "ax.text(3350, 0.5, 'Total: 3.3B words', fontsize=10, color='#e74c3c', fontweight='bold')\n",
    "\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training Details\n",
    "\n",
    "### Compute Requirements\n",
    "\n",
    "| Model | Hardware | Training Time |\n",
    "|-------|----------|---------------|\n",
    "| BERT-Base | 16 TPU chips | 4 days |\n",
    "| BERT-Large | 64 TPU chips | 4 days |\n",
    "\n",
    "This was expensive in 2018. Today you'd use pre-trained weights.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "```\n",
    "Batch size: 256 sequences\n",
    "Max sequence length: 512 tokens\n",
    "Learning rate: 1e-4 (with warmup)\n",
    "Optimizer: Adam (β1=0.9, β2=0.999)\n",
    "Training steps: 1,000,000\n",
    "Warmup steps: 10,000\n",
    "```\n",
    "\n",
    "### Sequence Length Trick\n",
    "\n",
    "An interesting detail: BERT trains with shorter sequences first.\n",
    "\n",
    "- First 90% of training: max_length = 128\n",
    "- Last 10% of training: max_length = 512\n",
    "\n",
    "Why? Shorter sequences are faster to process. This reduces training time significantly while still learning long-range dependencies at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Complete Pre-training Loss\n",
    "\n",
    "BERT's total loss is the sum of both tasks:\n",
    "\n",
    "$$\\mathcal{L}_{total} = \\mathcal{L}_{MLM} + \\mathcal{L}_{NSP}$$\n",
    "\n",
    "Both are cross-entropy losses:\n",
    "- MLM: averaged over masked positions\n",
    "- NSP: single binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined loss visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.text(6, 7.5, 'BERT Pre-training: Combined Loss', fontsize=14, ha='center', fontweight='bold')\n",
    "\n",
    "# Input\n",
    "input_box = FancyBboxPatch((2, 5.5), 8, 1, boxstyle=\"round,pad=0.05\",\n",
    "                            facecolor='#ecf0f1', edgecolor='#bdc3c7', linewidth=2)\n",
    "ax.add_patch(input_box)\n",
    "ax.text(6, 6, '[CLS] The [MASK] sat ... [SEP] He was ... [SEP]', fontsize=10, ha='center')\n",
    "\n",
    "# BERT\n",
    "bert_box = FancyBboxPatch((2, 3.5), 8, 1.2, boxstyle=\"round,pad=0.05\",\n",
    "                           facecolor='#2c3e50', edgecolor='none', alpha=0.9)\n",
    "ax.add_patch(bert_box)\n",
    "ax.text(6, 4.1, 'BERT', fontsize=12, ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "ax.annotate('', xy=(6, 4.7), xytext=(6, 5.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='#333', lw=2))\n",
    "\n",
    "# Two loss branches\n",
    "# MLM\n",
    "mlm_box = FancyBboxPatch((0.5, 1.5), 4.5, 1.2, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor='#e74c3c', edgecolor='none', alpha=0.8)\n",
    "ax.add_patch(mlm_box)\n",
    "ax.text(2.75, 2.1, 'MLM Loss\\nPredict masked tokens', fontsize=9, ha='center', va='center',\n",
    "        color='white', fontweight='bold')\n",
    "\n",
    "ax.annotate('', xy=(2.75, 2.7), xytext=(4.5, 3.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='#333', lw=1.5))\n",
    "\n",
    "# NSP\n",
    "nsp_box = FancyBboxPatch((7, 1.5), 4.5, 1.2, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor='#3498db', edgecolor='none', alpha=0.8)\n",
    "ax.add_patch(nsp_box)\n",
    "ax.text(9.25, 2.1, 'NSP Loss\\nPredict if B follows A', fontsize=9, ha='center', va='center',\n",
    "        color='white', fontweight='bold')\n",
    "\n",
    "ax.annotate('', xy=(9.25, 2.7), xytext=(7.5, 3.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='#333', lw=1.5))\n",
    "\n",
    "# Total loss\n",
    "ax.text(6, 0.5, 'Total Loss = L_MLM + L_NSP', fontsize=12, ha='center', fontweight='bold',\n",
    "        bbox=dict(boxstyle='round', facecolor='#f39c12', edgecolor='none', alpha=0.8),\n",
    "        color='white')\n",
    "\n",
    "ax.annotate('', xy=(4, 0.7), xytext=(2.75, 1.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='#333', lw=1))\n",
    "ax.annotate('', xy=(8, 0.7), xytext=(9.25, 1.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='#333', lw=1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Pre-training\n",
    "\n",
    "| Aspect | Details |\n",
    "|--------|--------|\n",
    "| **Task 1: MLM** | Predict 15% masked tokens |\n",
    "| Masking strategy | 80% [MASK], 10% random, 10% unchanged |\n",
    "| **Task 2: NSP** | Binary: does B follow A? |\n",
    "| Training data | 50% IsNext, 50% NotNext |\n",
    "| **Data** | Wikipedia + BooksCorpus (3.3B words) |\n",
    "| **Compute** | 4 days on 16-64 TPU chips |\n",
    "\n",
    "The key insight: these tasks are **self-supervised**. No human labels needed - the supervision comes from the text itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What's Next: Part 4\n",
    "\n",
    "Pre-training gives us a general language understanding model. In Part 4, we'll see how to use it:\n",
    "\n",
    "- **Fine-tuning** on specific tasks\n",
    "- Different task types (classification, NER, QA)\n",
    "- How to add task-specific heads\n",
    "\n",
    "---\n",
    "\n",
    "*Paper:* [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
