{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to JAX: The Framework That Thinks in Gradients\n",
    "\n",
    "**Google's secret weapon for differentiable everything**\n",
    "\n",
    "---\n",
    "\n",
    "There's a quiet revolution happening in machine learning.\n",
    "\n",
    "While everyone argues about PyTorch vs TensorFlow, a third framework has been silently conquering research labs at DeepMind, Google Brain, and top universities. It powers AlphaFold. It runs cutting-edge reinforcement learning. It trains transformers at unprecedented speeds.\n",
    "\n",
    "It's called **JAX**.\n",
    "\n",
    "And it's not just another deep learning framework. It's a fundamentally different way of thinking about numerical computation.\n",
    "\n",
    "This notebook will take you from zero to dangerous. By the end, you'll understand not just *how* to use JAX, but *why* it exists and when it's the right tool for the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: What Is JAX?\n",
    "\n",
    "JAX is three things fused into one:\n",
    "\n",
    "1. **NumPy, but faster** - Same API you know, but running on GPU/TPU\n",
    "2. **Automatic differentiation** - Gradients of arbitrary Python functions\n",
    "3. **A compiler** - XLA compilation for insane speedups\n",
    "\n",
    "The name \"JAX\" comes from:\n",
    "- **J**ust-in-time compilation\n",
    "- **A**utomatic differentiation\n",
    "- **X**LA (Accelerated Linear Algebra)\n",
    "\n",
    "### The Philosophy\n",
    "\n",
    "JAX is built on a radical idea: **functions should be transformable**.\n",
    "\n",
    "You write a function. JAX gives you tools to:\n",
    "- Get its gradient (`grad`)\n",
    "- Compile it for speed (`jit`)\n",
    "- Vectorize it over batches (`vmap`)\n",
    "- Parallelize it across devices (`pmap`)\n",
    "\n",
    "These transformations compose. You can take the gradient of a jit-compiled, vmapped function. This composability is JAX's superpower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install JAX (CPU version for this notebook)\n",
    "# For GPU: pip install jax[cuda] -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "!pip install jax jaxlib -q\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, random\n",
    "import numpy as np\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Devices available: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: JAX as NumPy's Cooler Sibling\n",
    "\n",
    "If you know NumPy, you already know 80% of JAX. The API is intentionally familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy code\n",
    "np_array = np.array([1.0, 2.0, 3.0])\n",
    "np_result = np.sin(np_array) + np.cos(np_array)\n",
    "\n",
    "# JAX code - literally the same, just change the import\n",
    "jax_array = jnp.array([1.0, 2.0, 3.0])\n",
    "jax_result = jnp.sin(jax_array) + jnp.cos(jax_array)\n",
    "\n",
    "print(\"NumPy result:\", np_result)\n",
    "print(\"JAX result:  \", jax_result)\n",
    "print(\"\\nAre they equal?\", np.allclose(np_result, np.array(jax_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most NumPy operations work identically\n",
    "\n",
    "# Matrix operations\n",
    "A = jnp.array([[1, 2], [3, 4]])\n",
    "B = jnp.array([[5, 6], [7, 8]])\n",
    "\n",
    "print(\"Matrix multiplication:\")\n",
    "print(A @ B)\n",
    "\n",
    "print(\"\\nElement-wise operations:\")\n",
    "print(A * B)\n",
    "\n",
    "print(\"\\nReductions:\")\n",
    "print(f\"Sum: {jnp.sum(A)}, Mean: {jnp.mean(A)}, Max: {jnp.max(A)}\")\n",
    "\n",
    "print(\"\\nLinear algebra:\")\n",
    "print(f\"Determinant: {jnp.linalg.det(A.astype(float))}\")\n",
    "print(f\"Eigenvalues: {jnp.linalg.eigvals(A.astype(float))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAX arrays are immutable - this is a KEY difference\n",
    "\n",
    "# NumPy allows in-place modification\n",
    "np_arr = np.array([1, 2, 3])\n",
    "np_arr[0] = 100  # This works\n",
    "print(\"NumPy (mutated):\", np_arr)\n",
    "\n",
    "# JAX does NOT allow this\n",
    "jax_arr = jnp.array([1, 2, 3])\n",
    "try:\n",
    "    jax_arr[0] = 100  # This will fail!\n",
    "except TypeError as e:\n",
    "    print(f\"JAX error: {e}\")\n",
    "\n",
    "# Instead, use .at[].set() which returns a NEW array\n",
    "jax_arr_new = jax_arr.at[0].set(100)\n",
    "print(\"JAX (new array):\", jax_arr_new)\n",
    "print(\"Original unchanged:\", jax_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Immutability?\n",
    "\n",
    "This isn't JAX being difficult. It's *functional programming*, and it enables:\n",
    "\n",
    "1. **Safe parallelism** - No race conditions if data can't change\n",
    "2. **Reliable gradients** - Autodiff needs predictable behavior\n",
    "3. **Aggressive compilation** - XLA can optimize better when it knows data won't mutate\n",
    "\n",
    "Think of it like this: every JAX array is a mathematical value, not a mutable container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Automatic Differentiation with `grad`\n",
    "\n",
    "This is where JAX starts to feel like magic.\n",
    "\n",
    "You write a Python function. JAX gives you its derivative. Automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start simple: f(x) = x^2\n",
    "# We know the derivative is f'(x) = 2x\n",
    "\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "# grad() returns a NEW function that computes the derivative\n",
    "df = grad(f)\n",
    "\n",
    "# Test it\n",
    "x = 3.0\n",
    "print(f\"f({x}) = {f(x)}\")\n",
    "print(f\"f'({x}) = {df(x)}\")\n",
    "print(f\"Expected: 2 * {x} = {2 * x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It works with complex functions too\n",
    "\n",
    "def complex_function(x):\n",
    "    \"\"\"f(x) = sin(x^2) + e^(-x) * cos(x)\"\"\"\n",
    "    return jnp.sin(x**2) + jnp.exp(-x) * jnp.cos(x)\n",
    "\n",
    "# Get the derivative\n",
    "d_complex = grad(complex_function)\n",
    "\n",
    "# And the second derivative!\n",
    "d2_complex = grad(grad(complex_function))\n",
    "\n",
    "x = 1.0\n",
    "print(f\"f({x})   = {complex_function(x):.6f}\")\n",
    "print(f\"f'({x})  = {d_complex(x):.6f}\")\n",
    "print(f\"f''({x}) = {d2_complex(x):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradients work with control flow - JAX traces through your code\n",
    "\n",
    "def piecewise_function(x):\n",
    "    \"\"\"A function with branches.\"\"\"\n",
    "    if x < 0:\n",
    "        return -x  # f(x) = -x for x < 0, so f'(x) = -1\n",
    "    else:\n",
    "        return x ** 2  # f(x) = x^2 for x >= 0, so f'(x) = 2x\n",
    "\n",
    "d_piecewise = grad(piecewise_function)\n",
    "\n",
    "print(\"Derivative at x = -2.0:\", d_piecewise(-2.0))  # Should be -1\n",
    "print(\"Derivative at x = 3.0:\", d_piecewise(3.0))    # Should be 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-variable functions: gradients become vectors\n",
    "\n",
    "def multivar_function(params):\n",
    "    \"\"\"f(x, y) = x^2 + 3xy + y^2\"\"\"\n",
    "    x, y = params\n",
    "    return x**2 + 3*x*y + y**2\n",
    "\n",
    "# grad computes partial derivatives\n",
    "gradient_fn = grad(multivar_function)\n",
    "\n",
    "params = jnp.array([1.0, 2.0])  # x=1, y=2\n",
    "grads = gradient_fn(params)\n",
    "\n",
    "print(f\"Point: x={params[0]}, y={params[1]}\")\n",
    "print(f\"f(x,y) = {multivar_function(params)}\")\n",
    "print(f\"Gradient: [df/dx, df/dy] = {grads}\")\n",
    "print()\n",
    "print(\"Manual calculation:\")\n",
    "print(f\"  df/dx = 2x + 3y = 2(1) + 3(2) = 8\")\n",
    "print(f\"  df/dy = 3x + 2y = 3(1) + 2(2) = 7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Does Autodiff Work?\n",
    "\n",
    "JAX uses **reverse-mode automatic differentiation** (backpropagation).\n",
    "\n",
    "It's NOT:\n",
    "- Symbolic differentiation (like Mathematica) - that explodes for complex functions\n",
    "- Numerical differentiation (finite differences) - that's slow and imprecise\n",
    "\n",
    "It IS:\n",
    "- Tracing your function to build a computation graph\n",
    "- Applying the chain rule backwards through that graph\n",
    "- Returning exact derivatives (up to floating point)\n",
    "\n",
    "The cost? Computing gradients costs roughly 2-3x the cost of computing the function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_and_grad gives you both the function value AND gradient in one pass\n",
    "from jax import value_and_grad\n",
    "\n",
    "def loss_function(params):\n",
    "    return jnp.sum(params ** 2)\n",
    "\n",
    "# This is more efficient than calling f(x) and grad(f)(x) separately\n",
    "value_and_grad_fn = value_and_grad(loss_function)\n",
    "\n",
    "params = jnp.array([1.0, 2.0, 3.0])\n",
    "value, grads = value_and_grad_fn(params)\n",
    "\n",
    "print(f\"Loss: {value}\")\n",
    "print(f\"Gradients: {grads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: JIT Compilation - Making Code Fly\n",
    "\n",
    "Python is slow. JAX fixes this by compiling your functions with XLA.\n",
    "\n",
    "`jit` (just-in-time) compilation traces your function once, then runs the optimized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def slow_function(x):\n",
    "    \"\"\"A computation-heavy function.\"\"\"\n",
    "    for _ in range(10):\n",
    "        x = jnp.sin(x) + jnp.cos(x)\n",
    "        x = jnp.tanh(x @ x.T)\n",
    "    return jnp.sum(x)\n",
    "\n",
    "# Create a JIT-compiled version\n",
    "fast_function = jit(slow_function)\n",
    "\n",
    "# Test data\n",
    "x = random.normal(random.PRNGKey(0), (1000, 1000))\n",
    "\n",
    "# Warm up JIT (first call triggers compilation)\n",
    "_ = fast_function(x).block_until_ready()\n",
    "\n",
    "# Time comparison\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    result_slow = slow_function(x).block_until_ready()\n",
    "time_slow = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    result_fast = fast_function(x).block_until_ready()\n",
    "time_fast = time.time() - start\n",
    "\n",
    "print(f\"Without JIT: {time_slow:.3f}s\")\n",
    "print(f\"With JIT:    {time_fast:.3f}s\")\n",
    "print(f\"Speedup:     {time_slow/time_fast:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use @jit as a decorator\n",
    "\n",
    "@jit\n",
    "def neural_network_layer(params, x):\n",
    "    \"\"\"A simple dense layer.\"\"\"\n",
    "    W, b = params['W'], params['b']\n",
    "    return jnp.tanh(x @ W + b)\n",
    "\n",
    "# Initialize\n",
    "key = random.PRNGKey(42)\n",
    "params = {\n",
    "    'W': random.normal(key, (784, 256)) * 0.01,\n",
    "    'b': jnp.zeros(256)\n",
    "}\n",
    "x = random.normal(key, (32, 784))  # Batch of 32 images\n",
    "\n",
    "# This will be fast\n",
    "output = neural_network_layer(params, x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JIT Gotchas\n",
    "\n",
    "JIT traces your function with abstract values. This means:\n",
    "\n",
    "1. **No Python side effects** - print() won't work as expected inside JIT\n",
    "2. **Static shapes preferred** - Dynamic shapes require recompilation\n",
    "3. **Control flow constraints** - Use `jax.lax.cond` instead of Python `if` for traced values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: print() inside JIT only runs during tracing\n",
    "\n",
    "@jit\n",
    "def sneaky_function(x):\n",
    "    print(\"This only prints once during tracing!\")  # Not every call\n",
    "    return x ** 2\n",
    "\n",
    "print(\"First call (triggers tracing):\")\n",
    "result1 = sneaky_function(2.0)\n",
    "\n",
    "print(\"\\nSecond call (uses cached compilation):\")\n",
    "result2 = sneaky_function(3.0)\n",
    "\n",
    "print(f\"\\nResults: {result1}, {result2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Vectorization with `vmap`\n",
    "\n",
    "You write a function for a single example. `vmap` makes it work on batches.\n",
    "\n",
    "No manual batch dimensions. No reshape gymnastics. Just vmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that works on a single vector\n",
    "def single_prediction(weights, x):\n",
    "    \"\"\"Predict for ONE input vector.\"\"\"\n",
    "    return jnp.dot(weights, x)\n",
    "\n",
    "# Create data\n",
    "weights = jnp.array([1.0, 2.0, 3.0])\n",
    "single_x = jnp.array([0.5, 0.3, 0.2])\n",
    "\n",
    "# Works for single input\n",
    "print(\"Single prediction:\", single_prediction(weights, single_x))\n",
    "\n",
    "# But what about a batch of inputs?\n",
    "batch_x = random.normal(random.PRNGKey(0), (100, 3))  # 100 samples\n",
    "\n",
    "# vmap to the rescue!\n",
    "batched_prediction = vmap(single_prediction, in_axes=(None, 0))\n",
    "#                                            weights: not batched, x: batched along axis 0\n",
    "\n",
    "batch_results = batched_prediction(weights, batch_x)\n",
    "print(f\"Batch prediction shape: {batch_results.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vmap is incredibly powerful for neural networks\n",
    "\n",
    "def single_forward(params, x):\n",
    "    \"\"\"Forward pass for a SINGLE example.\"\"\"\n",
    "    # Layer 1\n",
    "    h = jnp.tanh(x @ params['W1'] + params['b1'])\n",
    "    # Layer 2\n",
    "    out = h @ params['W2'] + params['b2']\n",
    "    return out\n",
    "\n",
    "# Initialize network\n",
    "key = random.PRNGKey(42)\n",
    "keys = random.split(key, 4)\n",
    "params = {\n",
    "    'W1': random.normal(keys[0], (784, 128)) * 0.01,\n",
    "    'b1': jnp.zeros(128),\n",
    "    'W2': random.normal(keys[1], (128, 10)) * 0.01,\n",
    "    'b2': jnp.zeros(10)\n",
    "}\n",
    "\n",
    "# Single example\n",
    "single_x = random.normal(keys[2], (784,))\n",
    "print(\"Single output:\", single_forward(params, single_x).shape)\n",
    "\n",
    "# Batched - just vmap!\n",
    "batched_forward = vmap(single_forward, in_axes=(None, 0))\n",
    "batch_x = random.normal(keys[3], (64, 784))\n",
    "print(\"Batch output:\", batched_forward(params, batch_x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vmap composes with grad - per-example gradients!\n",
    "\n",
    "def loss_single(params, x, y):\n",
    "    \"\"\"Loss for a single example.\"\"\"\n",
    "    pred = single_forward(params, x)\n",
    "    return jnp.sum((pred - y) ** 2)\n",
    "\n",
    "# Gradient for single example\n",
    "grad_single = grad(loss_single)\n",
    "\n",
    "# Per-example gradients for a batch\n",
    "per_example_grads = vmap(grad_single, in_axes=(None, 0, 0))\n",
    "\n",
    "# Test\n",
    "batch_x = random.normal(random.PRNGKey(0), (32, 784))\n",
    "batch_y = random.normal(random.PRNGKey(1), (32, 10))\n",
    "\n",
    "grads = per_example_grads(params, batch_x, batch_y)\n",
    "print(\"Per-example gradient shapes:\")\n",
    "for name, g in grads.items():\n",
    "    print(f\"  {name}: {g.shape}\")  # First dimension is batch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Random Numbers in JAX\n",
    "\n",
    "JAX handles randomness differently than NumPy. This trips up everyone at first, but it's actually better.\n",
    "\n",
    "### The Problem with NumPy's RNG\n",
    "\n",
    "NumPy uses global state for random numbers:\n",
    "```python\n",
    "np.random.seed(42)\n",
    "x = np.random.randn(10)  # Uses and updates global state\n",
    "```\n",
    "\n",
    "This is:\n",
    "- **Non-reproducible** across function calls\n",
    "- **Not thread-safe** for parallel execution\n",
    "- **Incompatible with JIT** (side effects!)\n",
    "\n",
    "### JAX Solution: Explicit Keys\n",
    "\n",
    "Every random operation takes a **key**. You split keys to get independent randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "\n",
    "# Create a starting key\n",
    "key = random.PRNGKey(42)\n",
    "print(f\"Initial key: {key}\")\n",
    "\n",
    "# Generate random numbers\n",
    "x = random.normal(key, (3,))\n",
    "print(f\"Random array: {x}\")\n",
    "\n",
    "# IMPORTANT: Using the same key gives the SAME numbers!\n",
    "x_again = random.normal(key, (3,))\n",
    "print(f\"Same key again: {x_again}\")\n",
    "print(f\"Identical? {jnp.allclose(x, x_again)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get different random numbers, SPLIT the key\n",
    "\n",
    "key = random.PRNGKey(42)\n",
    "\n",
    "# Split into 2 new keys\n",
    "key1, key2 = random.split(key)\n",
    "\n",
    "x1 = random.normal(key1, (3,))\n",
    "x2 = random.normal(key2, (3,))\n",
    "\n",
    "print(f\"x1: {x1}\")\n",
    "print(f\"x2: {x2}\")\n",
    "print(f\"Different? {not jnp.allclose(x1, x2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pattern: split before each use\n",
    "\n",
    "def initialize_layer(key, in_features, out_features):\n",
    "    \"\"\"Initialize a layer with proper key handling.\"\"\"\n",
    "    key_w, key_b = random.split(key)\n",
    "    W = random.normal(key_w, (in_features, out_features)) * 0.01\n",
    "    b = random.normal(key_b, (out_features,)) * 0.01\n",
    "    return {'W': W, 'b': b}\n",
    "\n",
    "def initialize_network(key, layer_sizes):\n",
    "    \"\"\"Initialize a full network.\"\"\"\n",
    "    keys = random.split(key, len(layer_sizes) - 1)\n",
    "    params = []\n",
    "    for i, (k, in_f, out_f) in enumerate(zip(keys, layer_sizes[:-1], layer_sizes[1:])):\n",
    "        params.append(initialize_layer(k, in_f, out_f))\n",
    "    return params\n",
    "\n",
    "# Initialize a 784 -> 256 -> 128 -> 10 network\n",
    "key = random.PRNGKey(0)\n",
    "network_params = initialize_network(key, [784, 256, 128, 10])\n",
    "\n",
    "print(\"Network architecture:\")\n",
    "for i, layer in enumerate(network_params):\n",
    "    print(f\"  Layer {i}: W{layer['W'].shape}, b{layer['b'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random distributions available\n",
    "\n",
    "key = random.PRNGKey(42)\n",
    "keys = random.split(key, 6)\n",
    "\n",
    "print(\"Available distributions (examples):\")\n",
    "print(f\"  Normal:      {random.normal(keys[0], (3,))}\")\n",
    "print(f\"  Uniform:     {random.uniform(keys[1], (3,))}\")\n",
    "print(f\"  Categorical: {random.categorical(keys[2], jnp.array([0.1, 0.3, 0.6]), shape=(5,))}\")\n",
    "print(f\"  Bernoulli:   {random.bernoulli(keys[3], 0.7, shape=(5,))}\")\n",
    "print(f\"  Randint:     {random.randint(keys[4], (5,), 0, 10)}\")\n",
    "print(f\"  Permutation: {random.permutation(keys[5], 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: PyTrees - Nested Data Structures\n",
    "\n",
    "Neural networks have nested parameters: layers containing weights and biases, attention heads, etc.\n",
    "\n",
    "JAX handles these with **PyTrees** - arbitrary nested structures of arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import tree_util\n",
    "\n",
    "# PyTrees can be dicts, lists, tuples, or nested combinations\n",
    "params = {\n",
    "    'encoder': {\n",
    "        'layer1': {'W': jnp.ones((10, 5)), 'b': jnp.zeros(5)},\n",
    "        'layer2': {'W': jnp.ones((5, 3)), 'b': jnp.zeros(3)},\n",
    "    },\n",
    "    'decoder': {\n",
    "        'layer1': {'W': jnp.ones((3, 5)), 'b': jnp.zeros(5)},\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get all leaves (the actual arrays)\n",
    "leaves = tree_util.tree_leaves(params)\n",
    "print(f\"Number of parameter arrays: {len(leaves)}\")\n",
    "print(f\"Total parameters: {sum(l.size for l in leaves)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree_map applies a function to all leaves\n",
    "\n",
    "# Initialize all weights with small random values\n",
    "def init_weight(x):\n",
    "    return x * 0.01 if 'W' in str(type(x)) else x\n",
    "\n",
    "# Or more practically: scale all parameters\n",
    "scaled_params = tree_util.tree_map(lambda x: x * 0.1, params)\n",
    "\n",
    "print(\"Original W shape and values:\")\n",
    "print(f\"  {params['encoder']['layer1']['W'][0, :3]}\")\n",
    "print(\"Scaled:\")\n",
    "print(f\"  {scaled_params['encoder']['layer1']['W'][0, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad works on PyTrees automatically!\n",
    "\n",
    "def simple_loss(params, x, y):\n",
    "    \"\"\"Loss with nested params.\"\"\"\n",
    "    W1, b1 = params['layer1']['W'], params['layer1']['b']\n",
    "    W2, b2 = params['layer2']['W'], params['layer2']['b']\n",
    "    \n",
    "    h = jnp.tanh(x @ W1 + b1)\n",
    "    pred = h @ W2 + b2\n",
    "    return jnp.mean((pred - y) ** 2)\n",
    "\n",
    "# Parameters\n",
    "params = {\n",
    "    'layer1': {'W': random.normal(random.PRNGKey(0), (10, 5)), 'b': jnp.zeros(5)},\n",
    "    'layer2': {'W': random.normal(random.PRNGKey(1), (5, 2)), 'b': jnp.zeros(2)}\n",
    "}\n",
    "\n",
    "# Data\n",
    "x = random.normal(random.PRNGKey(2), (32, 10))\n",
    "y = random.normal(random.PRNGKey(3), (32, 2))\n",
    "\n",
    "# Gradient is a PyTree with same structure!\n",
    "grads = grad(simple_loss)(params, x, y)\n",
    "\n",
    "print(\"Gradient structure matches params:\")\n",
    "print(f\"  grads['layer1']['W'].shape = {grads['layer1']['W'].shape}\")\n",
    "print(f\"  grads['layer2']['b'].shape = {grads['layer2']['b'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD update with PyTrees\n",
    "\n",
    "def sgd_update(params, grads, learning_rate=0.01):\n",
    "    \"\"\"Apply SGD update to nested params.\"\"\"\n",
    "    return tree_util.tree_map(\n",
    "        lambda p, g: p - learning_rate * g,\n",
    "        params, grads\n",
    "    )\n",
    "\n",
    "# One training step\n",
    "loss_before = simple_loss(params, x, y)\n",
    "grads = grad(simple_loss)(params, x, y)\n",
    "params = sgd_update(params, grads)\n",
    "loss_after = simple_loss(params, x, y)\n",
    "\n",
    "print(f\"Loss before: {loss_before:.4f}\")\n",
    "print(f\"Loss after:  {loss_after:.4f}\")\n",
    "print(f\"Improved: {loss_after < loss_before}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Putting It All Together - Training a Neural Network\n",
    "\n",
    "Let's train a real neural network from scratch using everything we've learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic classification data\n",
    "\n",
    "def generate_spiral_data(key, n_points, n_classes=3):\n",
    "    \"\"\"Generate spiral dataset for classification.\"\"\"\n",
    "    keys = random.split(key, n_classes)\n",
    "    \n",
    "    X_list, y_list = [], []\n",
    "    for c, k in enumerate(keys):\n",
    "        # Spiral parameters\n",
    "        r = jnp.linspace(0.1, 1, n_points)  # Radius\n",
    "        t = jnp.linspace(c * 4, (c + 1) * 4, n_points) + random.normal(k, (n_points,)) * 0.2\n",
    "        \n",
    "        # Convert to Cartesian\n",
    "        x = r * jnp.sin(t)\n",
    "        y = r * jnp.cos(t)\n",
    "        \n",
    "        X_list.append(jnp.stack([x, y], axis=1))\n",
    "        y_list.append(jnp.full(n_points, c))\n",
    "    \n",
    "    X = jnp.concatenate(X_list)\n",
    "    y = jnp.concatenate(y_list)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "key = random.PRNGKey(42)\n",
    "X, y = generate_spiral_data(key, 100, n_classes=3)\n",
    "print(f\"Data shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Classes: {jnp.unique(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network\n",
    "\n",
    "def init_mlp(key, layer_sizes):\n",
    "    \"\"\"Initialize MLP parameters.\"\"\"\n",
    "    params = []\n",
    "    keys = random.split(key, len(layer_sizes) - 1)\n",
    "    \n",
    "    for k, n_in, n_out in zip(keys, layer_sizes[:-1], layer_sizes[1:]):\n",
    "        k1, k2 = random.split(k)\n",
    "        # Xavier initialization\n",
    "        W = random.normal(k1, (n_in, n_out)) * jnp.sqrt(2.0 / n_in)\n",
    "        b = jnp.zeros(n_out)\n",
    "        params.append({'W': W, 'b': b})\n",
    "    \n",
    "    return params\n",
    "\n",
    "def mlp_forward(params, x):\n",
    "    \"\"\"Forward pass through MLP.\"\"\"\n",
    "    for layer in params[:-1]:\n",
    "        x = jnp.tanh(x @ layer['W'] + layer['b'])\n",
    "    # No activation on final layer (logits)\n",
    "    x = x @ params[-1]['W'] + params[-1]['b']\n",
    "    return x\n",
    "\n",
    "def softmax_cross_entropy(logits, labels):\n",
    "    \"\"\"Compute cross-entropy loss.\"\"\"\n",
    "    # Numerically stable softmax\n",
    "    logits = logits - jnp.max(logits, axis=-1, keepdims=True)\n",
    "    log_probs = logits - jnp.log(jnp.sum(jnp.exp(logits), axis=-1, keepdims=True))\n",
    "    # One-hot encode labels\n",
    "    one_hot = jax.nn.one_hot(labels, logits.shape[-1])\n",
    "    return -jnp.mean(jnp.sum(one_hot * log_probs, axis=-1))\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    \"\"\"Compute loss for a batch.\"\"\"\n",
    "    logits = mlp_forward(params, x)\n",
    "    return softmax_cross_entropy(logits, y)\n",
    "\n",
    "def accuracy(params, x, y):\n",
    "    \"\"\"Compute classification accuracy.\"\"\"\n",
    "    logits = mlp_forward(params, x)\n",
    "    predictions = jnp.argmax(logits, axis=-1)\n",
    "    return jnp.mean(predictions == y)\n",
    "\n",
    "# Initialize\n",
    "key = random.PRNGKey(0)\n",
    "params = init_mlp(key, [2, 64, 64, 3])  # 2 inputs, 2 hidden layers, 3 outputs\n",
    "\n",
    "print(\"Network initialized:\")\n",
    "for i, layer in enumerate(params):\n",
    "    print(f\"  Layer {i}: W{layer['W'].shape}, b{layer['b'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "@jit\n",
    "def train_step(params, x, y, learning_rate=0.1):\n",
    "    \"\"\"One training step.\"\"\"\n",
    "    loss, grads = value_and_grad(loss_fn)(params, x, y)\n",
    "    # SGD update\n",
    "    params = tree_util.tree_map(\n",
    "        lambda p, g: p - learning_rate * g,\n",
    "        params, grads\n",
    "    )\n",
    "    return params, loss\n",
    "\n",
    "# Train!\n",
    "n_epochs = 500\n",
    "history = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    params, loss = train_step(params, X, y)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        acc = accuracy(params, X, y)\n",
    "        history.append((epoch, float(loss), float(acc)))\n",
    "        print(f\"Epoch {epoch:4d} | Loss: {loss:.4f} | Accuracy: {acc:.2%}\")\n",
    "\n",
    "# Final accuracy\n",
    "final_acc = accuracy(params, X, y)\n",
    "print(f\"\\nFinal accuracy: {final_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create grid\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = jnp.meshgrid(jnp.linspace(x_min, x_max, 100),\n",
    "                       jnp.linspace(y_min, y_max, 100))\n",
    "grid = jnp.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# Get predictions\n",
    "logits = mlp_forward(params, grid)\n",
    "Z = jnp.argmax(logits, axis=-1).reshape(xx.shape)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='black', s=50)\n",
    "plt.colorbar(scatter)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title(f'JAX Neural Network - Spiral Classification\\nAccuracy: {final_acc:.2%}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Advanced Topics - Quick Reference\n",
    "\n",
    "### `pmap` - Parallel Execution Across Devices\n",
    "\n",
    "```python\n",
    "from jax import pmap\n",
    "\n",
    "# Runs on all available devices in parallel\n",
    "@pmap\n",
    "def parallel_fn(x):\n",
    "    return x ** 2\n",
    "```\n",
    "\n",
    "### `lax.scan` - Efficient Loops\n",
    "\n",
    "```python\n",
    "from jax import lax\n",
    "\n",
    "# Instead of Python loops (slow, unrolled)\n",
    "def scan_fn(carry, x):\n",
    "    return carry + x, carry\n",
    "\n",
    "final, history = lax.scan(scan_fn, init=0, xs=jnp.arange(100))\n",
    "```\n",
    "\n",
    "### `lax.cond` - JIT-Compatible Conditionals\n",
    "\n",
    "```python\n",
    "# Instead of Python if (which can't be traced)\n",
    "result = lax.cond(condition, true_fn, false_fn, operand)\n",
    "```\n",
    "\n",
    "### Custom Gradients\n",
    "\n",
    "```python\n",
    "from jax import custom_vjp\n",
    "\n",
    "@custom_vjp\n",
    "def my_fn(x):\n",
    "    return x ** 2\n",
    "\n",
    "def my_fn_fwd(x):\n",
    "    return my_fn(x), x  # (output, residuals)\n",
    "\n",
    "def my_fn_bwd(residuals, g):\n",
    "    x = residuals\n",
    "    return (2 * x * g,)  # Custom gradient\n",
    "\n",
    "my_fn.defvjp(my_fn_fwd, my_fn_bwd)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: The JAX Ecosystem\n",
    "\n",
    "JAX is low-level by design. Higher-level libraries build on top:\n",
    "\n",
    "| Library | Purpose | Style |\n",
    "|---------|---------|-------|\n",
    "| **Flax** | Neural networks | Functional, explicit state |\n",
    "| **Haiku** | Neural networks | Transformed functions (DeepMind) |\n",
    "| **Optax** | Optimizers | Composable gradient transforms |\n",
    "| **Equinox** | Neural networks | PyTorch-like, but functional |\n",
    "| **Diffrax** | Differential equations | ODEs, SDEs, CDEs |\n",
    "| **JAXopt** | Optimization | Differentiable optimizers |\n",
    "\n",
    "### Quick Flax Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flax optax -q\n",
    "\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "\n",
    "# Define model with Flax\n",
    "class MLP(nn.Module):\n",
    "    hidden_dim: int\n",
    "    output_dim: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(self.hidden_dim)(x)\n",
    "        x = nn.tanh(x)\n",
    "        x = nn.Dense(self.hidden_dim)(x)\n",
    "        x = nn.tanh(x)\n",
    "        x = nn.Dense(self.output_dim)(x)\n",
    "        return x\n",
    "\n",
    "# Initialize\n",
    "model = MLP(hidden_dim=64, output_dim=3)\n",
    "params = model.init(random.PRNGKey(0), jnp.ones((1, 2)))\n",
    "\n",
    "print(\"Flax model initialized!\")\n",
    "print(f\"Parameter count: {sum(p.size for p in tree_util.tree_leaves(params))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optax for optimization\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optax.adam(learning_rate=0.01)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Loss function\n",
    "def flax_loss_fn(params, x, y):\n",
    "    logits = model.apply(params, x)\n",
    "    return softmax_cross_entropy(logits, y)\n",
    "\n",
    "@jit\n",
    "def flax_train_step(params, opt_state, x, y):\n",
    "    loss, grads = value_and_grad(flax_loss_fn)(params, x, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "# Quick training\n",
    "for epoch in range(200):\n",
    "    params, opt_state, loss = flax_train_step(params, opt_state, X, y)\n",
    "    if epoch % 50 == 0:\n",
    "        acc = jnp.mean(jnp.argmax(model.apply(params, X), axis=-1) == y)\n",
    "        print(f\"Epoch {epoch:3d} | Loss: {loss:.4f} | Acc: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Common Gotchas and Debugging\n",
    "\n",
    "### Gotcha 1: Arrays Are Immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Trying to mutate\n",
    "x = jnp.array([1, 2, 3])\n",
    "# x[0] = 100  # Error!\n",
    "\n",
    "# RIGHT: Create new array\n",
    "x = x.at[0].set(100)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gotcha 2: Shapes Must Be Static in JIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Dynamic shape causes recompilation\n",
    "@jit\n",
    "def bad_fn(x, n):\n",
    "    return x[:n]  # n changes shape!\n",
    "\n",
    "# RIGHT: Use static_argnums for values that affect shapes\n",
    "@jit\n",
    "def good_fn(x, n):\n",
    "    return x[:n]\n",
    "\n",
    "good_fn_static = jit(good_fn, static_argnums=(1,))\n",
    "\n",
    "x = jnp.arange(10)\n",
    "print(good_fn_static(x, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gotcha 3: Random Keys Must Be Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Reusing key gives same values\n",
    "key = random.PRNGKey(0)\n",
    "a = random.normal(key, (3,))\n",
    "b = random.normal(key, (3,))  # Same as a!\n",
    "print(f\"Reused key: a={a}, b={b}\")\n",
    "\n",
    "# RIGHT: Split key\n",
    "key = random.PRNGKey(0)\n",
    "key1, key2 = random.split(key)\n",
    "a = random.normal(key1, (3,))\n",
    "b = random.normal(key2, (3,))\n",
    "print(f\"Split keys: a={a}, b={b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gotcha 4: print() Doesn't Work as Expected in JIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use jax.debug.print for debugging inside JIT\n",
    "from jax import debug\n",
    "\n",
    "@jit\n",
    "def debuggable_fn(x):\n",
    "    debug.print(\"x = {x}\", x=x)  # Works!\n",
    "    return x ** 2\n",
    "\n",
    "result = debuggable_fn(jnp.array([1.0, 2.0, 3.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: When to Use JAX\n",
    "\n",
    "### JAX Excels At:\n",
    "- Research requiring custom gradients or architectures\n",
    "- Anything involving Hessians or higher-order derivatives\n",
    "- Probabilistic programming (see NumPyro)\n",
    "- Scientific computing with autodiff\n",
    "- TPU training (first-class support)\n",
    "- Functional programming enthusiasts\n",
    "\n",
    "### Maybe Use Something Else If:\n",
    "- You need extensive pre-built model zoo (PyTorch/TF have more)\n",
    "- You're deploying to mobile/edge (TFLite, CoreML have better tooling)\n",
    "- You want maximum beginner-friendliness (PyTorch is gentler)\n",
    "- You need eager execution for debugging (JAX prefers traced execution)\n",
    "\n",
    "### The Sweet Spot:\n",
    "JAX shines when you need **composable transformations**. If you find yourself saying \"I wish I could take the gradient of X\" or \"I wish this loop was faster\" or \"I want per-example gradients\" -- JAX probably has a clean solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **JAX = NumPy + Autodiff + XLA** - Familiar API with superpowers\n",
    "\n",
    "2. **Four Core Transformations**:\n",
    "   - `grad` - automatic differentiation\n",
    "   - `jit` - compilation for speed\n",
    "   - `vmap` - automatic vectorization\n",
    "   - `pmap` - parallel across devices\n",
    "\n",
    "3. **Functional Paradigm**:\n",
    "   - Immutable arrays\n",
    "   - Pure functions\n",
    "   - Explicit random keys\n",
    "\n",
    "4. **PyTrees** handle nested structures naturally\n",
    "\n",
    "5. **Ecosystem**: Flax/Haiku for NNs, Optax for optimizers\n",
    "\n",
    "---\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **Official Docs**: https://jax.readthedocs.io/\n",
    "- **JAX GitHub**: https://github.com/google/jax\n",
    "- **Flax**: https://github.com/google/flax\n",
    "- **Optax**: https://github.com/deepmind/optax\n",
    "- **JAX Tutorial (Google)**: https://jax.readthedocs.io/en/latest/tutorials.html\n",
    "\n",
    "---\n",
    "\n",
    "*\"The best code is no code. The second best is code that writes itself.\"*\n",
    "\n",
    "JAX doesn't write your code, but it transforms it in ways that feel like magic. Once you internalize the functional mindset, you'll wonder how you ever lived without `grad`, `jit`, and `vmap`.\n",
    "\n",
    "Now go differentiate everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**About this notebook**: A comprehensive introduction to JAX covering core concepts, transformations, and practical neural network training from scratch.\n",
    "\n",
    "**Connect**: [kaggle.com/seki32](https://kaggle.com/seki32) | [github.com/Rekhii](https://github.com/Rekhii)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
