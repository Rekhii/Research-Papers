{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 Research Paper | Part IV\n",
    "\n",
    "## Training & Scaling: The Hidden Engineering Story\n",
    "\n",
    "---\n",
    "\n",
    "**Paper:** [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "**Authors:** Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever (OpenAI, 2019)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story Nobody Tells\n",
    "\n",
    "Most GPT-2 discussions focus on what it can do. This notebook focuses on **how it was built**.\n",
    "\n",
    "Training GPT-2 XL (1.5B parameters) in 2019 was a **massive engineering challenge**. The decisions made here shaped all future LLM development.\n",
    "\n",
    "This notebook covers:\n",
    "\n",
    "1. **WebText Dataset Creation** - The exact pipeline that made GPT-2 possible\n",
    "2. **Byte-Level BPE Tokenization** - A technical deep dive\n",
    "3. **Training Infrastructure** - Hardware, distributed training, optimizations\n",
    "4. **Training Dynamics** - Loss curves, learning rates, batch sizes\n",
    "5. **Scaling Laws** - The discovery that changed AI forever\n",
    "6. **Compute Analysis** - FLOPs, GPU-hours, costs\n",
    "7. **Reproducibility Guide** - How to train your own GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Rectangle, Circle, FancyArrowPatch, Wedge, Arc\n",
    "from matplotlib.patches import ConnectionPatch, Polygon, Ellipse, PathPatch\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.sankey import Sankey\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Ready to explore GPT-2 training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. WebText: Building the Dataset\n",
    "\n",
    "### 1.1 The Problem with Existing Datasets\n",
    "\n",
    "Before GPT-2, language models were trained on:\n",
    "\n",
    "| Dataset | Size | Problem |\n",
    "|---------|------|--------|\n",
    "| BooksCorpus | ~1GB | Small, limited domains |\n",
    "| Wikipedia | ~12GB | Encyclopedic style only |\n",
    "| Common Crawl | ~300GB | Extremely noisy |\n",
    "\n",
    "### 1.2 The WebText Innovation\n",
    "\n",
    "From the paper:\n",
    "\n",
    "> *\"We created a new web scrape which emphasizes document quality. To do this, we only scraped web pages which have been curated/filtered by humans.\"*\n",
    "\n",
    "The key insight: **Use Reddit as a quality filter.**\n",
    "\n",
    "> *\"We scraped all outbound links from Reddit, a social media platform, which received at least 3 karma. This can be thought of as a heuristic indicator for whether other users found the link interesting, educational, or just funny.\"*\n",
    "\n",
    "### 1.3 The Complete Pipeline\n",
    "\n",
    "```\n",
    "Reddit API ‚Üí Extract links with 3+ karma ‚Üí Download pages ‚Üí \n",
    "Extract text ‚Üí Clean/Deduplicate ‚Üí Remove Wikipedia ‚Üí WebText\n",
    "```\n",
    "\n",
    "### 1.4 WebText Statistics\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Total Documents | ~8 million |\n",
    "| Total Size | ~40 GB |\n",
    "| Token Count | ~10 billion BPE tokens |\n",
    "| Avg Doc Length | ~500 words |\n",
    "\n",
    "### 1.5 Why Wikipedia Was Excluded\n",
    "\n",
    "From the paper:\n",
    "\n",
    "> *\"Wikipedia is excluded from WebText since it is a common data source for other datasets and could complicate the analysis due to overlapping training data.\"*\n",
    "\n",
    "This was for **fair evaluation**, not because Wikipedia is bad data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_webtext_pipeline():\n",
    "    \"\"\"\n",
    "    Comprehensive visualization of the WebText creation pipeline.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20, 14))\n",
    "    \n",
    "    gs = gridspec.GridSpec(3, 2, height_ratios=[1.5, 1, 1], hspace=0.35, wspace=0.25)\n",
    "    \n",
    "    # === TOP: Complete Pipeline ===\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    ax1.set_xlim(0, 24)\n",
    "    ax1.set_ylim(0, 10)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('WebText Creation Pipeline: From Reddit to Training Data', \n",
    "                  fontsize=14, fontweight='bold', pad=15)\n",
    "    \n",
    "    # Pipeline stages\n",
    "    stages = [\n",
    "        (2, 'Reddit\\nAPI', '~45M posts\\nwith links', '#FF4500', '100%'),\n",
    "        (6, 'Karma\\nFilter', 'Keep only\\n3+ karma', '#f39c12', '~60%'),\n",
    "        (10, 'URL\\nExtraction', 'Outbound\\nlinks only', '#3498db', '~45%'),\n",
    "        (14, 'Web\\nScraping', 'Download\\npages', '#27ae60', '~35%'),\n",
    "        (18, 'Cleaning', 'Dedup, filter\\nboilerplate', '#9b59b6', '~20%'),\n",
    "        (22, 'WebText', '8M docs\\n40GB', '#e74c3c', 'Final'),\n",
    "    ]\n",
    "    \n",
    "    for i, (x, name, desc, color, pct) in enumerate(stages):\n",
    "        # Main box\n",
    "        rect = FancyBboxPatch((x-1.5, 4), 3, 4, boxstyle=\"round,pad=0.05\",\n",
    "                              facecolor=color, edgecolor='black', linewidth=2)\n",
    "        ax1.add_patch(rect)\n",
    "        ax1.text(x, 7, name, ha='center', va='center', \n",
    "                fontsize=11, fontweight='bold', color='white')\n",
    "        ax1.text(x, 5, desc, ha='center', va='center', \n",
    "                fontsize=9, color='white')\n",
    "        \n",
    "        # Percentage below\n",
    "        ax1.text(x, 3, pct, ha='center', fontsize=10, fontweight='bold', color=color)\n",
    "        \n",
    "        # Arrow to next\n",
    "        if i < len(stages) - 1:\n",
    "            ax1.annotate('', xy=(x+2, 6), xytext=(x+1.6, 6),\n",
    "                        arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "    \n",
    "    # Funnel visualization\n",
    "    ax1.text(12, 1.5, 'Data Funnel: Most content filtered out for quality', \n",
    "             ha='center', fontsize=11, color='gray', style='italic')\n",
    "    \n",
    "    # === MIDDLE LEFT: Karma Distribution ===\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    \n",
    "    # Simulated karma distribution (power law)\n",
    "    karma_values = np.concatenate([\n",
    "        np.random.exponential(2, 30000),  # Most posts low karma\n",
    "        np.random.exponential(10, 5000) + 10,  # Some medium\n",
    "        np.random.exponential(50, 500) + 100,  # Few high\n",
    "    ])\n",
    "    karma_values = np.clip(karma_values, 0, 500)\n",
    "    \n",
    "    ax2.hist(karma_values, bins=50, color='#3498db', edgecolor='black', alpha=0.7)\n",
    "    ax2.axvline(x=3, color='#e74c3c', linewidth=3, linestyle='--', label='Threshold (3 karma)')\n",
    "    \n",
    "    # Shade included region\n",
    "    ax2.axvspan(3, 500, alpha=0.2, color='#27ae60')\n",
    "    \n",
    "    ax2.set_xlabel('Karma', fontsize=11)\n",
    "    ax2.set_ylabel('Number of Posts', fontsize=11)\n",
    "    ax2.set_title('Karma Distribution & Filter Threshold', fontsize=12, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.set_xlim(0, 100)\n",
    "    \n",
    "    # === MIDDLE RIGHT: Content Types ===\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    categories = ['News/Articles', 'Educational', 'Entertainment', \n",
    "                  'Technical', 'Discussion', 'Creative']\n",
    "    sizes = [28, 22, 18, 14, 10, 8]  # Estimated percentages\n",
    "    colors = ['#e74c3c', '#3498db', '#f39c12', '#27ae60', '#9b59b6', '#1abc9c']\n",
    "    explode = (0.05, 0, 0, 0, 0, 0)\n",
    "    \n",
    "    wedges, texts, autotexts = ax3.pie(sizes, explode=explode, labels=categories, \n",
    "                                        colors=colors, autopct='%1.0f%%',\n",
    "                                        shadow=True, startangle=90,\n",
    "                                        textprops={'fontsize': 9})\n",
    "    ax3.set_title('Estimated Content Distribution in WebText', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # === BOTTOM: Dataset Comparison ===\n",
    "    ax4 = fig.add_subplot(gs[2, :])\n",
    "    ax4.set_xlim(0, 20)\n",
    "    ax4.set_ylim(0, 8)\n",
    "    ax4.axis('off')\n",
    "    ax4.set_title('WebText vs Other Datasets', fontsize=13, fontweight='bold', pad=10)\n",
    "    \n",
    "    datasets = [\n",
    "        ('BooksCorpus\\n(GPT-1)', 1, '#95a5a6', '~7K books', 'Limited domains'),\n",
    "        ('Wikipedia', 12, '#3498db', '6M articles', 'One style'),\n",
    "        ('WebText', 40, '#e74c3c', '8M pages', 'Diverse, quality'),\n",
    "        ('Common Crawl', 300, '#f39c12', 'Petabytes', 'Very noisy'),\n",
    "    ]\n",
    "    \n",
    "    max_size = 300\n",
    "    bar_positions = [2, 6, 10, 15]\n",
    "    \n",
    "    for (name, size, color, count, note), x in zip(datasets, bar_positions):\n",
    "        # Log scale for height\n",
    "        height = 5 * np.log10(size + 1) / np.log10(max_size + 1)\n",
    "        \n",
    "        rect = FancyBboxPatch((x-1, 2), 2.5, height, boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor=color, edgecolor='black', linewidth=2)\n",
    "        ax4.add_patch(rect)\n",
    "        \n",
    "        ax4.text(x+0.25, 1.5, name, ha='center', fontsize=9, fontweight='bold')\n",
    "        ax4.text(x+0.25, 2 + height + 0.3, f'{size} GB', ha='center', fontsize=10, fontweight='bold')\n",
    "        ax4.text(x+0.25, 2 + height/2, count, ha='center', va='center', fontsize=8, color='white')\n",
    "        ax4.text(x+0.25, 0.7, note, ha='center', fontsize=8, color='gray')\n",
    "    \n",
    "    # Highlight WebText\n",
    "    ax4.annotate('Best quality-size\\ntradeoff!', xy=(10, 5.5), xytext=(12.5, 6.5),\n",
    "                fontsize=10, fontweight='bold', color='#e74c3c',\n",
    "                arrowprops=dict(arrowstyle='->', color='#e74c3c', lw=2))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_webtext_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Byte-Level BPE: The Tokenization Revolution\n",
    "\n",
    "### 2.1 The Problem with Previous Tokenizers\n",
    "\n",
    "GPT-1 and earlier models used **word-level BPE** with preprocessing:\n",
    "\n",
    "```python\n",
    "# GPT-1 style\n",
    "text = \"Hello caf√©!\"  \n",
    "tokens = [\"hello\", \"caf\", \"[UNK]\", \"!\"]  # √© becomes UNK!\n",
    "```\n",
    "\n",
    "Problems:\n",
    "- Unknown tokens (`[UNK]`) for rare characters\n",
    "- Language-specific preprocessing required\n",
    "- Can't handle code, emoji, or special characters\n",
    "\n",
    "### 2.2 The Byte-Level BPE Solution\n",
    "\n",
    "From the paper:\n",
    "\n",
    "> *\"A base vocabulary of 256 bytes means that any text can be encoded without any unknown tokens.\"*\n",
    "\n",
    "The key insight: **Work at the byte level, not the character level.**\n",
    "\n",
    "### 2.3 How It Works\n",
    "\n",
    "1. **Start with 256 byte tokens** (0x00 to 0xFF)\n",
    "2. **Count all byte pairs** in the training data\n",
    "3. **Merge the most common pair** into a new token\n",
    "4. **Repeat 50,000 times**\n",
    "5. **Add 1 special token** (`<|endoftext|>`)\n",
    "6. **Result: 50,257 tokens**\n",
    "\n",
    "### 2.4 The Vocabulary Composition\n",
    "\n",
    "| Component | Count | Description |\n",
    "|-----------|-------|-------------|\n",
    "| Base bytes | 256 | All possible byte values |\n",
    "| BPE merges | 50,000 | Learned subword units |\n",
    "| Special tokens | 1 | `<|endoftext|>` |\n",
    "| **Total** | **50,257** | Complete vocabulary |\n",
    "\n",
    "### 2.5 Preventing Suboptimal Merges\n",
    "\n",
    "From the paper:\n",
    "\n",
    "> *\"BPE would merge very common words like dog to many versions like dog. dog! dog? ... We prevent BPE from merging across character categories.\"*\n",
    "\n",
    "They added rules to prevent merges across:\n",
    "- Letters and punctuation\n",
    "- Letters and whitespace\n",
    "- Numbers and letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_byte_level_bpe_detailed():\n",
    "    \"\"\"\n",
    "    Detailed visualization of byte-level BPE tokenization.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20, 14))\n",
    "    \n",
    "    gs = gridspec.GridSpec(3, 2, height_ratios=[1.2, 1, 1], hspace=0.35, wspace=0.25)\n",
    "    \n",
    "    # === TOP LEFT: The BPE Algorithm ===\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.set_xlim(0, 12)\n",
    "    ax1.set_ylim(0, 12)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('BPE Merge Algorithm', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    # Show merge iterations\n",
    "    text = \"the cat sat\"\n",
    "    iterations = [\n",
    "        ('Start', list(text), 256),\n",
    "        ('Merge 1: t+h', ['th', 'e', ' ', 'c', 'a', 't', ' ', 's', 'a', 't'], 257),\n",
    "        ('Merge 2: th+e', ['the', ' ', 'c', 'a', 't', ' ', 's', 'a', 't'], 258),\n",
    "        ('Merge 3: a+t', ['the', ' ', 'c', 'at', ' ', 's', 'at'], 259),\n",
    "        ('Merge 4: c+at', ['the', ' ', 'cat', ' ', 's', 'at'], 260),\n",
    "    ]\n",
    "    \n",
    "    for i, (stage, tokens, vocab_size) in enumerate(iterations):\n",
    "        y = 10.5 - i * 2\n",
    "        \n",
    "        # Stage label\n",
    "        ax1.text(0.2, y, stage, fontsize=9, va='center', fontweight='bold')\n",
    "        \n",
    "        # Tokens\n",
    "        x = 3.5\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(tokens)))\n",
    "        for j, (tok, color) in enumerate(zip(tokens, colors)):\n",
    "            width = max(len(tok) * 0.3, 0.4)\n",
    "            rect = FancyBboxPatch((x, y-0.35), width, 0.7, boxstyle=\"round,pad=0.02\",\n",
    "                                  facecolor=color, edgecolor='black', linewidth=1)\n",
    "            ax1.add_patch(rect)\n",
    "            display_tok = tok if tok != ' ' else '‚ê£'\n",
    "            ax1.text(x + width/2, y, display_tok, ha='center', va='center', \n",
    "                    fontsize=8, fontweight='bold')\n",
    "            x += width + 0.1\n",
    "        \n",
    "        # Vocab size\n",
    "        ax1.text(11, y, f'V={vocab_size}', fontsize=9, va='center', color='gray')\n",
    "    \n",
    "    ax1.text(6, 0.3, '...continue for 50,000 merges', fontsize=10, ha='center', \n",
    "             style='italic', color='gray')\n",
    "    \n",
    "    # === TOP RIGHT: Byte vs Character Level ===\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.set_xlim(0, 12)\n",
    "    ax2.set_ylim(0, 12)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('Byte-Level vs Character-Level', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    # Character level problem\n",
    "    rect1 = FancyBboxPatch((0.5, 7), 11, 4, boxstyle=\"round,pad=0.05\",\n",
    "                           facecolor='#fadbd8', edgecolor='#e74c3c', linewidth=2)\n",
    "    ax2.add_patch(rect1)\n",
    "    ax2.text(6, 10.5, 'Character-Level (GPT-1 style)', fontsize=11, ha='center', \n",
    "             fontweight='bold', color='#c0392b')\n",
    "    ax2.text(6, 9.5, 'Input: \"Hello Êó•Êú¨Ë™û üòÄ\"', fontsize=10, ha='center', family='monospace')\n",
    "    ax2.text(6, 8.5, 'Output: [Hello] [Êó•] [Êú¨] [Ë™û] [UNK]', fontsize=10, ha='center', \n",
    "             family='monospace')\n",
    "    ax2.text(6, 7.5, '‚Üí Emoji becomes [UNK]! Information lost.', fontsize=9, ha='center', \n",
    "             color='#c0392b')\n",
    "    \n",
    "    # Byte level solution\n",
    "    rect2 = FancyBboxPatch((0.5, 2), 11, 4, boxstyle=\"round,pad=0.05\",\n",
    "                           facecolor='#d5f4e6', edgecolor='#27ae60', linewidth=2)\n",
    "    ax2.add_patch(rect2)\n",
    "    ax2.text(6, 5.5, 'Byte-Level (GPT-2)', fontsize=11, ha='center', \n",
    "             fontweight='bold', color='#1e8449')\n",
    "    ax2.text(6, 4.5, 'Input: \"Hello Êó•Êú¨Ë™û üòÄ\"', fontsize=10, ha='center', family='monospace')\n",
    "    ax2.text(6, 3.5, 'Output: [Hello] [ƒ†] [√¶][ƒ∫][¬•] ... [√∞≈Åƒ∫][ƒ¢]', fontsize=10, ha='center', \n",
    "             family='monospace')\n",
    "    ax2.text(6, 2.5, '‚Üí Everything encoded! No information loss.', fontsize=9, ha='center', \n",
    "             color='#1e8449')\n",
    "    \n",
    "    ax2.text(6, 0.8, 'Key: 256 bytes can represent ANY text', fontsize=10, ha='center', \n",
    "             fontweight='bold', color='#2c3e50')\n",
    "    \n",
    "    # === MIDDLE: Vocabulary Composition ===\n",
    "    ax3 = fig.add_subplot(gs[1, :])\n",
    "    ax3.set_xlim(0, 20)\n",
    "    ax3.set_ylim(0, 8)\n",
    "    ax3.axis('off')\n",
    "    ax3.set_title('GPT-2 Vocabulary Composition: 50,257 Tokens', fontsize=13, fontweight='bold', pad=10)\n",
    "    \n",
    "    # Visual breakdown (scaled bar)\n",
    "    total_width = 16\n",
    "    start_x = 2\n",
    "    \n",
    "    components = [\n",
    "        ('Base Bytes (256)', 256, '#e74c3c', 'ASCII + extended'),\n",
    "        ('BPE Merges (50,000)', 50000, '#3498db', 'Learned subwords'),\n",
    "        ('<|endoftext|> (1)', 1, '#27ae60', 'Document separator'),\n",
    "    ]\n",
    "    \n",
    "    # Draw as proportional blocks\n",
    "    x = start_x\n",
    "    total = 50257\n",
    "    \n",
    "    for name, count, color, desc in components:\n",
    "        # Width proportional to count (log scale for visibility)\n",
    "        width = max(total_width * np.log(count + 1) / np.log(total + 1), 0.8)\n",
    "        if count == 1:\n",
    "            width = 0.8  # Make special token visible\n",
    "        \n",
    "        rect = FancyBboxPatch((x, 3.5), width, 2.5, boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor=color, edgecolor='black', linewidth=2)\n",
    "        ax3.add_patch(rect)\n",
    "        \n",
    "        ax3.text(x + width/2, 5.5, name.split('(')[0].strip(), ha='center', \n",
    "                fontsize=9, fontweight='bold', color='white')\n",
    "        ax3.text(x + width/2, 4.3, f'{count:,}', ha='center', fontsize=10, color='white')\n",
    "        ax3.text(x + width/2, 2.8, desc, ha='center', fontsize=8, color='gray')\n",
    "        \n",
    "        x += width + 0.2\n",
    "    \n",
    "    # Formula\n",
    "    ax3.text(10, 1, '256 + 50,000 + 1 = 50,257 total tokens', \n",
    "             ha='center', fontsize=12, fontweight='bold', family='monospace')\n",
    "    \n",
    "    # === BOTTOM: Merge Prevention Rules ===\n",
    "    ax4 = fig.add_subplot(gs[2, :])\n",
    "    ax4.set_xlim(0, 20)\n",
    "    ax4.set_ylim(0, 6)\n",
    "    ax4.axis('off')\n",
    "    ax4.set_title('Merge Prevention Rules: Avoiding Suboptimal Tokenization', \n",
    "                  fontsize=13, fontweight='bold', pad=10)\n",
    "    \n",
    "    rules = [\n",
    "        ('Letters + Punctuation', 'dog + ! ‚Üí dog!', 'Keeps \"dog\" reusable', '#e74c3c'),\n",
    "        ('Letters + Whitespace', 'the + ‚ê£ ‚Üí the‚ê£', 'Preserves word boundaries', '#3498db'),\n",
    "        ('Numbers + Letters', '123 + abc ‚Üí 123abc', 'Keeps numbers separate', '#27ae60'),\n",
    "        ('Across Categories', 'Various merges', 'Prevented by regex rules', '#f39c12'),\n",
    "    ]\n",
    "    \n",
    "    for i, (rule, example, benefit, color) in enumerate(rules):\n",
    "        x = 0.5 + i * 5\n",
    "        \n",
    "        rect = FancyBboxPatch((x, 1), 4.5, 4, boxstyle=\"round,pad=0.05\",\n",
    "                              facecolor=color, edgecolor='black', linewidth=2, alpha=0.85)\n",
    "        ax4.add_patch(rect)\n",
    "        \n",
    "        ax4.text(x + 2.25, 4.3, 'BLOCKED', ha='center', fontsize=9, \n",
    "                fontweight='bold', color='white')\n",
    "        ax4.text(x + 2.25, 3.4, rule, ha='center', fontsize=9, color='white')\n",
    "        ax4.text(x + 2.25, 2.5, example, ha='center', fontsize=8, \n",
    "                color='white', family='monospace')\n",
    "        ax4.text(x + 2.25, 1.5, benefit, ha='center', fontsize=7, color='white')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_byte_level_bpe_detailed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBPE:\n",
    "    \"\"\"\n",
    "    Simplified BPE implementation for demonstration.\n",
    "    \n",
    "    This shows the core algorithm used in GPT-2 tokenization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab = {bytes([i]): i for i in range(256)}  # Base bytes\n",
    "        self.merges = {}  # (token1, token2) -> new_token\n",
    "        self.next_id = 256\n",
    "    \n",
    "    def get_pairs(self, tokens: List[bytes]) -> Dict[Tuple[bytes, bytes], int]:\n",
    "        \"\"\"Count all adjacent pairs.\"\"\"\n",
    "        pairs = {}\n",
    "        for i in range(len(tokens) - 1):\n",
    "            pair = (tokens[i], tokens[i + 1])\n",
    "            pairs[pair] = pairs.get(pair, 0) + 1\n",
    "        return pairs\n",
    "    \n",
    "    def merge_pair(self, tokens: List[bytes], pair: Tuple[bytes, bytes]) -> List[bytes]:\n",
    "        \"\"\"Merge all occurrences of a pair.\"\"\"\n",
    "        new_token = pair[0] + pair[1]\n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == pair:\n",
    "                new_tokens.append(new_token)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        return new_tokens\n",
    "    \n",
    "    def train(self, text: str, num_merges: int = 100, verbose: bool = True) -> None:\n",
    "        \"\"\"Train BPE on text.\"\"\"\n",
    "        # Convert to bytes\n",
    "        tokens = [bytes([b]) for b in text.encode('utf-8')]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Starting with {len(set(tokens))} unique byte tokens\")\n",
    "            print(f\"Initial sequence length: {len(tokens)}\")\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "            # Get pair counts\n",
    "            pairs = self.get_pairs(tokens)\n",
    "            if not pairs:\n",
    "                break\n",
    "            \n",
    "            # Find most common pair\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            \n",
    "            # Merge it\n",
    "            new_token = best_pair[0] + best_pair[1]\n",
    "            self.vocab[new_token] = self.next_id\n",
    "            self.merges[best_pair] = new_token\n",
    "            self.next_id += 1\n",
    "            \n",
    "            # Apply merge\n",
    "            tokens = self.merge_pair(tokens, best_pair)\n",
    "            \n",
    "            if verbose and (i + 1) % 20 == 0:\n",
    "                print(f\"Merge {i+1}: {repr(best_pair[0])} + {repr(best_pair[1])} -> \"\n",
    "                      f\"{repr(new_token)} (count: {pairs[best_pair]})\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nFinal vocab size: {len(self.vocab)}\")\n",
    "            print(f\"Final sequence length: {len(tokens)}\")\n",
    "            print(f\"Compression ratio: {len(text.encode('utf-8')) / len(tokens):.2f}x\")\n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Encode text to token IDs.\"\"\"\n",
    "        tokens = [bytes([b]) for b in text.encode('utf-8')]\n",
    "        \n",
    "        # Apply merges in order\n",
    "        for pair, new_token in self.merges.items():\n",
    "            tokens = self.merge_pair(tokens, pair)\n",
    "        \n",
    "        return [self.vocab[t] for t in tokens]\n",
    "\n",
    "\n",
    "# Demonstrate\n",
    "print(\"=\" * 70)\n",
    "print(\"BPE TRAINING DEMONSTRATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sample_text = \"\"\"The cat sat on the mat. The cat was happy. The mat was soft.\n",
    "The happy cat sat on the soft mat. Cats like mats. Mats like cats.\"\"\"\n",
    "\n",
    "bpe = SimpleBPE()\n",
    "bpe.train(sample_text, num_merges=50, verbose=True)\n",
    "\n",
    "# Test encoding\n",
    "test = \"The cat sat\"\n",
    "encoded = bpe.encode(test)\n",
    "print(f\"\\nEncoded '{test}': {encoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Training Infrastructure\n",
    "\n",
    "### 3.1 Hardware Requirements (2019)\n",
    "\n",
    "Training GPT-2 XL required significant compute:\n",
    "\n",
    "| Component | Specification |\n",
    "|-----------|---------------|\n",
    "| GPUs | 8√ó V100 32GB (estimated) |\n",
    "| Memory | 32GB per GPU |\n",
    "| Interconnect | NVLink |\n",
    "| Training Time | ~1 week for XL |\n",
    "\n",
    "### 3.2 Training Configuration\n",
    "\n",
    "From the paper and follow-up releases:\n",
    "\n",
    "| Hyperparameter | Value |\n",
    "|----------------|-------|\n",
    "| Batch Size | 512 sequences |\n",
    "| Sequence Length | 1024 tokens |\n",
    "| Tokens per Batch | 512 √ó 1024 = 524,288 |\n",
    "| Learning Rate | Variable by model size |\n",
    "| Optimizer | Adam |\n",
    "| Weight Decay | 0.01 |\n",
    "| Warmup Steps | Model-dependent |\n",
    "\n",
    "### 3.3 Memory Optimization Techniques\n",
    "\n",
    "To fit large models in GPU memory:\n",
    "\n",
    "1. **Gradient Checkpointing**: Trade compute for memory\n",
    "2. **Mixed Precision (FP16)**: Halve memory usage\n",
    "3. **Gradient Accumulation**: Simulate larger batches\n",
    "4. **Model Parallelism**: Split across GPUs\n",
    "\n",
    "### 3.4 The Batch Size Choice\n",
    "\n",
    "From the paper:\n",
    "\n",
    "> *\"A larger batchsize of 512 is used.\"*\n",
    "\n",
    "Why 512? This is 8√ó larger than GPT-1's batch size of 64:\n",
    "- Better gradient estimates\n",
    "- More stable training\n",
    "- Better hardware utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_infrastructure():\n",
    "    \"\"\"\n",
    "    Visualize the training infrastructure for GPT-2.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20, 14))\n",
    "    \n",
    "    gs = gridspec.GridSpec(2, 2, hspace=0.3, wspace=0.25)\n",
    "    \n",
    "    # === TOP LEFT: Hardware Setup ===\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.set_xlim(0, 12)\n",
    "    ax1.set_ylim(0, 12)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Estimated Training Hardware (2019)', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    # GPU cluster visualization\n",
    "    for i in range(8):\n",
    "        row = i // 4\n",
    "        col = i % 4\n",
    "        x = 1.5 + col * 2.5\n",
    "        y = 7 - row * 3\n",
    "        \n",
    "        rect = FancyBboxPatch((x, y), 2, 2.5, boxstyle=\"round,pad=0.03\",\n",
    "                              facecolor='#27ae60', edgecolor='black', linewidth=2)\n",
    "        ax1.add_patch(rect)\n",
    "        ax1.text(x + 1, y + 1.8, f'GPU {i}', ha='center', fontsize=9, \n",
    "                fontweight='bold', color='white')\n",
    "        ax1.text(x + 1, y + 1.2, 'V100', ha='center', fontsize=8, color='white')\n",
    "        ax1.text(x + 1, y + 0.6, '32GB', ha='center', fontsize=8, color='white')\n",
    "    \n",
    "    # NVLink connections\n",
    "    ax1.text(6, 10.5, '8√ó NVIDIA V100 32GB', ha='center', fontsize=12, fontweight='bold')\n",
    "    ax1.text(6, 3, 'Connected via NVLink', ha='center', fontsize=10, color='gray')\n",
    "    \n",
    "    # Total memory\n",
    "    rect_total = FancyBboxPatch((2, 0.5), 8, 1.5, boxstyle=\"round,pad=0.03\",\n",
    "                                 facecolor='#3498db', edgecolor='black', linewidth=2)\n",
    "    ax1.add_patch(rect_total)\n",
    "    ax1.text(6, 1.25, 'Total GPU Memory: 256 GB', ha='center', \n",
    "             fontsize=11, fontweight='bold', color='white')\n",
    "    \n",
    "    # === TOP RIGHT: Training Configuration ===\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.set_xlim(0, 12)\n",
    "    ax2.set_ylim(0, 12)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('Training Configuration', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    configs = [\n",
    "        ('Batch Size', '512 sequences', '#e74c3c'),\n",
    "        ('Sequence Length', '1024 tokens', '#3498db'),\n",
    "        ('Tokens/Batch', '524,288', '#27ae60'),\n",
    "        ('Optimizer', 'Adam', '#f39c12'),\n",
    "        ('Weight Decay', '0.01', '#9b59b6'),\n",
    "        ('Precision', 'Mixed FP16/FP32', '#1abc9c'),\n",
    "    ]\n",
    "    \n",
    "    for i, (param, value, color) in enumerate(configs):\n",
    "        y = 10.5 - i * 1.7\n",
    "        \n",
    "        rect = FancyBboxPatch((0.5, y-0.5), 5, 1.2, boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor=color, edgecolor='black', linewidth=1.5)\n",
    "        ax2.add_patch(rect)\n",
    "        ax2.text(3, y, param, ha='center', va='center', \n",
    "                fontsize=10, fontweight='bold', color='white')\n",
    "        \n",
    "        rect2 = FancyBboxPatch((6, y-0.5), 5, 1.2, boxstyle=\"round,pad=0.02\",\n",
    "                               facecolor='#f5f5f5', edgecolor=color, linewidth=2)\n",
    "        ax2.add_patch(rect2)\n",
    "        ax2.text(8.5, y, value, ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # === BOTTOM LEFT: Memory Breakdown ===\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    \n",
    "    # Memory usage breakdown for GPT-2 XL\n",
    "    categories = ['Parameters\\n(FP16)', 'Gradients\\n(FP16)', 'Optimizer\\nStates', 'Activations']\n",
    "    sizes = [3.0, 3.0, 12.0, 8.0]  # GB, estimated\n",
    "    colors = ['#e74c3c', '#3498db', '#f39c12', '#27ae60']\n",
    "    \n",
    "    bars = ax3.bar(categories, sizes, color=colors, edgecolor='black', linewidth=2)\n",
    "    \n",
    "    ax3.set_ylabel('Memory (GB)', fontsize=11)\n",
    "    ax3.set_title('Memory Breakdown: GPT-2 XL Training (per GPU)', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylim(0, 15)\n",
    "    \n",
    "    for bar, size in zip(bars, sizes):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
    "                f'{size:.1f} GB', ha='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax3.axhline(y=32, color='gray', linestyle='--', linewidth=2)\n",
    "    ax3.text(3.5, 32.5, 'V100 32GB limit', fontsize=9, color='gray')\n",
    "    \n",
    "    total = sum(sizes)\n",
    "    ax3.text(1.5, 28, f'Total: ~{total:.0f} GB', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # === BOTTOM RIGHT: Training Timeline ===\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    ax4.set_xlim(0, 12)\n",
    "    ax4.set_ylim(0, 10)\n",
    "    ax4.axis('off')\n",
    "    ax4.set_title('Estimated Training Time by Model Size', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    models = [\n",
    "        ('GPT-2 Small', '117M', '~1 day', '#3498db'),\n",
    "        ('GPT-2 Medium', '345M', '~2 days', '#27ae60'),\n",
    "        ('GPT-2 Large', '774M', '~4 days', '#f39c12'),\n",
    "        ('GPT-2 XL', '1.5B', '~7 days', '#e74c3c'),\n",
    "    ]\n",
    "    \n",
    "    for i, (name, params, time, color) in enumerate(models):\n",
    "        y = 8 - i * 2\n",
    "        \n",
    "        # Model box\n",
    "        rect = FancyBboxPatch((0.5, y-0.5), 4, 1.3, boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor=color, edgecolor='black', linewidth=2)\n",
    "        ax4.add_patch(rect)\n",
    "        ax4.text(2.5, y+0.2, name, ha='center', fontsize=10, fontweight='bold', color='white')\n",
    "        ax4.text(2.5, y-0.3, params, ha='center', fontsize=9, color='white')\n",
    "        \n",
    "        # Time bar\n",
    "        days = float(time.split('~')[1].split()[0])\n",
    "        bar_width = days * 0.8\n",
    "        rect2 = FancyBboxPatch((5, y-0.35), bar_width, 0.9, boxstyle=\"round,pad=0.02\",\n",
    "                               facecolor=color, edgecolor='black', linewidth=1, alpha=0.6)\n",
    "        ax4.add_patch(rect2)\n",
    "        ax4.text(5 + bar_width + 0.3, y, time, va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax4.text(6, 0.5, '(On 8√ó V100 GPUs with optimizations)', ha='center', \n",
    "             fontsize=10, color='gray', style='italic')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_training_infrastructure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Training Dynamics\n",
    "\n",
    "### 4.1 Learning Rate Schedule\n",
    "\n",
    "GPT-2 uses a **cosine learning rate schedule with warmup**:\n",
    "\n",
    "1. **Warmup phase**: Linear increase from 0 to peak LR\n",
    "2. **Decay phase**: Cosine decay to ~0\n",
    "\n",
    "### 4.2 Learning Rates by Model Size\n",
    "\n",
    "Larger models typically use smaller learning rates:\n",
    "\n",
    "| Model | Peak Learning Rate |\n",
    "|-------|-------------------|\n",
    "| Small (117M) | 2.5e-4 |\n",
    "| Medium (345M) | 2.5e-4 |\n",
    "| Large (774M) | 1.5e-4 |\n",
    "| XL (1.5B) | 1.0e-4 |\n",
    "\n",
    "### 4.3 Loss Curves\n",
    "\n",
    "What we expect during training:\n",
    "\n",
    "1. **Rapid initial drop**: Model learns basic patterns\n",
    "2. **Gradual improvement**: Learns finer patterns\n",
    "3. **Plateau**: Approaches dataset entropy\n",
    "\n",
    "### 4.4 Overfitting Considerations\n",
    "\n",
    "With 10B tokens and 1.5B parameters:\n",
    "- Ratio: ~6.7 tokens per parameter\n",
    "- This is actually **undertrained** by modern standards\n",
    "- Chinchilla (2022) suggests 20 tokens per parameter optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_dynamics():\n",
    "    \"\"\"\n",
    "    Visualize training dynamics including learning rate and loss curves.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    gs = gridspec.GridSpec(2, 2, hspace=0.3, wspace=0.25)\n",
    "    \n",
    "    # === TOP LEFT: Learning Rate Schedule ===\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    \n",
    "    # Cosine schedule with warmup\n",
    "    total_steps = 100000\n",
    "    warmup_steps = 2000\n",
    "    peak_lr = 2.5e-4\n",
    "    \n",
    "    steps = np.arange(total_steps)\n",
    "    lr = np.zeros(total_steps)\n",
    "    \n",
    "    # Warmup\n",
    "    lr[:warmup_steps] = peak_lr * steps[:warmup_steps] / warmup_steps\n",
    "    \n",
    "    # Cosine decay\n",
    "    decay_steps = total_steps - warmup_steps\n",
    "    lr[warmup_steps:] = peak_lr * 0.5 * (1 + np.cos(np.pi * np.arange(decay_steps) / decay_steps))\n",
    "    \n",
    "    ax1.plot(steps / 1000, lr * 1e4, color='#3498db', linewidth=2.5)\n",
    "    ax1.fill_between(steps / 1000, 0, lr * 1e4, alpha=0.3, color='#3498db')\n",
    "    \n",
    "    ax1.axvline(x=warmup_steps/1000, color='#e74c3c', linestyle='--', linewidth=2)\n",
    "    ax1.text(warmup_steps/1000 + 2, peak_lr * 1e4 * 0.9, 'Warmup\\nends', \n",
    "             fontsize=9, color='#e74c3c')\n",
    "    \n",
    "    ax1.set_xlabel('Training Steps (thousands)', fontsize=11)\n",
    "    ax1.set_ylabel('Learning Rate (√ó10‚Åª‚Å¥)', fontsize=11)\n",
    "    ax1.set_title('Learning Rate Schedule: Cosine with Warmup', fontsize=12, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(0, 100)\n",
    "    \n",
    "    # === TOP RIGHT: Simulated Loss Curve ===\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    \n",
    "    # Simulate loss curves for different model sizes\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    def simulate_loss(steps, final_loss, noise_scale=0.02):\n",
    "        \"\"\"Simulate a realistic loss curve.\"\"\"\n",
    "        initial_loss = np.log(50257)  # ~10.8 (random baseline)\n",
    "        decay = np.exp(-steps / 20000)\n",
    "        base_loss = final_loss + (initial_loss - final_loss) * decay\n",
    "        noise = np.random.randn(len(steps)) * noise_scale * decay\n",
    "        return base_loss + noise\n",
    "    \n",
    "    steps = np.linspace(0, 100000, 500)\n",
    "    \n",
    "    models_loss = [\n",
    "        ('GPT-2 Small', 4.5, '#3498db'),\n",
    "        ('GPT-2 Medium', 3.8, '#27ae60'),\n",
    "        ('GPT-2 Large', 3.3, '#f39c12'),\n",
    "        ('GPT-2 XL', 3.0, '#e74c3c'),\n",
    "    ]\n",
    "    \n",
    "    for name, final_loss, color in models_loss:\n",
    "        loss = simulate_loss(steps, final_loss)\n",
    "        ax2.plot(steps / 1000, loss, linewidth=2, color=color, label=name, alpha=0.8)\n",
    "    \n",
    "    ax2.set_xlabel('Training Steps (thousands)', fontsize=11)\n",
    "    ax2.set_ylabel('Training Loss (nats)', fontsize=11)\n",
    "    ax2.set_title('Training Loss by Model Size (Simulated)', fontsize=12, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xlim(0, 100)\n",
    "    ax2.set_ylim(2, 11)\n",
    "    \n",
    "    # Random baseline\n",
    "    ax2.axhline(y=np.log(50257), color='gray', linestyle='--', linewidth=1.5)\n",
    "    ax2.text(80, np.log(50257) + 0.2, 'Random baseline', fontsize=9, color='gray')\n",
    "    \n",
    "    # === BOTTOM LEFT: Tokens per Parameter ===\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    \n",
    "    models = ['Small', 'Medium', 'Large', 'XL']\n",
    "    params = [117, 345, 774, 1500]  # Millions\n",
    "    tokens = 10000  # Millions (10B)\n",
    "    \n",
    "    ratios = [tokens / p for p in params]\n",
    "    colors = ['#3498db', '#27ae60', '#f39c12', '#e74c3c']\n",
    "    \n",
    "    bars = ax3.bar(models, ratios, color=colors, edgecolor='black', linewidth=2)\n",
    "    \n",
    "    ax3.axhline(y=20, color='#9b59b6', linestyle='--', linewidth=2, label='Chinchilla optimal (20:1)')\n",
    "    \n",
    "    ax3.set_ylabel('Tokens per Parameter', fontsize=11)\n",
    "    ax3.set_title('Data-to-Parameter Ratio', fontsize=12, fontweight='bold')\n",
    "    ax3.legend(fontsize=10)\n",
    "    \n",
    "    for bar, ratio in zip(bars, ratios):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "                f'{ratio:.1f}:1', ha='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax3.text(1.5, 75, 'GPT-2 was undertrained!', fontsize=11, fontweight='bold', color='#e74c3c')\n",
    "    ax3.text(1.5, 68, '(By modern standards)', fontsize=10, color='gray')\n",
    "    \n",
    "    # === BOTTOM RIGHT: Learning Rate by Model Size ===\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    ax4.set_xlim(0, 12)\n",
    "    ax4.set_ylim(0, 10)\n",
    "    ax4.axis('off')\n",
    "    ax4.set_title('Learning Rate Selection by Model Size', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    lr_data = [\n",
    "        ('GPT-2 Small', '117M', '2.5e-4', '#3498db'),\n",
    "        ('GPT-2 Medium', '345M', '2.5e-4', '#27ae60'),\n",
    "        ('GPT-2 Large', '774M', '1.5e-4', '#f39c12'),\n",
    "        ('GPT-2 XL', '1.5B', '1.0e-4', '#e74c3c'),\n",
    "    ]\n",
    "    \n",
    "    for i, (name, params, lr_val, color) in enumerate(lr_data):\n",
    "        y = 8 - i * 2\n",
    "        \n",
    "        rect = FancyBboxPatch((0.5, y-0.5), 3, 1.3, boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor=color, edgecolor='black', linewidth=2)\n",
    "        ax4.add_patch(rect)\n",
    "        ax4.text(2, y, name, ha='center', va='center', \n",
    "                fontsize=10, fontweight='bold', color='white')\n",
    "        \n",
    "        ax4.text(4.5, y, params, va='center', fontsize=10)\n",
    "        \n",
    "        # LR bar\n",
    "        lr_float = float(lr_val)\n",
    "        bar_width = lr_float / 2.5e-4 * 4  # Normalized\n",
    "        rect2 = FancyBboxPatch((6, y-0.35), bar_width, 0.7, boxstyle=\"round,pad=0.02\",\n",
    "                               facecolor=color, edgecolor='black', linewidth=1, alpha=0.6)\n",
    "        ax4.add_patch(rect2)\n",
    "        ax4.text(6 + bar_width + 0.3, y, lr_val, va='center', fontsize=10, family='monospace')\n",
    "    \n",
    "    ax4.text(6, 0.5, 'Larger models ‚Üí Smaller learning rates', ha='center', \n",
    "             fontsize=11, fontweight='bold', color='#2c3e50')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_training_dynamics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Scaling Laws: The Discovery That Changed AI\n",
    "\n",
    "### 5.1 What GPT-2 Revealed\n",
    "\n",
    "The paper showed a remarkable finding:\n",
    "\n",
    "> *\"Performance on many NLP tasks continues to improve with increasing model capacity in a log-linear fashion.\"*\n",
    "\n",
    "### 5.2 The Power Law Relationship\n",
    "\n",
    "Loss scales as a power law with:\n",
    "\n",
    "$$L(N) = \\left(\\frac{N_c}{N}\\right)^{\\alpha_N}$$\n",
    "\n",
    "Where:\n",
    "- $L$ = Loss\n",
    "- $N$ = Number of parameters\n",
    "- $N_c$ = Critical parameter count\n",
    "- $\\alpha_N$ ‚âà 0.076 (from later Kaplan et al. 2020)\n",
    "\n",
    "### 5.3 The Three Scaling Dimensions\n",
    "\n",
    "Later work (Kaplan et al. 2020) formalized three scaling dimensions:\n",
    "\n",
    "1. **Parameters (N)**: Model size\n",
    "2. **Data (D)**: Training tokens\n",
    "3. **Compute (C)**: FLOPs\n",
    "\n",
    "All follow power laws!\n",
    "\n",
    "### 5.4 Why This Matters\n",
    "\n",
    "If performance is predictable:\n",
    "- Can estimate results before training\n",
    "- Can plan compute budgets\n",
    "- Can justify large investments\n",
    "- No \"plateau\" means scaling continues to work\n",
    "\n",
    "This insight directly led to GPT-3 and the modern LLM era."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_scaling_laws():\n",
    "    \"\"\"\n",
    "    Comprehensive visualization of scaling laws.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20, 14))\n",
    "    \n",
    "    gs = gridspec.GridSpec(2, 2, hspace=0.3, wspace=0.25)\n",
    "    \n",
    "    # === TOP LEFT: Loss vs Parameters (Linear) ===\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    \n",
    "    # GPT-2 data\n",
    "    params = np.array([117, 345, 774, 1500])  # Millions\n",
    "    perplexity = np.array([37.50, 26.37, 22.05, 18.34])  # WikiText-103\n",
    "    loss = np.log(perplexity)  # Convert to nats\n",
    "    \n",
    "    ax1.plot(params, loss, 'o-', color='#e74c3c', linewidth=2.5, markersize=12,\n",
    "             markerfacecolor='white', markeredgewidth=2)\n",
    "    \n",
    "    for p, l in zip(params, loss):\n",
    "        ax1.annotate(f'{np.exp(l):.1f}', xy=(p, l), xytext=(p+50, l+0.1),\n",
    "                    fontsize=9, fontweight='bold')\n",
    "    \n",
    "    ax1.set_xlabel('Parameters (Millions)', fontsize=11)\n",
    "    ax1.set_ylabel('Loss (nats)', fontsize=11)\n",
    "    ax1.set_title('Loss vs Parameters (Linear Scale)', fontsize=12, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(0, 1700)\n",
    "    \n",
    "    # === TOP RIGHT: Loss vs Parameters (Log-Log) ===\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    \n",
    "    ax2.loglog(params, loss, 'o', color='#e74c3c', markersize=12,\n",
    "               markerfacecolor='white', markeredgewidth=2)\n",
    "    \n",
    "    # Power law fit\n",
    "    log_params = np.log10(params)\n",
    "    log_loss = np.log10(loss)\n",
    "    coeffs = np.polyfit(log_params, log_loss, 1)\n",
    "    \n",
    "    fit_params = np.logspace(1.5, 6, 100)  # 30M to 1T\n",
    "    fit_loss = 10 ** (coeffs[0] * np.log10(fit_params) + coeffs[1])\n",
    "    \n",
    "    ax2.loglog(fit_params, fit_loss, '--', color='#3498db', linewidth=2,\n",
    "               label=f'Power law: L ‚àù N^{{{coeffs[0]:.3f}}}')\n",
    "    \n",
    "    # Extrapolation points\n",
    "    future_models = [\n",
    "        (175000, 'GPT-3', '#27ae60'),\n",
    "        (540000, 'PaLM', '#9b59b6'),\n",
    "    ]\n",
    "    \n",
    "    for size, name, color in future_models:\n",
    "        pred_loss = 10 ** (coeffs[0] * np.log10(size) + coeffs[1])\n",
    "        ax2.scatter([size], [pred_loss], color=color, s=150, marker='*', zorder=5)\n",
    "        ax2.annotate(name, xy=(size, pred_loss), xytext=(size*1.5, pred_loss*1.1),\n",
    "                    fontsize=9, color=color, fontweight='bold')\n",
    "    \n",
    "    ax2.set_xlabel('Parameters (Millions)', fontsize=11)\n",
    "    ax2.set_ylabel('Loss (nats)', fontsize=11)\n",
    "    ax2.set_title('Log-Log Scale: THE POWER LAW!', fontsize=12, fontweight='bold', color='#27ae60')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # === BOTTOM LEFT: Three Scaling Dimensions ===\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    ax3.set_xlim(0, 12)\n",
    "    ax3.set_ylim(0, 10)\n",
    "    ax3.axis('off')\n",
    "    ax3.set_title('The Three Scaling Dimensions', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    dimensions = [\n",
    "        ('N', 'Parameters', 'Model size\\n(weights)', '#e74c3c', 'L ‚àù N^{-0.076}'),\n",
    "        ('D', 'Data', 'Training tokens\\n(examples)', '#3498db', 'L ‚àù D^{-0.095}'),\n",
    "        ('C', 'Compute', 'FLOPs\\n(operations)', '#27ae60', 'L ‚àù C^{-0.050}'),\n",
    "    ]\n",
    "    \n",
    "    for i, (symbol, name, desc, color, formula) in enumerate(dimensions):\n",
    "        x = 1 + i * 4\n",
    "        \n",
    "        # Main circle\n",
    "        circle = plt.Circle((x + 1.5, 6), 1.3, facecolor=color, edgecolor='black', linewidth=2)\n",
    "        ax3.add_patch(circle)\n",
    "        ax3.text(x + 1.5, 6.3, symbol, ha='center', va='center', \n",
    "                fontsize=20, fontweight='bold', color='white')\n",
    "        ax3.text(x + 1.5, 5.5, name, ha='center', va='center', \n",
    "                fontsize=10, color='white')\n",
    "        \n",
    "        # Description\n",
    "        ax3.text(x + 1.5, 4, desc, ha='center', va='center', fontsize=9, color='gray')\n",
    "        \n",
    "        # Formula\n",
    "        rect = FancyBboxPatch((x, 1.5), 3, 1, boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor='#f5f5f5', edgecolor=color, linewidth=2)\n",
    "        ax3.add_patch(rect)\n",
    "        ax3.text(x + 1.5, 2, formula, ha='center', va='center', \n",
    "                fontsize=10, family='monospace')\n",
    "    \n",
    "    ax3.text(6, 0.3, 'All three follow power laws! (Kaplan et al. 2020)', \n",
    "             ha='center', fontsize=11, fontweight='bold', color='#2c3e50')\n",
    "    \n",
    "    # === BOTTOM RIGHT: Why It Matters ===\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    ax4.set_xlim(0, 12)\n",
    "    ax4.set_ylim(0, 10)\n",
    "    ax4.axis('off')\n",
    "    ax4.set_title('Why Scaling Laws Changed AI', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    implications = [\n",
    "        ('Predictable', 'Can forecast performance\\nbefore training', '#e74c3c'),\n",
    "        ('No Plateau', 'Scaling continues to work\\n(no ceiling in sight)', '#3498db'),\n",
    "        ('Investment\\nJustification', '10√ó compute ‚Üí\\nmeasurable improvement', '#27ae60'),\n",
    "        ('Research\\nDirection', 'Scale > Architecture\\n(for now)', '#f39c12'),\n",
    "    ]\n",
    "    \n",
    "    for i, (title, desc, color) in enumerate(implications):\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "        x = 0.5 + col * 6\n",
    "        y = 6.5 - row * 4\n",
    "        \n",
    "        rect = FancyBboxPatch((x, y), 5, 3, boxstyle=\"round,pad=0.05\",\n",
    "                              facecolor=color, edgecolor='black', linewidth=2, alpha=0.85)\n",
    "        ax4.add_patch(rect)\n",
    "        \n",
    "        ax4.text(x + 2.5, y + 2.2, title, ha='center', va='center', \n",
    "                fontsize=10, fontweight='bold', color='white')\n",
    "        ax4.text(x + 2.5, y + 1, desc, ha='center', va='center', \n",
    "                fontsize=9, color='white')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_scaling_laws()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Compute Analysis\n",
    "\n",
    "### 6.1 FLOPs Calculation\n",
    "\n",
    "For a transformer forward pass:\n",
    "\n",
    "$$\\text{FLOPs} \\approx 6 \\times N \\times D$$\n",
    "\n",
    "Where:\n",
    "- N = Number of parameters\n",
    "- D = Number of tokens processed\n",
    "- 6 = Forward (2) + Backward (4) passes\n",
    "\n",
    "### 6.2 GPT-2 XL Training Compute\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Parameters | 1.5B |\n",
    "| Training tokens | ~10B |\n",
    "| Total FLOPs | ~9 √ó 10¬π‚Åπ |\n",
    "| V100 peak | 125 TFLOPs |\n",
    "| Utilization | ~30-50% |\n",
    "| Effective | ~50 TFLOPs |\n",
    "| GPU-hours | ~500-1000 |\n",
    "\n",
    "### 6.3 Cost Estimation (2019 Prices)\n",
    "\n",
    "| Resource | Cost |\n",
    "|----------|------|\n",
    "| V100 GPU (cloud) | ~$3/hour |\n",
    "| 8 GPUs √ó 168 hours | ~$4,000 |\n",
    "| With overhead | ~$10,000-50,000 |\n",
    "\n",
    "### 6.4 Comparison to GPT-3\n",
    "\n",
    "| Model | Parameters | Compute | Estimated Cost |\n",
    "|-------|------------|---------|----------------|\n",
    "| GPT-2 XL | 1.5B | 9√ó10¬π‚Åπ | ~$50K |\n",
    "| GPT-3 | 175B | 3.1√ó10¬≤¬≥ | ~$5-12M |\n",
    "\n",
    "GPT-3 required **~3,400√ó more compute**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_training_flops(params_millions: float, tokens_billions: float) -> float:\n",
    "    \"\"\"\n",
    "    Estimate training FLOPs using the 6ND approximation.\n",
    "    \n",
    "    Args:\n",
    "        params_millions: Model parameters in millions\n",
    "        tokens_billions: Training tokens in billions\n",
    "        \n",
    "    Returns:\n",
    "        Total FLOPs\n",
    "    \"\"\"\n",
    "    N = params_millions * 1e6\n",
    "    D = tokens_billions * 1e9\n",
    "    return 6 * N * D\n",
    "\n",
    "\n",
    "def flops_to_gpu_hours(flops: float, gpu_tflops: float = 50.0) -> float:\n",
    "    \"\"\"\n",
    "    Convert FLOPs to GPU-hours.\n",
    "    \n",
    "    Args:\n",
    "        flops: Total FLOPs\n",
    "        gpu_tflops: Effective GPU throughput in TFLOPs\n",
    "        \n",
    "    Returns:\n",
    "        GPU-hours required\n",
    "    \"\"\"\n",
    "    flops_per_hour = gpu_tflops * 1e12 * 3600\n",
    "    return flops / flops_per_hour\n",
    "\n",
    "\n",
    "def print_compute_analysis():\n",
    "    \"\"\"Print detailed compute analysis.\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"GPT-2 COMPUTE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    models = [\n",
    "        ('GPT-2 Small', 117, 10),\n",
    "        ('GPT-2 Medium', 345, 10),\n",
    "        ('GPT-2 Large', 774, 10),\n",
    "        ('GPT-2 XL', 1500, 10),\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n{'Model':<18} {'Params':<10} {'Tokens':<10} {'FLOPs':<15} {'GPU-hrs':<12} {'Cost (est)'}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for name, params, tokens in models:\n",
    "        flops = compute_training_flops(params, tokens)\n",
    "        gpu_hours = flops_to_gpu_hours(flops)\n",
    "        cost = gpu_hours * 3  # $3/GPU-hour estimate\n",
    "        \n",
    "        print(f\"{name:<18} {params}M{'':<6} {tokens}B{'':<6} {flops:.2e}   \"\n",
    "              f\"{gpu_hours:,.0f}        ${cost:,.0f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"COMPARISON TO LATER MODELS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    comparison = [\n",
    "        ('GPT-2 XL', 1500, 10, 50000),\n",
    "        ('GPT-3', 175000, 300, 5000000),\n",
    "        ('GPT-4 (est)', 1000000, 2000, 100000000),\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n{'Model':<15} {'Params':<12} {'Tokens':<12} {'FLOPs':<18} {'Cost (est)'}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for name, params, tokens, cost in comparison:\n",
    "        flops = compute_training_flops(params, tokens)\n",
    "        print(f\"{name:<15} {params/1000:.0f}B{'':<8} {tokens}B{'':<8} {flops:.2e}     ${cost:,}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"KEY FORMULAS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nTraining FLOPs ‚âà 6 √ó N √ó D\")\n",
    "    print(\"  N = parameters, D = tokens\")\n",
    "    print(\"  6 = 2 (forward) + 4 (backward)\")\n",
    "    print(\"\\nGPU-hours = FLOPs / (GPU_TFLOPS √ó 3600 √ó efficiency)\")\n",
    "    print(\"  Typical efficiency: 30-50%\")\n",
    "\n",
    "print_compute_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_compute_analysis():\n",
    "    \"\"\"\n",
    "    Visualize compute requirements and costs.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    gs = gridspec.GridSpec(2, 2, hspace=0.3, wspace=0.25)\n",
    "    \n",
    "    # === TOP LEFT: FLOPs by Model Size ===\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    \n",
    "    models = ['Small\\n117M', 'Medium\\n345M', 'Large\\n774M', 'XL\\n1.5B']\n",
    "    flops = [compute_training_flops(117, 10),\n",
    "             compute_training_flops(345, 10),\n",
    "             compute_training_flops(774, 10),\n",
    "             compute_training_flops(1500, 10)]\n",
    "    \n",
    "    colors = plt.cm.Reds(np.linspace(0.3, 0.8, 4))\n",
    "    bars = ax1.bar(models, [f/1e18 for f in flops], color=colors, edgecolor='black', linewidth=2)\n",
    "    \n",
    "    ax1.set_ylabel('FLOPs (√ó10¬π‚Å∏)', fontsize=11)\n",
    "    ax1.set_title('Training Compute by Model Size', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    for bar, f in zip(bars, flops):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "                f'{f:.1e}', ha='center', fontsize=9, rotation=45)\n",
    "    \n",
    "    # === TOP RIGHT: GPU Hours ===\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    \n",
    "    gpu_hours = [flops_to_gpu_hours(f) for f in flops]\n",
    "    \n",
    "    bars2 = ax2.bar(models, gpu_hours, color=colors, edgecolor='black', linewidth=2)\n",
    "    \n",
    "    ax2.set_ylabel('GPU-hours (V100 @ 50 TFLOPS)', fontsize=11)\n",
    "    ax2.set_title('Training Time by Model Size', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    for bar, h in zip(bars2, gpu_hours):\n",
    "        days = h / 24 / 8  # Assuming 8 GPUs\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20,\n",
    "                f'{h:.0f}h\\n({days:.1f}d @ 8 GPU)', ha='center', fontsize=8)\n",
    "    \n",
    "    # === BOTTOM LEFT: Cost Comparison ===\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    \n",
    "    model_costs = [\n",
    "        ('GPT-2 XL', 50000, '#3498db'),\n",
    "        ('GPT-3', 5000000, '#e74c3c'),\n",
    "        ('GPT-4 (est)', 100000000, '#27ae60'),\n",
    "    ]\n",
    "    \n",
    "    names = [m[0] for m in model_costs]\n",
    "    costs = [m[1] for m in model_costs]\n",
    "    colors_cost = [m[2] for m in model_costs]\n",
    "    \n",
    "    bars3 = ax3.barh(names, costs, color=colors_cost, edgecolor='black', linewidth=2)\n",
    "    ax3.set_xscale('log')\n",
    "    ax3.set_xlabel('Estimated Training Cost ($)', fontsize=11)\n",
    "    ax3.set_title('Training Cost Comparison (Log Scale)', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    for bar, cost in zip(bars3, costs):\n",
    "        ax3.text(cost * 1.5, bar.get_y() + bar.get_height()/2,\n",
    "                f'${cost:,.0f}', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # === BOTTOM RIGHT: Compute Formula ===\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    ax4.set_xlim(0, 12)\n",
    "    ax4.set_ylim(0, 10)\n",
    "    ax4.axis('off')\n",
    "    ax4.set_title('The Compute Formula', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    # Main formula\n",
    "    rect = FancyBboxPatch((1, 6), 10, 3, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor='#e8f6f3', edgecolor='#1abc9c', linewidth=2)\n",
    "    ax4.add_patch(rect)\n",
    "    ax4.text(6, 8.2, 'Training FLOPs Formula', fontsize=12, ha='center', fontweight='bold')\n",
    "    ax4.text(6, 7, 'C ‚âà 6 √ó N √ó D', fontsize=16, ha='center', family='monospace', fontweight='bold')\n",
    "    ax4.text(6, 6.3, 'C=compute, N=params, D=tokens', fontsize=9, ha='center', color='gray')\n",
    "    \n",
    "    # Breakdown\n",
    "    breakdown = [\n",
    "        ('6 =', 'Forward (2) + Backward (4)', '#e74c3c'),\n",
    "        ('N =', 'Model parameters', '#3498db'),\n",
    "        ('D =', 'Training tokens', '#27ae60'),\n",
    "    ]\n",
    "    \n",
    "    for i, (symbol, desc, color) in enumerate(breakdown):\n",
    "        y = 4.5 - i * 1.3\n",
    "        \n",
    "        rect = FancyBboxPatch((1.5, y-0.4), 1.5, 0.8, boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor=color, edgecolor='black', linewidth=1.5)\n",
    "        ax4.add_patch(rect)\n",
    "        ax4.text(2.25, y, symbol, ha='center', va='center', \n",
    "                fontsize=10, fontweight='bold', color='white')\n",
    "        ax4.text(3.5, y, desc, va='center', fontsize=10)\n",
    "    \n",
    "    # GPT-2 XL example\n",
    "    ax4.text(6, 0.5, 'GPT-2 XL: 6 √ó 1.5B √ó 10B ‚âà 9√ó10¬π‚Åπ FLOPs', \n",
    "             ha='center', fontsize=10, family='monospace')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_compute_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Reproducibility Guide\n",
    "\n",
    "### 7.1 Can You Train GPT-2 Today?\n",
    "\n",
    "**Yes!** With modern hardware, GPT-2 is very trainable:\n",
    "\n",
    "| Model | Minimum GPU | Training Time |\n",
    "|-------|-------------|---------------|\n",
    "| Small (117M) | 1√ó RTX 3090 | ~12 hours |\n",
    "| Medium (345M) | 1√ó A100 40GB | ~24 hours |\n",
    "| Large (774M) | 2√ó A100 40GB | ~3 days |\n",
    "| XL (1.5B) | 4√ó A100 80GB | ~5 days |\n",
    "\n",
    "### 7.2 Open-Source Implementations\n",
    "\n",
    "1. **nanoGPT** (Karpathy): Clean, minimal implementation\n",
    "2. **Hugging Face**: Full-featured transformers library\n",
    "3. **Megatron-LM**: NVIDIA's distributed training\n",
    "\n",
    "### 7.3 Key Training Tips\n",
    "\n",
    "1. **Use mixed precision (FP16/BF16)**: 2√ó memory savings\n",
    "2. **Gradient checkpointing**: Trade compute for memory\n",
    "3. **Flash Attention**: 2-4√ó speedup on attention\n",
    "4. **Gradient accumulation**: Simulate larger batches\n",
    "5. **Learning rate warmup**: Critical for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reproducibility_guide():\n",
    "    \"\"\"\n",
    "    Guide for reproducing GPT-2 training.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(18, 14))\n",
    "    \n",
    "    gs = gridspec.GridSpec(2, 2, hspace=0.3, wspace=0.25)\n",
    "    \n",
    "    # === TOP LEFT: Hardware Requirements ===\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.set_xlim(0, 12)\n",
    "    ax1.set_ylim(0, 12)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Hardware Requirements (2024)', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    requirements = [\n",
    "        ('GPT-2 Small', '117M', '1√ó RTX 3090', '12GB', '~12 hrs', '#3498db'),\n",
    "        ('GPT-2 Medium', '345M', '1√ó A100 40GB', '24GB', '~24 hrs', '#27ae60'),\n",
    "        ('GPT-2 Large', '774M', '2√ó A100 40GB', '48GB', '~3 days', '#f39c12'),\n",
    "        ('GPT-2 XL', '1.5B', '4√ó A100 80GB', '160GB', '~5 days', '#e74c3c'),\n",
    "    ]\n",
    "    \n",
    "    headers = ['Model', 'Params', 'GPU', 'VRAM', 'Time']\n",
    "    \n",
    "    # Header\n",
    "    for j, header in enumerate(headers):\n",
    "        x = 0.5 + j * 2.2\n",
    "        rect = FancyBboxPatch((x, 10), 2, 1, boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor='#2c3e50', edgecolor='black', linewidth=1)\n",
    "        ax1.add_patch(rect)\n",
    "        ax1.text(x + 1, 10.5, header, ha='center', va='center', \n",
    "                fontsize=9, fontweight='bold', color='white')\n",
    "    \n",
    "    # Rows\n",
    "    for i, (model, params, gpu, vram, time, color) in enumerate(requirements):\n",
    "        y = 8.5 - i * 2\n",
    "        values = [model, params, gpu, vram, time]\n",
    "        \n",
    "        for j, val in enumerate(values):\n",
    "            x = 0.5 + j * 2.2\n",
    "            facecolor = color if j == 0 else '#f5f5f5'\n",
    "            textcolor = 'white' if j == 0 else 'black'\n",
    "            \n",
    "            rect = FancyBboxPatch((x, y), 2, 1.5, boxstyle=\"round,pad=0.02\",\n",
    "                                  facecolor=facecolor, edgecolor='gray', linewidth=1)\n",
    "            ax1.add_patch(rect)\n",
    "            ax1.text(x + 1, y + 0.75, val, ha='center', va='center', \n",
    "                    fontsize=8, color=textcolor, fontweight='bold' if j == 0 else 'normal')\n",
    "    \n",
    "    # === TOP RIGHT: Training Tips ===\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.set_xlim(0, 12)\n",
    "    ax2.set_ylim(0, 12)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('Essential Training Tips', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    tips = [\n",
    "        ('Mixed Precision', 'FP16/BF16', '2√ó memory savings', '#e74c3c'),\n",
    "        ('Flash Attention', 'Fused kernels', '2-4√ó speedup', '#3498db'),\n",
    "        ('Grad Checkpoint', 'Recompute activs', '3√ó less memory', '#27ae60'),\n",
    "        ('Grad Accumulation', 'Virtual batches', 'Simulate large batch', '#f39c12'),\n",
    "        ('LR Warmup', 'Linear ramp', 'Training stability', '#9b59b6'),\n",
    "    ]\n",
    "    \n",
    "    for i, (name, technique, benefit, color) in enumerate(tips):\n",
    "        y = 10.5 - i * 2\n",
    "        \n",
    "        rect = FancyBboxPatch((0.5, y-0.6), 3, 1.4, boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor=color, edgecolor='black', linewidth=2)\n",
    "        ax2.add_patch(rect)\n",
    "        ax2.text(2, y+0.2, name, ha='center', va='center', \n",
    "                fontsize=9, fontweight='bold', color='white')\n",
    "        ax2.text(2, y-0.3, technique, ha='center', va='center', \n",
    "                fontsize=8, color='white')\n",
    "        \n",
    "        ax2.text(4, y, f'‚Üí {benefit}', va='center', fontsize=9)\n",
    "    \n",
    "    # === BOTTOM: Training Code Example ===\n",
    "    ax3 = fig.add_subplot(gs[1, :])\n",
    "    ax3.set_xlim(0, 20)\n",
    "    ax3.set_ylim(0, 10)\n",
    "    ax3.axis('off')\n",
    "    ax3.set_title('Training Configuration Example (nanoGPT Style)', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    # Code box\n",
    "    rect = FancyBboxPatch((1, 1), 18, 8, boxstyle=\"round,pad=0.05\",\n",
    "                          facecolor='#1e1e1e', edgecolor='#3498db', linewidth=2)\n",
    "    ax3.add_patch(rect)\n",
    "    \n",
    "    code = \"\"\"# GPT-2 Small Training Configuration\n",
    "config = {\n",
    "    'n_layer': 12,\n",
    "    'n_head': 12,\n",
    "    'n_embd': 768,\n",
    "    'vocab_size': 50257,\n",
    "    'block_size': 1024,      # Context length\n",
    "    'batch_size': 12,        # Per GPU\n",
    "    'gradient_accumulation_steps': 40,  # Effective batch = 480\n",
    "    'learning_rate': 6e-4,\n",
    "    'warmup_iters': 2000,\n",
    "    'lr_decay_iters': 600000,\n",
    "    'weight_decay': 0.1,\n",
    "    'dtype': 'bfloat16',     # Mixed precision\n",
    "}\"\"\"\n",
    "    \n",
    "    lines = code.split('\\n')\n",
    "    for i, line in enumerate(lines):\n",
    "        color = '#f39c12' if line.strip().startswith('#') else '#d4d4d4'\n",
    "        if '=' in line and not line.strip().startswith('#'):\n",
    "            parts = line.split('=')\n",
    "            ax3.text(1.5, 8.3 - i * 0.5, parts[0], fontsize=9, \n",
    "                    family='monospace', color='#9cdcfe')\n",
    "            ax3.text(1.5 + len(parts[0]) * 0.15, 8.3 - i * 0.5, '=' + parts[1], \n",
    "                    fontsize=9, family='monospace', color='#ce9178')\n",
    "        else:\n",
    "            ax3.text(1.5, 8.3 - i * 0.5, line, fontsize=9, \n",
    "                    family='monospace', color=color)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_reproducibility_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Training Insights\n",
    "\n",
    "| Aspect | Detail |\n",
    "|--------|--------|\n",
    "| **Dataset** | WebText: 40GB from Reddit-filtered web pages |\n",
    "| **Tokenizer** | Byte-level BPE with 50,257 tokens |\n",
    "| **Training** | ~10B tokens, batch size 512, cosine LR |\n",
    "| **Compute** | ~9√ó10¬π‚Åπ FLOPs for GPT-2 XL |\n",
    "| **Scaling** | Power law: L ‚àù N^{-0.076} |\n",
    "\n",
    "### The Legacy\n",
    "\n",
    "GPT-2's training established:\n",
    "1. **WebText methodology** ‚Üí CommonCrawl filtering\n",
    "2. **Byte-level BPE** ‚Üí Universal tokenization\n",
    "3. **Scaling laws** ‚Üí Predictable improvement\n",
    "4. **Compute scaling** ‚Üí Billion-dollar training runs\n",
    "\n",
    "---\n",
    "\n",
    "### What's Next: Part V - Complete Implementation\n",
    "\n",
    "We'll cover:\n",
    "- Full GPT-2 implementation from scratch\n",
    "- Loading pretrained weights\n",
    "- Text generation\n",
    "- Fine-tuning examples\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Radford et al. (2019). [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "2. Kaplan et al. (2020). [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)\n",
    "3. Hoffmann et al. (2022). [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (Chinchilla)\n",
    "4. Karpathy. [nanoGPT](https://github.com/karpathy/nanoGPT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
