{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 Research Paper | Part V\n",
    "\n",
    "## Complete Implementation: From Scratch to Generation\n",
    "\n",
    "---\n",
    "\n",
    "**Paper:** [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "**Authors:** Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever (OpenAI, 2019)\n",
    "\n",
    "---\n",
    "\n",
    "## The Final Piece: Working Code\n",
    "\n",
    "This notebook brings everything together into a **complete, working implementation**.\n",
    "\n",
    "We'll cover:\n",
    "\n",
    "1. **Full GPT-2 Architecture** - Every component explained and implemented\n",
    "2. **Weight Loading** - Load pretrained weights from Hugging Face\n",
    "3. **Tokenization** - Using tiktoken for byte-level BPE\n",
    "4. **Text Generation** - Multiple decoding strategies\n",
    "5. **Zero-Shot Prompting** - Implementing tasks from the paper\n",
    "6. **Fine-Tuning** - Adapt GPT-2 for custom tasks\n",
    "7. **Performance Optimization** - Making it fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Rectangle, Circle\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List, Dict, Union\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Complete GPT-2 Configuration\n",
    "\n",
    "### 1.1 The Config Class\n",
    "\n",
    "First, let's define a configuration class that holds all hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPT2Config:\n",
    "    \"\"\"\n",
    "    GPT-2 Model Configuration.\n",
    "    \n",
    "    Contains all hyperparameters needed to build a GPT-2 model.\n",
    "    Default values correspond to GPT-2 Small (124M parameters).\n",
    "    \n",
    "    Attributes:\n",
    "        vocab_size: Size of vocabulary (50257 for GPT-2)\n",
    "        n_positions: Maximum sequence length (1024 for GPT-2)\n",
    "        n_embd: Embedding dimension\n",
    "        n_layer: Number of transformer blocks\n",
    "        n_head: Number of attention heads\n",
    "        n_inner: FFN inner dimension (default: 4 * n_embd)\n",
    "        activation: Activation function ('gelu' or 'gelu_new')\n",
    "        resid_pdrop: Dropout probability for residual connections\n",
    "        embd_pdrop: Dropout probability for embeddings\n",
    "        attn_pdrop: Dropout probability for attention\n",
    "        layer_norm_eps: Epsilon for layer normalization\n",
    "    \"\"\"\n",
    "    # Model architecture\n",
    "    vocab_size: int = 50257\n",
    "    n_positions: int = 1024\n",
    "    n_embd: int = 768\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_inner: Optional[int] = None  # Defaults to 4 * n_embd\n",
    "    activation: str = 'gelu_new'\n",
    "    \n",
    "    # Regularization\n",
    "    resid_pdrop: float = 0.1\n",
    "    embd_pdrop: float = 0.1\n",
    "    attn_pdrop: float = 0.1\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    \n",
    "    # Initialization\n",
    "    initializer_range: float = 0.02\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.n_inner is None:\n",
    "            self.n_inner = 4 * self.n_embd\n",
    "    \n",
    "    @property\n",
    "    def head_dim(self) -> int:\n",
    "        \"\"\"Dimension of each attention head.\"\"\"\n",
    "        return self.n_embd // self.n_head\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_name: str) -> 'GPT2Config':\n",
    "        \"\"\"\n",
    "        Create config for a pretrained model.\n",
    "        \n",
    "        Args:\n",
    "            model_name: One of 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'\n",
    "        \"\"\"\n",
    "        configs = {\n",
    "            'gpt2': dict(n_layer=12, n_head=12, n_embd=768),\n",
    "            'gpt2-medium': dict(n_layer=24, n_head=16, n_embd=1024),\n",
    "            'gpt2-large': dict(n_layer=36, n_head=20, n_embd=1280),\n",
    "            'gpt2-xl': dict(n_layer=48, n_head=25, n_embd=1600),\n",
    "        }\n",
    "        if model_name not in configs:\n",
    "            raise ValueError(f\"Unknown model: {model_name}. Choose from {list(configs.keys())}\")\n",
    "        return cls(**configs[model_name])\n",
    "\n",
    "\n",
    "# Show all configurations\n",
    "print(\"GPT-2 Model Configurations:\")\n",
    "print(\"=\" * 70)\n",
    "for name in ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']:\n",
    "    config = GPT2Config.from_pretrained(name)\n",
    "    params = config.vocab_size * config.n_embd  # Embeddings\n",
    "    params += config.n_positions * config.n_embd  # Position embeddings\n",
    "    # Per layer: attention (4 * n_embd^2) + MLP (8 * n_embd^2) + LN (4 * n_embd)\n",
    "    params += config.n_layer * (12 * config.n_embd ** 2 + 13 * config.n_embd)\n",
    "    params += 2 * config.n_embd  # Final LN\n",
    "    print(f\"{name:<15} layers={config.n_layer:<3} heads={config.n_head:<3} \"\n",
    "          f\"d_model={config.n_embd:<5} params≈{params/1e6:.0f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Building Blocks\n",
    "\n",
    "### 2.1 GELU Activation\n",
    "\n",
    "GPT-2 uses the GELU (Gaussian Error Linear Unit) activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu_new(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    GELU activation function (new/fast approximation).\n",
    "    \n",
    "    This is the version used in GPT-2 (slightly different from original GELU).\n",
    "    \n",
    "    Formula:\n",
    "        GELU(x) ≈ 0.5 * x * (1 + tanh(sqrt(2/π) * (x + 0.044715 * x³)))\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor\n",
    "        \n",
    "    Returns:\n",
    "        Activated tensor\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1.0 + torch.tanh(\n",
    "        math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))\n",
    "    ))\n",
    "\n",
    "\n",
    "# Visualize GELU\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "x = torch.linspace(-4, 4, 1000)\n",
    "\n",
    "# GELU vs ReLU\n",
    "axes[0].plot(x.numpy(), gelu_new(x).numpy(), 'b-', linewidth=2.5, label='GELU')\n",
    "axes[0].plot(x.numpy(), F.relu(x).numpy(), 'r--', linewidth=2, label='ReLU')\n",
    "axes[0].axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "axes[0].axvline(x=0, color='gray', linestyle='-', alpha=0.3)\n",
    "axes[0].set_xlabel('x', fontsize=11)\n",
    "axes[0].set_ylabel('f(x)', fontsize=11)\n",
    "axes[0].set_title('GELU vs ReLU', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xlim(-4, 4)\n",
    "axes[0].set_ylim(-1, 4)\n",
    "\n",
    "# Gradient\n",
    "x_grad = x.clone().requires_grad_(True)\n",
    "y = gelu_new(x_grad)\n",
    "grad = torch.autograd.grad(y.sum(), x_grad)[0]\n",
    "\n",
    "x_relu = x.clone().requires_grad_(True)\n",
    "y_relu = F.relu(x_relu)\n",
    "grad_relu = torch.autograd.grad(y_relu.sum(), x_relu)[0]\n",
    "\n",
    "axes[1].plot(x.numpy(), grad.detach().numpy(), 'b-', linewidth=2.5, label='GELU gradient')\n",
    "axes[1].plot(x.numpy(), grad_relu.detach().numpy(), 'r--', linewidth=2, label='ReLU gradient')\n",
    "axes[1].axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "axes[1].axhline(y=1, color='gray', linestyle='--', alpha=0.3)\n",
    "axes[1].set_xlabel('x', fontsize=11)\n",
    "axes[1].set_ylabel('df/dx', fontsize=11)\n",
    "axes[1].set_title('Gradient Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim(-4, 4)\n",
    "axes[1].set_ylim(-0.5, 1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey difference: GELU has smooth gradients everywhere (no hard cutoff at 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer Normalization with learnable parameters.\n",
    "    \n",
    "    Normalizes the last dimension of the input tensor.\n",
    "    \n",
    "    Formula:\n",
    "        y = (x - mean) / sqrt(var + eps) * weight + bias\n",
    "    \n",
    "    Args:\n",
    "        normalized_shape: Size of the last dimension\n",
    "        eps: Small constant for numerical stability\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, normalized_shape: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Compute mean and variance along last dimension\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        \n",
    "        # Normalize\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        \n",
    "        # Scale and shift\n",
    "        return self.weight * x_norm + self.bias\n",
    "\n",
    "\n",
    "# Test LayerNorm\n",
    "ln = LayerNorm(768)\n",
    "x = torch.randn(2, 10, 768)\n",
    "y = ln(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Output mean (should be ~0): {y.mean().item():.6f}\")\n",
    "print(f\"Output std (should be ~1): {y.std().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Multi-Head Causal Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Causal Self-Attention.\n",
    "    \n",
    "    This implements the attention mechanism used in GPT-2 with:\n",
    "    - Combined Q/K/V projection (efficient single matmul)\n",
    "    - Causal masking (tokens can only attend to previous tokens)\n",
    "    - Multi-head attention\n",
    "    - Scaled dot-product attention\n",
    "    \n",
    "    Args:\n",
    "        config: GPT2Config object\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert config.n_embd % config.n_head == 0, \\\n",
    "            f\"n_embd ({config.n_embd}) must be divisible by n_head ({config.n_head})\"\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = config.head_dim\n",
    "        \n",
    "        # Combined Q, K, V projection: [n_embd] -> [3 * n_embd]\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        \n",
    "        # Output projection: [n_embd] -> [n_embd]\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        \n",
    "        # Dropout\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        \n",
    "        # Causal mask (lower triangular)\n",
    "        # Register as buffer so it's moved with the model but not trained\n",
    "        self.register_buffer(\n",
    "            'bias',\n",
    "            torch.tril(torch.ones(config.n_positions, config.n_positions))\n",
    "                .view(1, 1, config.n_positions, config.n_positions)\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        use_cache: bool = False,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch, seq_len, n_embd]\n",
    "            attention_mask: Optional attention mask\n",
    "            use_cache: Whether to return key/value for caching\n",
    "            past_key_value: Cached key/value from previous forward pass\n",
    "            \n",
    "        Returns:\n",
    "            output: Attention output [batch, seq_len, n_embd]\n",
    "            present_key_value: Cached key/value (if use_cache=True)\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape  # batch, sequence length, embedding dim\n",
    "        \n",
    "        # Calculate Q, K, V in one matmul\n",
    "        qkv = self.c_attn(x)  # [B, T, 3*C]\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)  # Each: [B, T, C]\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        # [B, T, C] -> [B, T, n_head, head_dim] -> [B, n_head, T, head_dim]\n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Handle cached key/values for efficient generation\n",
    "        if past_key_value is not None:\n",
    "            past_k, past_v = past_key_value\n",
    "            k = torch.cat([past_k, k], dim=2)\n",
    "            v = torch.cat([past_v, v], dim=2)\n",
    "        \n",
    "        present_key_value = (k, v) if use_cache else None\n",
    "        \n",
    "        # Attention scores: [B, n_head, T, T_kv]\n",
    "        # Scale by 1/sqrt(head_dim) for stable gradients\n",
    "        scale = 1.0 / math.sqrt(self.head_dim)\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "        \n",
    "        # Apply causal mask\n",
    "        T_kv = k.shape[2]\n",
    "        causal_mask = self.bias[:, :, T_kv - T:T_kv, :T_kv]\n",
    "        attn = attn.masked_fill(causal_mask == 0, float('-inf'))\n",
    "        \n",
    "        # Apply additional attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            attn = attn + attention_mask\n",
    "        \n",
    "        # Softmax and dropout\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        \n",
    "        # Apply attention to values: [B, n_head, T, head_dim]\n",
    "        out = torch.matmul(attn, v)\n",
    "        \n",
    "        # Reshape back: [B, n_head, T, head_dim] -> [B, T, C]\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Output projection and dropout\n",
    "        out = self.resid_dropout(self.c_proj(out))\n",
    "        \n",
    "        return out, present_key_value\n",
    "\n",
    "\n",
    "# Test attention\n",
    "config = GPT2Config()\n",
    "attn = CausalSelfAttention(config)\n",
    "x = torch.randn(2, 10, 768)\n",
    "out, _ = attn(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Causal mask shape: {attn.bias.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Feed-Forward Network (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network.\n",
    "    \n",
    "    Two linear transformations with GELU activation:\n",
    "        FFN(x) = Dropout(Linear₂(GELU(Linear₁(x))))\n",
    "    \n",
    "    The inner dimension is typically 4× the embedding dimension.\n",
    "    \n",
    "    Args:\n",
    "        config: GPT2Config object\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Expand: n_embd -> n_inner (4 * n_embd)\n",
    "        self.c_fc = nn.Linear(config.n_embd, config.n_inner)\n",
    "        \n",
    "        # Contract: n_inner -> n_embd\n",
    "        self.c_proj = nn.Linear(config.n_inner, config.n_embd)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.c_fc(x)\n",
    "        x = gelu_new(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test MLP\n",
    "mlp = MLP(config)\n",
    "x = torch.randn(2, 10, 768)\n",
    "out = mlp(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Inner dimension: {config.n_inner}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT-2 Transformer Block with Pre-LayerNorm.\n",
    "    \n",
    "    Architecture:\n",
    "        x = x + Attention(LayerNorm(x))  # Pre-LN for attention\n",
    "        x = x + MLP(LayerNorm(x))        # Pre-LN for MLP\n",
    "    \n",
    "    This is different from the original Transformer (Post-LN):\n",
    "        x = LayerNorm(x + Attention(x))  # Post-LN\n",
    "    \n",
    "    Pre-LN provides better gradient flow for deep networks.\n",
    "    \n",
    "    Args:\n",
    "        config: GPT2Config object\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pre-LayerNorm for attention\n",
    "        self.ln_1 = LayerNorm(config.n_embd, eps=config.layer_norm_eps)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        \n",
    "        # Pre-LayerNorm for MLP\n",
    "        self.ln_2 = LayerNorm(config.n_embd, eps=config.layer_norm_eps)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        use_cache: bool = False,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch, seq_len, n_embd]\n",
    "            attention_mask: Optional attention mask\n",
    "            use_cache: Whether to cache key/values\n",
    "            past_key_value: Cached key/values from previous step\n",
    "            \n",
    "        Returns:\n",
    "            output: Block output [batch, seq_len, n_embd]\n",
    "            present_key_value: Cached key/values (if use_cache=True)\n",
    "        \"\"\"\n",
    "        # Pre-LN Attention: x = x + Attn(LN(x))\n",
    "        attn_out, present_key_value = self.attn(\n",
    "            self.ln_1(x),\n",
    "            attention_mask=attention_mask,\n",
    "            use_cache=use_cache,\n",
    "            past_key_value=past_key_value,\n",
    "        )\n",
    "        x = x + attn_out\n",
    "        \n",
    "        # Pre-LN MLP: x = x + MLP(LN(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        \n",
    "        return x, present_key_value\n",
    "\n",
    "\n",
    "# Test Block\n",
    "block = Block(config)\n",
    "x = torch.randn(2, 10, 768)\n",
    "out, _ = block(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Complete GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete GPT-2 Language Model.\n",
    "    \n",
    "    Architecture:\n",
    "        1. Token Embedding + Position Embedding\n",
    "        2. Dropout\n",
    "        3. N × Transformer Blocks (Pre-LN)\n",
    "        4. Final LayerNorm\n",
    "        5. Language Model Head (tied with token embedding)\n",
    "    \n",
    "    Args:\n",
    "        config: GPT2Config object\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        \n",
    "        # Position embeddings (learned, not sinusoidal)\n",
    "        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n",
    "        \n",
    "        # Dropout after embeddings\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.h = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
    "        \n",
    "        # Final layer norm (GPT-2 specific)\n",
    "        self.ln_f = LayerNorm(config.n_embd, eps=config.layer_norm_eps)\n",
    "        \n",
    "        # Language model head (weight tied with wte)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying\n",
    "        self.lm_head.weight = self.wte.weight\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # Apply special scaled initialization to residual projections\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(\n",
    "                    p, \n",
    "                    mean=0.0, \n",
    "                    std=config.initializer_range / math.sqrt(2 * config.n_layer)\n",
    "                )\n",
    "        \n",
    "        # Report number of parameters\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"GPT-2 initialized with {n_params:,} parameters\")\n",
    "    \n",
    "    def _init_weights(self, module: nn.Module):\n",
    "        \"\"\"Initialize weights.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        use_cache: bool = False,\n",
    "        past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs [batch, seq_len]\n",
    "            attention_mask: Attention mask [batch, seq_len]\n",
    "            labels: Labels for language modeling loss [batch, seq_len]\n",
    "            use_cache: Whether to return cached key/values\n",
    "            past_key_values: Cached key/values from previous forward\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing:\n",
    "                - logits: Vocabulary logits [batch, seq_len, vocab_size]\n",
    "                - loss: Language modeling loss (if labels provided)\n",
    "                - past_key_values: Cached key/values (if use_cache=True)\n",
    "        \"\"\"\n",
    "        B, T = input_ids.shape\n",
    "        \n",
    "        # Determine position indices\n",
    "        if past_key_values is not None:\n",
    "            past_length = past_key_values[0][0].shape[2]\n",
    "        else:\n",
    "            past_length = 0\n",
    "        \n",
    "        position_ids = torch.arange(\n",
    "            past_length, past_length + T, \n",
    "            device=input_ids.device\n",
    "        ).unsqueeze(0)\n",
    "        \n",
    "        # Get embeddings\n",
    "        token_emb = self.wte(input_ids)  # [B, T, n_embd]\n",
    "        pos_emb = self.wpe(position_ids)  # [1, T, n_embd]\n",
    "        \n",
    "        # Combine embeddings\n",
    "        hidden_states = self.drop(token_emb + pos_emb)\n",
    "        \n",
    "        # Process attention mask\n",
    "        if attention_mask is not None:\n",
    "            # Convert [B, T] mask to [B, 1, 1, T] for broadcasting\n",
    "            attention_mask = attention_mask.view(B, 1, 1, -1)\n",
    "            attention_mask = (1.0 - attention_mask) * -10000.0\n",
    "        \n",
    "        # Forward through transformer blocks\n",
    "        presents = [] if use_cache else None\n",
    "        \n",
    "        for i, block in enumerate(self.h):\n",
    "            past_kv = past_key_values[i] if past_key_values is not None else None\n",
    "            \n",
    "            hidden_states, present_kv = block(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                use_cache=use_cache,\n",
    "                past_key_value=past_kv,\n",
    "            )\n",
    "            \n",
    "            if use_cache:\n",
    "                presents.append(present_kv)\n",
    "        \n",
    "        # Final layer norm\n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "        \n",
    "        # Language model head\n",
    "        logits = self.lm_head(hidden_states)  # [B, T, vocab_size]\n",
    "        \n",
    "        # Calculate loss if labels provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            \n",
    "            loss = F.cross_entropy(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1)\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'loss': loss,\n",
    "            'past_key_values': presents,\n",
    "        }\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        max_new_tokens: int = 50,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: Optional[int] = None,\n",
    "        top_p: Optional[float] = None,\n",
    "        do_sample: bool = True,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate text autoregressively.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Starting token IDs [batch, seq_len]\n",
    "            max_new_tokens: Maximum tokens to generate\n",
    "            temperature: Sampling temperature (higher = more random)\n",
    "            top_k: Keep only top k tokens with highest probability\n",
    "            top_p: Keep tokens with cumulative probability >= top_p\n",
    "            do_sample: Whether to sample (True) or take argmax (False)\n",
    "            \n",
    "        Returns:\n",
    "            Generated token IDs [batch, seq_len + max_new_tokens]\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop to max context length if needed\n",
    "            idx_cond = input_ids if input_ids.shape[1] <= self.config.n_positions \\\n",
    "                       else input_ids[:, -self.config.n_positions:]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self(idx_cond)\n",
    "            logits = outputs['logits'][:, -1, :]  # [B, vocab_size]\n",
    "            \n",
    "            # Apply temperature\n",
    "            logits = logits / temperature\n",
    "            \n",
    "            # Apply top-k filtering\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = float('-inf')\n",
    "            \n",
    "            # Apply top-p (nucleus) filtering\n",
    "            if top_p is not None:\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                \n",
    "                # Remove tokens with cumulative probability above threshold\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                \n",
    "                indices_to_remove = sorted_indices_to_remove.scatter(\n",
    "                    1, sorted_indices, sorted_indices_to_remove\n",
    "                )\n",
    "                logits[indices_to_remove] = float('-inf')\n",
    "            \n",
    "            # Sample or argmax\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            if do_sample:\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                next_token = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "            \n",
    "            # Append to sequence\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        \n",
    "        return input_ids\n",
    "\n",
    "\n",
    "# Create model\n",
    "print(\"Creating GPT-2 Small:\")\n",
    "model = GPT2Model(GPT2Config())\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randint(0, 50257, (2, 10))\n",
    "outputs = model(x, labels=x)\n",
    "\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Logits shape: {outputs['logits'].shape}\")\n",
    "print(f\"Loss: {outputs['loss'].item():.4f}\")\n",
    "print(f\"Expected loss (random): {math.log(50257):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Loading Pretrained Weights\n",
    "\n",
    "Let's load the official GPT-2 weights from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_gpt2(model_name: str = 'gpt2') -> GPT2Model:\n",
    "    \"\"\"\n",
    "    Load pretrained GPT-2 weights from Hugging Face.\n",
    "    \n",
    "    Args:\n",
    "        model_name: One of 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'\n",
    "        \n",
    "    Returns:\n",
    "        GPT2Model with pretrained weights\n",
    "    \"\"\"\n",
    "    from transformers import GPT2LMHeadModel\n",
    "    \n",
    "    print(f\"Loading {model_name} from Hugging Face...\")\n",
    "    \n",
    "    # Load HF model\n",
    "    hf_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    hf_state = hf_model.state_dict()\n",
    "    \n",
    "    # Create our model\n",
    "    config = GPT2Config.from_pretrained(model_name)\n",
    "    model = GPT2Model(config)\n",
    "    \n",
    "    # Map HF keys to our keys\n",
    "    key_mapping = {\n",
    "        'transformer.wte.weight': 'wte.weight',\n",
    "        'transformer.wpe.weight': 'wpe.weight',\n",
    "        'transformer.ln_f.weight': 'ln_f.weight',\n",
    "        'transformer.ln_f.bias': 'ln_f.bias',\n",
    "        'lm_head.weight': 'lm_head.weight',\n",
    "    }\n",
    "    \n",
    "    # Add block mappings\n",
    "    for i in range(config.n_layer):\n",
    "        prefix_hf = f'transformer.h.{i}'\n",
    "        prefix_ours = f'h.{i}'\n",
    "        \n",
    "        key_mapping.update({\n",
    "            f'{prefix_hf}.ln_1.weight': f'{prefix_ours}.ln_1.weight',\n",
    "            f'{prefix_hf}.ln_1.bias': f'{prefix_ours}.ln_1.bias',\n",
    "            f'{prefix_hf}.attn.c_attn.weight': f'{prefix_ours}.attn.c_attn.weight',\n",
    "            f'{prefix_hf}.attn.c_attn.bias': f'{prefix_ours}.attn.c_attn.bias',\n",
    "            f'{prefix_hf}.attn.c_proj.weight': f'{prefix_ours}.attn.c_proj.weight',\n",
    "            f'{prefix_hf}.attn.c_proj.bias': f'{prefix_ours}.attn.c_proj.bias',\n",
    "            f'{prefix_hf}.ln_2.weight': f'{prefix_ours}.ln_2.weight',\n",
    "            f'{prefix_hf}.ln_2.bias': f'{prefix_ours}.ln_2.bias',\n",
    "            f'{prefix_hf}.mlp.c_fc.weight': f'{prefix_ours}.mlp.c_fc.weight',\n",
    "            f'{prefix_hf}.mlp.c_fc.bias': f'{prefix_ours}.mlp.c_fc.bias',\n",
    "            f'{prefix_hf}.mlp.c_proj.weight': f'{prefix_ours}.mlp.c_proj.weight',\n",
    "            f'{prefix_hf}.mlp.c_proj.bias': f'{prefix_ours}.mlp.c_proj.bias',\n",
    "        })\n",
    "    \n",
    "    # Copy weights\n",
    "    new_state = {}\n",
    "    for hf_key, our_key in key_mapping.items():\n",
    "        if hf_key in hf_state:\n",
    "            weight = hf_state[hf_key]\n",
    "            \n",
    "            # HF uses Conv1D, we use Linear (need to transpose)\n",
    "            if 'c_attn.weight' in hf_key or 'c_proj.weight' in hf_key or \\\n",
    "               'c_fc.weight' in hf_key:\n",
    "                weight = weight.t()\n",
    "            \n",
    "            new_state[our_key] = weight\n",
    "    \n",
    "    # Load into model\n",
    "    model.load_state_dict(new_state, strict=False)\n",
    "    \n",
    "    print(f\"Loaded {len(new_state)} weight tensors\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Load pretrained model\n",
    "try:\n",
    "    model = load_pretrained_gpt2('gpt2')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print(\"\\nModel loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load pretrained weights: {e}\")\n",
    "    print(\"Using randomly initialized model instead.\")\n",
    "    model = GPT2Model(GPT2Config()).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Tokenization\n",
    "\n",
    "GPT-2 uses byte-level BPE tokenization. Let's use tiktoken for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tiktoken\n",
    "    enc = tiktoken.get_encoding('gpt2')\n",
    "    print(\"Using tiktoken for tokenization\")\n",
    "except ImportError:\n",
    "    from transformers import GPT2Tokenizer\n",
    "    enc = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    print(\"Using Hugging Face tokenizer\")\n",
    "\n",
    "\n",
    "def encode(text: str) -> torch.Tensor:\n",
    "    \"\"\"Encode text to token IDs.\"\"\"\n",
    "    if hasattr(enc, 'encode'):\n",
    "        tokens = enc.encode(text)\n",
    "    else:\n",
    "        tokens = enc(text)['input_ids']\n",
    "    return torch.tensor(tokens, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "\n",
    "def decode(tokens: torch.Tensor) -> str:\n",
    "    \"\"\"Decode token IDs to text.\"\"\"\n",
    "    if hasattr(enc, 'decode'):\n",
    "        return enc.decode(tokens.squeeze().tolist())\n",
    "    else:\n",
    "        return enc.decode(tokens.squeeze().tolist())\n",
    "\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"Hello, world! How are you today?\"\n",
    "tokens = encode(test_text)\n",
    "decoded = decode(tokens)\n",
    "\n",
    "print(f\"Original: '{test_text}'\")\n",
    "print(f\"Tokens: {tokens.tolist()}\")\n",
    "print(f\"Decoded: '{decoded}'\")\n",
    "print(f\"Match: {test_text == decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Text Generation\n",
    "\n",
    "### 6.1 Different Decoding Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    model: GPT2Model,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: Optional[int] = None,\n",
    "    top_p: Optional[float] = None,\n",
    "    do_sample: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate text from a prompt.\n",
    "    \n",
    "    Args:\n",
    "        model: GPT-2 model\n",
    "        prompt: Starting text\n",
    "        max_new_tokens: Number of tokens to generate\n",
    "        temperature: Sampling temperature\n",
    "        top_k: Top-k filtering\n",
    "        top_p: Nucleus sampling\n",
    "        do_sample: Whether to sample\n",
    "        \n",
    "    Returns:\n",
    "        Generated text\n",
    "    \"\"\"\n",
    "    input_ids = encode(prompt).to(device)\n",
    "    \n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        do_sample=do_sample,\n",
    "    )\n",
    "    \n",
    "    return decode(output_ids)\n",
    "\n",
    "\n",
    "# Test generation with different strategies\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEXT GENERATION WITH DIFFERENT STRATEGIES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nPrompt: '{prompt}'\\n\")\n",
    "\n",
    "strategies = [\n",
    "    ('Greedy (argmax)', dict(do_sample=False)),\n",
    "    ('Temperature 0.7', dict(temperature=0.7)),\n",
    "    ('Top-k = 50', dict(top_k=50)),\n",
    "    ('Top-p = 0.9', dict(top_p=0.9)),\n",
    "    ('Combined', dict(temperature=0.8, top_k=50, top_p=0.95)),\n",
    "]\n",
    "\n",
    "for name, kwargs in strategies:\n",
    "    print(f\"\\n[{name}]\")\n",
    "    text = generate_text(model, prompt, max_new_tokens=40, **kwargs)\n",
    "    print(text)\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_generation_strategies():\n",
    "    \"\"\"\n",
    "    Visualize different text generation strategies.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    gs = gridspec.GridSpec(2, 2, hspace=0.3, wspace=0.25)\n",
    "    \n",
    "    # Simulated probability distribution\n",
    "    np.random.seed(42)\n",
    "    vocab_size = 20\n",
    "    raw_logits = np.random.randn(vocab_size) * 2\n",
    "    raw_logits[0] = 5  # One token much more likely\n",
    "    raw_logits[1] = 3\n",
    "    raw_logits[2] = 2.5\n",
    "    \n",
    "    def softmax(x):\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / exp_x.sum()\n",
    "    \n",
    "    probs = softmax(raw_logits)\n",
    "    tokens = [f'T{i}' for i in range(vocab_size)]\n",
    "    \n",
    "    # === TOP LEFT: Temperature ===\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    \n",
    "    temps = [0.5, 1.0, 1.5]\n",
    "    colors = ['#e74c3c', '#3498db', '#27ae60']\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, (temp, color) in enumerate(zip(temps, colors)):\n",
    "        temp_probs = softmax(raw_logits / temp)\n",
    "        x = np.arange(vocab_size) + i * width\n",
    "        ax1.bar(x, temp_probs, width, label=f'T={temp}', color=color, alpha=0.8)\n",
    "    \n",
    "    ax1.set_xlabel('Token', fontsize=11)\n",
    "    ax1.set_ylabel('Probability', fontsize=11)\n",
    "    ax1.set_title('Temperature: Controls Distribution Sharpness', fontsize=12, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.set_xticks(np.arange(vocab_size) + width)\n",
    "    ax1.set_xticklabels(tokens, fontsize=8)\n",
    "    \n",
    "    # === TOP RIGHT: Top-k ===\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    \n",
    "    sorted_idx = np.argsort(probs)[::-1]\n",
    "    sorted_probs = probs[sorted_idx]\n",
    "    sorted_tokens = [tokens[i] for i in sorted_idx]\n",
    "    \n",
    "    k = 5\n",
    "    colors_topk = ['#27ae60' if i < k else '#95a5a6' for i in range(vocab_size)]\n",
    "    ax2.bar(range(vocab_size), sorted_probs, color=colors_topk)\n",
    "    ax2.axvline(x=k-0.5, color='#e74c3c', linestyle='--', linewidth=2)\n",
    "    ax2.text(k, max(sorted_probs)*0.8, f'k={k}', fontsize=11, color='#e74c3c', fontweight='bold')\n",
    "    \n",
    "    ax2.set_xlabel('Token (sorted by probability)', fontsize=11)\n",
    "    ax2.set_ylabel('Probability', fontsize=11)\n",
    "    ax2.set_title('Top-k: Keep Only k Most Likely Tokens', fontsize=12, fontweight='bold')\n",
    "    ax2.set_xticks(range(vocab_size))\n",
    "    ax2.set_xticklabels(sorted_tokens, fontsize=8, rotation=45)\n",
    "    \n",
    "    # === BOTTOM LEFT: Top-p (Nucleus) ===\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    \n",
    "    cumsum = np.cumsum(sorted_probs)\n",
    "    p = 0.9\n",
    "    cutoff = np.searchsorted(cumsum, p)\n",
    "    \n",
    "    colors_topp = ['#27ae60' if i <= cutoff else '#95a5a6' for i in range(vocab_size)]\n",
    "    ax3.bar(range(vocab_size), sorted_probs, color=colors_topp)\n",
    "    ax3.plot(range(vocab_size), cumsum, 'r-', linewidth=2, marker='o', markersize=4)\n",
    "    ax3.axhline(y=p, color='#e74c3c', linestyle='--', linewidth=2)\n",
    "    ax3.text(vocab_size-3, p+0.02, f'p={p}', fontsize=11, color='#e74c3c', fontweight='bold')\n",
    "    \n",
    "    ax3.set_xlabel('Token (sorted by probability)', fontsize=11)\n",
    "    ax3.set_ylabel('Probability / Cumulative', fontsize=11)\n",
    "    ax3.set_title('Top-p (Nucleus): Keep Until Cumulative ≥ p', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xticks(range(vocab_size))\n",
    "    ax3.set_xticklabels(sorted_tokens, fontsize=8, rotation=45)\n",
    "    \n",
    "    # === BOTTOM RIGHT: Strategy Comparison ===\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    ax4.set_xlim(0, 12)\n",
    "    ax4.set_ylim(0, 10)\n",
    "    ax4.axis('off')\n",
    "    ax4.set_title('Strategy Comparison', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    strategies_info = [\n",
    "        ('Greedy', 'Always pick highest prob', 'Deterministic, repetitive', '#e74c3c'),\n",
    "        ('Temperature', 'Scale logits before softmax', 'Controls randomness', '#3498db'),\n",
    "        ('Top-k', 'Sample from k best tokens', 'Excludes unlikely tokens', '#27ae60'),\n",
    "        ('Top-p', 'Sample until cumsum ≥ p', 'Adaptive vocabulary size', '#f39c12'),\n",
    "    ]\n",
    "    \n",
    "    for i, (name, desc, note, color) in enumerate(strategies_info):\n",
    "        y = 8 - i * 2\n",
    "        \n",
    "        rect = FancyBboxPatch((0.5, y-0.5), 3, 1.3, boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor=color, edgecolor='black', linewidth=2)\n",
    "        ax4.add_patch(rect)\n",
    "        ax4.text(2, y, name, ha='center', va='center', \n",
    "                fontsize=10, fontweight='bold', color='white')\n",
    "        \n",
    "        ax4.text(4, y+0.2, desc, va='center', fontsize=9)\n",
    "        ax4.text(4, y-0.3, f'→ {note}', va='center', fontsize=8, color='gray')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_generation_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Zero-Shot Task Prompting\n",
    "\n",
    "Let's implement the zero-shot tasks from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroShotTasks:\n",
    "    \"\"\"\n",
    "    Zero-shot task implementations using GPT-2.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: GPT2Model):\n",
    "        self.model = model\n",
    "    \n",
    "    def summarize(self, article: str, max_tokens: int = 50) -> str:\n",
    "        \"\"\"\n",
    "        Summarize an article using the TL;DR prompt.\n",
    "        \n",
    "        From the paper: \"To induce summarization behavior we add \n",
    "        the text TL;DR: after the article\"\n",
    "        \"\"\"\n",
    "        prompt = f\"{article}\\n\\nTL;DR:\"\n",
    "        return generate_text(\n",
    "            self.model, prompt, max_new_tokens=max_tokens,\n",
    "            temperature=0.7, top_p=0.9\n",
    "        )\n",
    "    \n",
    "    def translate(self, text: str, target_lang: str = \"French\") -> str:\n",
    "        \"\"\"\n",
    "        Translate text to another language.\n",
    "        \"\"\"\n",
    "        prompt = f\"Translate English to {target_lang}:\\n\\n{text} =>\"\n",
    "        return generate_text(\n",
    "            self.model, prompt, max_new_tokens=30,\n",
    "            temperature=0.3, top_k=50\n",
    "        )\n",
    "    \n",
    "    def answer_question(self, question: str, context: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Answer a question, optionally with context.\n",
    "        \"\"\"\n",
    "        if context:\n",
    "            prompt = f\"Context: {context}\\n\\nQ: {question}\\nA:\"\n",
    "        else:\n",
    "            prompt = f\"Q: {question}\\nA:\"\n",
    "        \n",
    "        return generate_text(\n",
    "            self.model, prompt, max_new_tokens=30,\n",
    "            temperature=0.5, top_k=40\n",
    "        )\n",
    "    \n",
    "    def complete_story(self, beginning: str, max_tokens: int = 100) -> str:\n",
    "        \"\"\"\n",
    "        Complete a story given a beginning.\n",
    "        \"\"\"\n",
    "        return generate_text(\n",
    "            self.model, beginning, max_new_tokens=max_tokens,\n",
    "            temperature=0.9, top_p=0.95\n",
    "        )\n",
    "    \n",
    "    def sentiment(self, review: str) -> str:\n",
    "        \"\"\"\n",
    "        Analyze sentiment of a review.\n",
    "        \"\"\"\n",
    "        prompt = f\"Review: {review}\\nSentiment (positive/negative):\"\n",
    "        return generate_text(\n",
    "            self.model, prompt, max_new_tokens=5,\n",
    "            temperature=0.3, top_k=10\n",
    "        )\n",
    "\n",
    "\n",
    "# Test zero-shot tasks\n",
    "tasks = ZeroShotTasks(model)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ZERO-SHOT TASK DEMONSTRATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Summarization\n",
    "article = \"\"\"Artificial intelligence has made remarkable progress in recent years. \n",
    "Deep learning models can now understand language, recognize images, and even generate \n",
    "creative content. Companies around the world are investing billions in AI research \n",
    "and development. However, concerns about AI safety, job displacement, and ethical \n",
    "implications continue to be debated by researchers and policymakers.\"\"\"\n",
    "\n",
    "print(\"\\n[SUMMARIZATION]\")\n",
    "print(f\"Article: {article[:100]}...\")\n",
    "summary = tasks.summarize(article)\n",
    "print(f\"Summary: {summary}\")\n",
    "\n",
    "# Question Answering\n",
    "print(\"\\n[QUESTION ANSWERING]\")\n",
    "question = \"What is the capital of France?\"\n",
    "answer = tasks.answer_question(question)\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"A: {answer}\")\n",
    "\n",
    "# Story Completion\n",
    "print(\"\\n[STORY COMPLETION]\")\n",
    "beginning = \"Once upon a time, in a distant galaxy, there lived a robot who dreamed of\"\n",
    "story = tasks.complete_story(beginning, max_tokens=50)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Fine-Tuning Example\n",
    "\n",
    "Let's show how to fine-tune GPT-2 on a custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Simple text dataset for fine-tuning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts: List[str], max_length: int = 256):\n",
    "        self.examples = []\n",
    "        \n",
    "        for text in texts:\n",
    "            tokens = encode(text).squeeze()\n",
    "            \n",
    "            # Split into chunks of max_length\n",
    "            for i in range(0, len(tokens) - 1, max_length):\n",
    "                chunk = tokens[i:i + max_length + 1]\n",
    "                if len(chunk) > 1:\n",
    "                    self.examples.append(chunk)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.examples[idx]\n",
    "        return {\n",
    "            'input_ids': tokens[:-1],\n",
    "            'labels': tokens[1:],\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function for DataLoader.\"\"\"\n",
    "    max_len = max(len(x['input_ids']) for x in batch)\n",
    "    \n",
    "    input_ids = torch.zeros(len(batch), max_len, dtype=torch.long)\n",
    "    labels = torch.full((len(batch), max_len), -100, dtype=torch.long)\n",
    "    \n",
    "    for i, x in enumerate(batch):\n",
    "        length = len(x['input_ids'])\n",
    "        input_ids[i, :length] = x['input_ids']\n",
    "        labels[i, :length] = x['labels']\n",
    "    \n",
    "    return {'input_ids': input_ids, 'labels': labels}\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc='Training'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = outputs['loss']\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "# Example: Fine-tune on Shakespeare\n",
    "shakespeare_texts = [\n",
    "    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind to suffer\",\n",
    "    \"The slings and arrows of outrageous fortune, Or to take arms against a sea of troubles\",\n",
    "    \"All the world's a stage, And all the men and women merely players\",\n",
    "    \"They have their exits and their entrances, And one man in his time plays many parts\",\n",
    "    \"What's in a name? That which we call a rose By any other name would smell as sweet\",\n",
    "    \"Romeo, Romeo, wherefore art thou Romeo? Deny thy father and refuse thy name\",\n",
    "    \"But soft, what light through yonder window breaks? It is the east, and Juliet is the sun\",\n",
    "    \"Good night, good night! Parting is such sweet sorrow, That I shall say good night till it be morrow\",\n",
    "] * 10  # Repeat for more data\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FINE-TUNING DEMONSTRATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TextDataset(shakespeare_texts, max_length=64)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"\\nDataset size: {len(dataset)} examples\")\n",
    "print(f\"Batch size: 4\")\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "# Train for a few epochs (demonstration only)\n",
    "n_epochs = 2\n",
    "print(f\"\\nTraining for {n_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss = train_epoch(model, dataloader, optimizer, device)\n",
    "    print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Test generation after fine-tuning\n",
    "print(\"\\n[Generation after fine-tuning]\")\n",
    "prompt = \"To be, or not to be,\"\n",
    "output = generate_text(model, prompt, max_new_tokens=30, temperature=0.8)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Model Architecture Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_complete_architecture():\n",
    "    \"\"\"\n",
    "    Complete visualization of GPT-2 architecture.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_xlim(0, 24)\n",
    "    ax.set_ylim(0, 22)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    ax.text(12, 21.5, 'GPT-2 Complete Architecture', fontsize=18, fontweight='bold', ha='center')\n",
    "    \n",
    "    # === INPUT ===\n",
    "    rect = FancyBboxPatch((9, 19.5), 6, 1, boxstyle=\"round,pad=0.03\",\n",
    "                          facecolor='#3498db', edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(12, 20, 'Input Token IDs [B, T]', ha='center', va='center', \n",
    "            fontsize=11, color='white', fontweight='bold')\n",
    "    \n",
    "    ax.annotate('', xy=(12, 18.5), xytext=(12, 19.4),\n",
    "               arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "    \n",
    "    # === EMBEDDINGS ===\n",
    "    # Token embedding\n",
    "    rect = FancyBboxPatch((5, 17), 5, 1.2, boxstyle=\"round,pad=0.03\",\n",
    "                          facecolor='#9b59b6', edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(7.5, 17.6, 'Token Embedding', ha='center', va='center', \n",
    "            fontsize=10, color='white', fontweight='bold')\n",
    "    ax.text(7.5, 17.2, 'wte: [50257, 768]', ha='center', fontsize=8, color='white')\n",
    "    \n",
    "    # Position embedding\n",
    "    rect = FancyBboxPatch((14, 17), 5, 1.2, boxstyle=\"round,pad=0.03\",\n",
    "                          facecolor='#9b59b6', edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(16.5, 17.6, 'Position Embedding', ha='center', va='center', \n",
    "            fontsize=10, color='white', fontweight='bold')\n",
    "    ax.text(16.5, 17.2, 'wpe: [1024, 768]', ha='center', fontsize=8, color='white')\n",
    "    \n",
    "    # Add\n",
    "    circle = plt.Circle((12, 15.5), 0.4, facecolor='#27ae60', edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(12, 15.5, '+', ha='center', va='center', fontsize=16, color='white', fontweight='bold')\n",
    "    \n",
    "    ax.annotate('', xy=(12, 15.1), xytext=(7.5, 16.9),\n",
    "               arrowprops=dict(arrowstyle='->', color='black', lw=1.5))\n",
    "    ax.annotate('', xy=(12, 15.1), xytext=(16.5, 16.9),\n",
    "               arrowprops=dict(arrowstyle='->', color='black', lw=1.5))\n",
    "    \n",
    "    # Dropout\n",
    "    ax.text(13, 15.5, 'Dropout', fontsize=9, va='center')\n",
    "    \n",
    "    # === TRANSFORMER BLOCK ===\n",
    "    block_rect = FancyBboxPatch((4, 5), 16, 9), \n",
    "    rect = FancyBboxPatch((4, 5), 16, 9, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='#f5f5f5', edgecolor='#2c3e50', linewidth=3)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(12, 13.5, 'Transformer Block × 12', ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Pre-LN 1\n",
    "    rect = FancyBboxPatch((5.5, 11.5), 3, 1, boxstyle=\"round,pad=0.02\",\n",
    "                          facecolor='#f39c12', edgecolor='black', linewidth=1.5)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(7, 12, 'LayerNorm', ha='center', va='center', fontsize=9, color='white', fontweight='bold')\n",
    "    \n",
    "    # Multi-Head Attention\n",
    "    rect = FancyBboxPatch((9.5, 11), 5, 2, boxstyle=\"round,pad=0.02\",\n",
    "                          facecolor='#e74c3c', edgecolor='black', linewidth=1.5)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(12, 12.3, 'Multi-Head', ha='center', va='center', fontsize=9, color='white', fontweight='bold')\n",
    "    ax.text(12, 11.7, 'Self-Attention', ha='center', va='center', fontsize=9, color='white')\n",
    "    ax.text(12, 11.2, '(12 heads)', ha='center', va='center', fontsize=8, color='white')\n",
    "    \n",
    "    ax.annotate('', xy=(9.4, 12), xytext=(8.6, 12),\n",
    "               arrowprops=dict(arrowstyle='->', color='black', lw=1.5))\n",
    "    \n",
    "    # Residual 1\n",
    "    ax.plot([5, 5], [12, 9.5], 'g-', linewidth=3)\n",
    "    ax.plot([5, 16], [9.5, 9.5], 'g-', linewidth=3)\n",
    "    \n",
    "    circle = plt.Circle((16, 9.5), 0.3, facecolor='#27ae60', edgecolor='black', linewidth=1.5)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(16, 9.5, '+', ha='center', va='center', fontsize=12, color='white', fontweight='bold')\n",
    "    \n",
    "    ax.annotate('', xy=(15.7, 9.5), xytext=(14.6, 11),\n",
    "               arrowprops=dict(arrowstyle='->', color='black', lw=1.5))\n",
    "    \n",
    "    # Pre-LN 2\n",
    "    ax.plot([16, 16], [9.2, 8.3], 'k-', linewidth=2)\n",
    "    ax.plot([16, 7], [8.3, 8.3], 'k-', linewidth=2)\n",
    "    \n",
    "    rect = FancyBboxPatch((5.5, 7.5), 3, 1, boxstyle=\"round,pad=0.02\",\n",
    "                          facecolor='#f39c12', edgecolor='black', linewidth=1.5)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(7, 8, 'LayerNorm', ha='center', va='center', fontsize=9, color='white', fontweight='bold')\n",
    "    \n",
    "    # MLP\n",
    "    rect = FancyBboxPatch((9.5, 6.5), 5, 2.5, boxstyle=\"round,pad=0.02\",\n",
    "                          facecolor='#3498db', edgecolor='black', linewidth=1.5)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(12, 8.2, 'MLP', ha='center', va='center', fontsize=10, color='white', fontweight='bold')\n",
    "    ax.text(12, 7.6, 'Linear: 768→3072', ha='center', va='center', fontsize=8, color='white')\n",
    "    ax.text(12, 7.1, 'GELU', ha='center', va='center', fontsize=8, color='white')\n",
    "    ax.text(12, 6.6, 'Linear: 3072→768', ha='center', va='center', fontsize=8, color='white')\n",
    "    \n",
    "    ax.annotate('', xy=(9.4, 7.75), xytext=(8.6, 7.75),\n",
    "               arrowprops=dict(arrowstyle='->', color='black', lw=1.5))\n",
    "    \n",
    "    # Residual 2\n",
    "    ax.plot([5, 5], [8, 5.5], 'g-', linewidth=3)\n",
    "    ax.plot([5, 16], [5.5, 5.5], 'g-', linewidth=3)\n",
    "    \n",
    "    circle = plt.Circle((16, 5.5), 0.3, facecolor='#27ae60', edgecolor='black', linewidth=1.5)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(16, 5.5, '+', ha='center', va='center', fontsize=12, color='white', fontweight='bold')\n",
    "    \n",
    "    ax.annotate('', xy=(15.7, 5.5), xytext=(14.6, 6.4),\n",
    "               arrowprops=dict(arrowstyle='->', color='black', lw=1.5))\n",
    "    \n",
    "    # === FINAL LAYER NORM ===\n",
    "    ax.annotate('', xy=(12, 3.7), xytext=(12, 4.9),\n",
    "               arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "    \n",
    "    rect = FancyBboxPatch((9, 2.5), 6, 1, boxstyle=\"round,pad=0.03\",\n",
    "                          facecolor='#f39c12', edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(12, 3, 'Final LayerNorm', ha='center', va='center', \n",
    "            fontsize=10, color='white', fontweight='bold')\n",
    "    \n",
    "    # === LM HEAD ===\n",
    "    ax.annotate('', xy=(12, 1.3), xytext=(12, 2.4),\n",
    "               arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "    \n",
    "    rect = FancyBboxPatch((9, 0.2), 6, 1, boxstyle=\"round,pad=0.03\",\n",
    "                          facecolor='#2c3e50', edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(12, 0.7, 'LM Head → Logits [B, T, 50257]', ha='center', va='center', \n",
    "            fontsize=10, color='white', fontweight='bold')\n",
    "    \n",
    "    # Weight tying note\n",
    "    ax.annotate('', xy=(7, 1), xytext=(7, 17),\n",
    "               arrowprops=dict(arrowstyle='<->', color='#9b59b6', lw=2, ls='--'))\n",
    "    ax.text(3, 9, 'Weight\\nTying', ha='center', fontsize=9, color='#9b59b6', fontweight='bold')\n",
    "    \n",
    "    # Legend\n",
    "    ax.text(20, 12, 'Key:', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    legend_items = [\n",
    "        ('Pre-LN', '#f39c12'),\n",
    "        ('Attention', '#e74c3c'),\n",
    "        ('MLP', '#3498db'),\n",
    "        ('Residual', '#27ae60'),\n",
    "    ]\n",
    "    \n",
    "    for i, (name, color) in enumerate(legend_items):\n",
    "        y = 11 - i * 1.2\n",
    "        rect = FancyBboxPatch((19, y-0.3), 2, 0.6, boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor=color, edgecolor='black', linewidth=1)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(20, y, name, ha='center', va='center', fontsize=8, color='white')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_complete_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Summary\n",
    "\n",
    "### What We Built\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **GPT2Config** | Configuration dataclass with all hyperparameters |\n",
    "| **LayerNorm** | Custom layer normalization |\n",
    "| **CausalSelfAttention** | Multi-head attention with causal masking |\n",
    "| **MLP** | Position-wise feed-forward network |\n",
    "| **Block** | Transformer block with Pre-LN |\n",
    "| **GPT2Model** | Complete model with generation |\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Pre-LayerNorm**: Better gradient flow than original Transformer\n",
    "2. **Scaled Initialization**: 1/√(2N) for residual layers\n",
    "3. **KV-Cache**: Efficient autoregressive generation\n",
    "4. **Multiple Decoding Strategies**: Greedy, temperature, top-k, top-p\n",
    "5. **Weight Tying**: Token embedding = LM head\n",
    "\n",
    "### Usage Patterns\n",
    "\n",
    "```python\n",
    "# Load pretrained model\n",
    "model = load_pretrained_gpt2('gpt2')\n",
    "\n",
    "# Generate text\n",
    "text = generate_text(model, \"The future of AI\", max_new_tokens=50)\n",
    "\n",
    "# Zero-shot tasks\n",
    "tasks = ZeroShotTasks(model)\n",
    "summary = tasks.summarize(article)\n",
    "answer = tasks.answer_question(question)\n",
    "\n",
    "# Fine-tune\n",
    "dataset = TextDataset(texts)\n",
    "train_epoch(model, dataloader, optimizer, device)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Complete GPT-2 Series\n",
    "\n",
    "This concludes our 5-part deep dive into GPT-2:\n",
    "\n",
    "1. **Part I: Genesis** - The vision and WebText dataset\n",
    "2. **Part II: Architecture** - Pre-LN, scaled init, all modifications\n",
    "3. **Part III: Zero-Shot** - Task prompting and emergence\n",
    "4. **Part IV: Training** - Compute, scaling laws, infrastructure\n",
    "5. **Part V: Implementation** - Complete working code\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Radford et al. (2019). [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "2. Karpathy. [nanoGPT](https://github.com/karpathy/nanoGPT)\n",
    "3. Hugging Face. [Transformers Library](https://github.com/huggingface/transformers)\n",
    "4. OpenAI. [GPT-2 GitHub](https://github.com/openai/gpt-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
