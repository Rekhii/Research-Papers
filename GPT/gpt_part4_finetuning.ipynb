{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Research Paper | Part IV\n",
    "\n",
    "## Fine-tuning: Task-Specific Adaptation\n",
    "\n",
    "---\n",
    "\n",
    "**Paper:** [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "\n",
    "**Authors:** Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever (OpenAI, 2018)\n",
    "\n",
    "---\n",
    "\n",
    "This is the **revolutionary contribution** of GPT: showing that a single pre-trained model can be fine-tuned to achieve state-of-the-art results across diverse NLP tasks.\n",
    "\n",
    "In this notebook:\n",
    "1. **The fine-tuning objective** - combining task loss with language modeling\n",
    "2. **Input transformations** - converting any task to GPT's format\n",
    "3. **Task-specific architectures** - minimal changes for each task type\n",
    "4. **Results analysis** - GPT's performance on 12 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Rectangle, FancyArrowPatch, Circle\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The Two-Stage Training Paradigm\n",
    "\n",
    "### 1.1 The Core Idea\n",
    "\n",
    "From Section 1 (Introduction):\n",
    "\n",
    "> *\"We demonstrate that large gains on these tasks can be realized by **generative pre-training** of a language model on a diverse corpus of unlabeled text, followed by **discriminative fine-tuning** on each specific task.\"*\n",
    "\n",
    "This establishes the **pre-train then fine-tune** paradigm that now dominates NLP:\n",
    "\n",
    "| Stage | Data | Objective | Duration |\n",
    "|-------|------|-----------|----------|\n",
    "| **Pre-training** | Unlabeled (BooksCorpus) | Language modeling | 100 epochs, weeks |\n",
    "| **Fine-tuning** | Labeled (task-specific) | Task + LM auxiliary | 3 epochs, hours |\n",
    "\n",
    "### 1.2 Why This Works\n",
    "\n",
    "From Section 1:\n",
    "\n",
    "> *\"In our experiments, we use a combination of unsupervised pre-training and supervised fine-tuning. Our training objective is to learn a universal representation that transfers with little adaptation to a wide range of tasks.\"*\n",
    "\n",
    "The key insight: **language modeling forces the model to learn useful representations**\n",
    "- To predict the next word, you must understand:\n",
    "  - Syntax (grammar)\n",
    "  - Semantics (meaning)\n",
    "  - World knowledge (facts)\n",
    "  - Reasoning patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_two_stage_paradigm():\n",
    "    \"\"\"Visualize the pre-train then fine-tune paradigm.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    ax.set_xlim(0, 16)\n",
    "    ax.set_ylim(0, 8)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Title\n",
    "    ax.text(8, 7.5, 'GPT: Pre-train then Fine-tune Paradigm', \n",
    "            fontsize=16, fontweight='bold', ha='center')\n",
    "    \n",
    "    # === STAGE 1: Pre-training ===\n",
    "    rect1 = FancyBboxPatch((0.5, 3.5), 6, 3, boxstyle=\"round,pad=0.05\",\n",
    "                           facecolor='#e8f4f8', edgecolor='#3498db', linewidth=2)\n",
    "    ax.add_patch(rect1)\n",
    "    \n",
    "    ax.text(3.5, 6.2, 'Stage 1: Pre-training', fontsize=13, fontweight='bold', \n",
    "            ha='center', color='#2980b9')\n",
    "    \n",
    "    # Data\n",
    "    ax.text(1, 5.5, 'Data:', fontsize=10, fontweight='bold')\n",
    "    ax.text(2.5, 5.5, 'BooksCorpus (~1B words)', fontsize=10)\n",
    "    ax.text(1, 5.0, 'Labels:', fontsize=10, fontweight='bold')\n",
    "    ax.text(2.5, 5.0, 'None (self-supervised)', fontsize=10, color='#27ae60')\n",
    "    ax.text(1, 4.5, 'Objective:', fontsize=10, fontweight='bold')\n",
    "    ax.text(2.5, 4.5, 'Predict next token', fontsize=10)\n",
    "    ax.text(1, 4.0, 'Duration:', fontsize=10, fontweight='bold')\n",
    "    ax.text(2.5, 4.0, '100 epochs (~weeks)', fontsize=10)\n",
    "    \n",
    "    # Arrow\n",
    "    ax.annotate('', xy=(7.5, 5), xytext=(6.7, 5),\n",
    "                arrowprops=dict(arrowstyle='->', color='black', lw=2.5))\n",
    "    ax.text(7.1, 5.4, 'Transfer\\nlearned\\nweights', fontsize=9, ha='center')\n",
    "    \n",
    "    # === STAGE 2: Fine-tuning ===\n",
    "    rect2 = FancyBboxPatch((8.5, 3.5), 7, 3, boxstyle=\"round,pad=0.05\",\n",
    "                           facecolor='#fef9e7', edgecolor='#f39c12', linewidth=2)\n",
    "    ax.add_patch(rect2)\n",
    "    \n",
    "    ax.text(12, 6.2, 'Stage 2: Fine-tuning', fontsize=13, fontweight='bold', \n",
    "            ha='center', color='#d68910')\n",
    "    \n",
    "    ax.text(9, 5.5, 'Data:', fontsize=10, fontweight='bold')\n",
    "    ax.text(10.5, 5.5, 'Task-specific (small)', fontsize=10)\n",
    "    ax.text(9, 5.0, 'Labels:', fontsize=10, fontweight='bold')\n",
    "    ax.text(10.5, 5.0, 'Yes (supervised)', fontsize=10, color='#e74c3c')\n",
    "    ax.text(9, 4.5, 'Objective:', fontsize=10, fontweight='bold')\n",
    "    ax.text(10.5, 4.5, 'Task loss + LM auxiliary', fontsize=10)\n",
    "    ax.text(9, 4.0, 'Duration:', fontsize=10, fontweight='bold')\n",
    "    ax.text(10.5, 4.0, '3 epochs (~hours)', fontsize=10)\n",
    "    \n",
    "    # === Equation ===\n",
    "    eq_box = FancyBboxPatch((2, 1), 12, 1.8, boxstyle=\"round,pad=0.05\",\n",
    "                            facecolor='#f5f5f5', edgecolor='gray', linewidth=1.5)\n",
    "    ax.add_patch(eq_box)\n",
    "    \n",
    "    ax.text(8, 2.5, 'The Fine-tuning Objective (Equation 3 from paper):', \n",
    "            fontsize=11, fontweight='bold', ha='center')\n",
    "    ax.text(8, 1.7, r'$L_3(C) = L_2(C) + \\lambda \\cdot L_1(C)$', \n",
    "            fontsize=14, ha='center', style='italic')\n",
    "    ax.text(8, 1.2, 'Task loss + (weight) x Language modeling loss', \n",
    "            fontsize=10, ha='center', color='gray')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_two_stage_paradigm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. The Fine-tuning Objective\n",
    "\n",
    "### 2.1 The Three Equations\n",
    "\n",
    "The paper defines three loss functions. Understanding them is crucial:\n",
    "\n",
    "**Equation 1: Pre-training (Language Modeling)**\n",
    "\n",
    "$$L_1(\\mathcal{U}) = \\sum_i \\log P(u_i | u_{i-k}, ..., u_{i-1}; \\Theta)$$\n",
    "\n",
    "> *\"Given an unsupervised corpus of tokens $\\mathcal{U}$, we use a standard language modeling objective.\"*\n",
    "\n",
    "**Equation 2: Task-Specific Supervised Loss**\n",
    "\n",
    "$$L_2(C) = \\sum_{(x, y)} \\log P(y | x^1, ..., x^m)$$\n",
    "\n",
    "From Section 3.2:\n",
    "\n",
    "> *\"Given a labeled dataset $C$, where each instance consists of a sequence of input tokens, $x^1, ..., x^m$, along with a label $y$, the inputs are passed through our pre-trained model to obtain the final transformer block's activation $h_l^m$, which is then fed into an added linear output layer with parameters $W_y$ to predict $y$.\"*\n",
    "\n",
    "$$P(y | x^1, ..., x^m) = \\text{softmax}(h_l^m W_y)$$\n",
    "\n",
    "**Equation 3: Combined Fine-tuning Objective**\n",
    "\n",
    "$$L_3(C) = L_2(C) + \\lambda \\cdot L_1(C)$$\n",
    "\n",
    "This is the **key innovation**: keep the language modeling objective as an auxiliary loss!\n",
    "\n",
    "### 2.2 Why the Auxiliary LM Loss?\n",
    "\n",
    "From Section 3.2:\n",
    "\n",
    "> *\"We additionally found that including language modeling as an auxiliary objective to the fine-tuning helped learning by (a) improving generalization of the supervised model, and (b) accelerating convergence.\"*\n",
    "\n",
    "The intuition:\n",
    "- The LM objective acts as a **regularizer**\n",
    "- Prevents the model from forgetting useful pre-trained features\n",
    "- Keeps gradients flowing through the entire model\n",
    "\n",
    "### 2.3 The Lambda Hyperparameter\n",
    "\n",
    "From Section 4.1:\n",
    "\n",
    "> *\"For fine-tuning, we use... a weight λ = 0.5 for the auxiliary language model loss.\"*\n",
    "\n",
    "So the actual objective is:\n",
    "\n",
    "$$L_3(C) = L_{\\text{task}} + 0.5 \\cdot L_{\\text{LM}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_finetuning_objective():\n",
    "    \"\"\"Visualize the combined fine-tuning objective.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # === Left: The three equations ===\n",
    "    ax1 = axes[0]\n",
    "    ax1.set_xlim(0, 10)\n",
    "    ax1.set_ylim(0, 10)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax1.text(5, 9.5, 'The Three Equations', fontsize=14, fontweight='bold', ha='center')\n",
    "    \n",
    "    # Equation 1\n",
    "    rect1 = FancyBboxPatch((0.5, 6.5), 9, 2.2, boxstyle=\"round,pad=0.03\",\n",
    "                           facecolor='#e8f4f8', edgecolor='#3498db', linewidth=2)\n",
    "    ax1.add_patch(rect1)\n",
    "    ax1.text(5, 8.3, 'Equation 1: Pre-training (Language Modeling)', \n",
    "             fontsize=11, fontweight='bold', ha='center', color='#2980b9')\n",
    "    ax1.text(5, 7.5, r'$L_1(\\mathcal{U}) = \\sum_i \\log P(u_i | u_{i-k}, ..., u_{i-1})$', \n",
    "             fontsize=12, ha='center')\n",
    "    ax1.text(5, 6.9, '\"Predict the next token\"', fontsize=9, ha='center', \n",
    "             style='italic', color='gray')\n",
    "    \n",
    "    # Equation 2\n",
    "    rect2 = FancyBboxPatch((0.5, 3.5), 9, 2.2, boxstyle=\"round,pad=0.03\",\n",
    "                           facecolor='#fef9e7', edgecolor='#f39c12', linewidth=2)\n",
    "    ax1.add_patch(rect2)\n",
    "    ax1.text(5, 5.3, 'Equation 2: Task-Specific Loss', \n",
    "             fontsize=11, fontweight='bold', ha='center', color='#d68910')\n",
    "    ax1.text(5, 4.5, r'$L_2(C) = \\sum_{(x,y)} \\log P(y | x^1, ..., x^m)$', \n",
    "             fontsize=12, ha='center')\n",
    "    ax1.text(5, 3.9, '\"Predict the task label\"', fontsize=9, ha='center', \n",
    "             style='italic', color='gray')\n",
    "    \n",
    "    # Equation 3\n",
    "    rect3 = FancyBboxPatch((0.5, 0.5), 9, 2.2, boxstyle=\"round,pad=0.03\",\n",
    "                           facecolor='#eafaf1', edgecolor='#27ae60', linewidth=2)\n",
    "    ax1.add_patch(rect3)\n",
    "    ax1.text(5, 2.3, 'Equation 3: Combined Fine-tuning', \n",
    "             fontsize=11, fontweight='bold', ha='center', color='#1e8449')\n",
    "    ax1.text(5, 1.5, r'$L_3(C) = L_2(C) + \\lambda \\cdot L_1(C)$', \n",
    "             fontsize=12, ha='center')\n",
    "    ax1.text(5, 0.9, r'\"Task loss + 0.5 $\\times$ LM loss\"', fontsize=9, ha='center', \n",
    "             style='italic', color='gray')\n",
    "    \n",
    "    # === Right: Why auxiliary loss helps ===\n",
    "    ax2 = axes[1]\n",
    "    ax2.set_xlim(0, 10)\n",
    "    ax2.set_ylim(0, 10)\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    ax2.text(5, 9.5, 'Why Auxiliary LM Loss Helps', fontsize=14, fontweight='bold', ha='center')\n",
    "    \n",
    "    benefits = [\n",
    "        ('1. Regularization', \n",
    "         'Prevents overfitting to small\\ntask-specific datasets'),\n",
    "        ('2. Generalization', \n",
    "         '\"Improving generalization of\\nthe supervised model\"'),\n",
    "        ('3. Faster Convergence', \n",
    "         '\"Accelerating convergence\"'),\n",
    "        ('4. Gradient Flow', \n",
    "         'Keeps all layers active,\\nnot just the classifier'),\n",
    "    ]\n",
    "    \n",
    "    colors = ['#e74c3c', '#3498db', '#27ae60', '#9b59b6']\n",
    "    \n",
    "    for i, ((title, desc), color) in enumerate(zip(benefits, colors)):\n",
    "        y = 8 - i * 2\n",
    "        rect = FancyBboxPatch((0.5, y - 0.8), 9, 1.5, boxstyle=\"round,pad=0.03\",\n",
    "                              facecolor=color, edgecolor='black', linewidth=1.5, alpha=0.15)\n",
    "        ax2.add_patch(rect)\n",
    "        ax2.text(1, y + 0.3, title, fontsize=11, fontweight='bold', color=color)\n",
    "        ax2.text(1, y - 0.3, desc, fontsize=10, color='black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_finetuning_objective()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Input Transformations: The Key Innovation\n",
    "\n",
    "### 3.1 The Challenge\n",
    "\n",
    "GPT is trained for **sequence-to-sequence** language modeling. But NLP tasks have diverse input formats:\n",
    "- **Classification**: Single text → Label\n",
    "- **Entailment**: Premise + Hypothesis → Label\n",
    "- **Similarity**: Text A + Text B → Score\n",
    "- **QA**: Context + Question → Answer\n",
    "- **Multiple Choice**: Question + Options → Choice\n",
    "\n",
    "### 3.2 The Solution: Structured Input Transformations\n",
    "\n",
    "From Section 3.3:\n",
    "\n",
    "> *\"For some tasks, like text classification, we can directly fine-tune our model as described above. Certain other tasks, like question answering or textual entailment, have structured inputs such as ordered sentence pairs, or triplets of document, question, and answers. Since our pre-trained model was trained on contiguous sequences of text, we require some modifications to apply it to these tasks.\"*\n",
    "\n",
    "The key insight:\n",
    "\n",
    "> *\"We use a traversal-style approach, where we convert structured inputs into an ordered sequence that our pre-trained model can process. These input transformations allow us to avoid making extensive changes to the architecture across tasks.\"*\n",
    "\n",
    "### 3.3 Special Tokens\n",
    "\n",
    "The paper introduces special delimiter tokens:\n",
    "\n",
    "| Token | Purpose |\n",
    "|-------|--------|\n",
    "| `<s>` | Start of sequence |\n",
    "| `<e>` | End of sequence / Extract token |\n",
    "| `$` | Delimiter between segments |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_input_transformations():\n",
    "    \"\"\"\n",
    "    Visualize the four input transformation types from Figure 1 of the paper.\n",
    "    This is one of the most important figures in the paper!\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Colors\n",
    "    c_start = '#3498db'    # Start token\n",
    "    c_text = '#ecf0f1'     # Text\n",
    "    c_delim = '#e74c3c'    # Delimiter\n",
    "    c_extract = '#27ae60'  # Extract token\n",
    "    \n",
    "    def draw_token_sequence(ax, tokens, colors, y, title, subtitle):\n",
    "        \"\"\"Draw a sequence of tokens.\"\"\"\n",
    "        ax.set_xlim(0, 14)\n",
    "        ax.set_ylim(0, 6)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        ax.text(7, 5.5, title, fontsize=13, fontweight='bold', ha='center')\n",
    "        ax.text(7, 5, subtitle, fontsize=10, ha='center', style='italic', color='gray')\n",
    "        \n",
    "        # Calculate positions\n",
    "        total_width = sum([len(t) * 0.15 + 0.8 for t in tokens])\n",
    "        start_x = (14 - total_width) / 2\n",
    "        \n",
    "        x = start_x\n",
    "        for tok, col in zip(tokens, colors):\n",
    "            width = len(tok) * 0.15 + 0.6\n",
    "            rect = FancyBboxPatch((x, y), width, 0.7, boxstyle=\"round,pad=0.02\",\n",
    "                                  facecolor=col, edgecolor='black', linewidth=1.5)\n",
    "            ax.add_patch(rect)\n",
    "            text_color = 'white' if col in [c_start, c_delim, c_extract] else 'black'\n",
    "            ax.text(x + width/2, y + 0.35, tok, ha='center', va='center', \n",
    "                   fontsize=9, color=text_color, fontweight='bold')\n",
    "            x += width + 0.1\n",
    "        \n",
    "        return start_x, x\n",
    "    \n",
    "    # === 1. Classification ===\n",
    "    ax1 = axes[0, 0]\n",
    "    tokens1 = ['<s>', 'This', 'movie', 'was', 'great', '!', '<e>']\n",
    "    colors1 = [c_start, c_text, c_text, c_text, c_text, c_text, c_extract]\n",
    "    draw_token_sequence(ax1, tokens1, colors1, 3, \n",
    "                        'Classification (e.g., Sentiment)',\n",
    "                        'Single text sequence')\n",
    "    \n",
    "    ax1.text(7, 2.2, 'Format: <s> Text <e>', fontsize=11, ha='center', \n",
    "             fontweight='bold', color='#2c3e50')\n",
    "    ax1.text(7, 1.6, 'Extract representation at <e> position', fontsize=10, \n",
    "             ha='center', color='gray')\n",
    "    ax1.text(7, 1.0, 'Linear layer maps to class probabilities', fontsize=10, \n",
    "             ha='center', color='gray')\n",
    "    \n",
    "    # === 2. Entailment ===\n",
    "    ax2 = axes[0, 1]\n",
    "    tokens2 = ['<s>', 'Premise', 'text', '$', 'Hypothesis', 'text', '<e>']\n",
    "    colors2 = [c_start, c_text, c_text, c_delim, c_text, c_text, c_extract]\n",
    "    draw_token_sequence(ax2, tokens2, colors2, 3,\n",
    "                        'Entailment (e.g., MNLI, SNLI)',\n",
    "                        'Premise-hypothesis pairs')\n",
    "    \n",
    "    ax2.text(7, 2.2, 'Format: <s> Premise $ Hypothesis <e>', fontsize=11, \n",
    "             ha='center', fontweight='bold', color='#2c3e50')\n",
    "    ax2.text(7, 1.6, 'Delimiter $ separates the two segments', fontsize=10, \n",
    "             ha='center', color='gray')\n",
    "    ax2.text(7, 1.0, 'Predict: entailment / contradiction / neutral', fontsize=10, \n",
    "             ha='center', color='gray')\n",
    "    \n",
    "    # === 3. Similarity ===\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.set_xlim(0, 14)\n",
    "    ax3.set_ylim(0, 6)\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    ax3.text(7, 5.5, 'Similarity (e.g., QQP, STS-B)', fontsize=13, \n",
    "             fontweight='bold', ha='center')\n",
    "    ax3.text(7, 5, 'Two text sequences (order independent)', fontsize=10, \n",
    "             ha='center', style='italic', color='gray')\n",
    "    \n",
    "    # Two orderings\n",
    "    tokens3a = ['<s>', 'Text', 'A', '$', 'Text', 'B', '<e>']\n",
    "    colors3 = [c_start, c_text, c_text, c_delim, c_text, c_text, c_extract]\n",
    "    \n",
    "    x = 1\n",
    "    for tok, col in zip(tokens3a, colors3):\n",
    "        width = len(tok) * 0.12 + 0.5\n",
    "        rect = FancyBboxPatch((x, 3.5), width, 0.6, boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor=col, edgecolor='black', linewidth=1.5)\n",
    "        ax3.add_patch(rect)\n",
    "        text_color = 'white' if col in [c_start, c_delim, c_extract] else 'black'\n",
    "        ax3.text(x + width/2, 3.8, tok, ha='center', va='center', \n",
    "                fontsize=9, color=text_color, fontweight='bold')\n",
    "        x += width + 0.08\n",
    "    \n",
    "    tokens3b = ['<s>', 'Text', 'B', '$', 'Text', 'A', '<e>']\n",
    "    x = 1\n",
    "    for tok, col in zip(tokens3b, colors3):\n",
    "        width = len(tok) * 0.12 + 0.5\n",
    "        rect = FancyBboxPatch((x, 2.5), width, 0.6, boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor=col, edgecolor='black', linewidth=1.5)\n",
    "        ax3.add_patch(rect)\n",
    "        text_color = 'white' if col in [c_start, c_delim, c_extract] else 'black'\n",
    "        ax3.text(x + width/2, 2.8, tok, ha='center', va='center', \n",
    "                fontsize=9, color=text_color, fontweight='bold')\n",
    "        x += width + 0.08\n",
    "    \n",
    "    ax3.text(7.5, 3.8, '+', fontsize=16, fontweight='bold', ha='center', va='center')\n",
    "    \n",
    "    ax3.text(7, 1.8, 'Process BOTH orderings, add representations', fontsize=10, \n",
    "             ha='center', fontweight='bold', color='#2c3e50')\n",
    "    ax3.text(7, 1.2, 'Makes the model symmetric to input order', fontsize=10, \n",
    "             ha='center', color='gray')\n",
    "    \n",
    "    # === 4. Multiple Choice ===\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.set_xlim(0, 14)\n",
    "    ax4.set_ylim(0, 6)\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    ax4.text(7, 5.5, 'Multiple Choice (e.g., RACE, ROCStories)', fontsize=13, \n",
    "             fontweight='bold', ha='center')\n",
    "    ax4.text(7, 5, 'Context + Question + Answer options', fontsize=10, \n",
    "             ha='center', style='italic', color='gray')\n",
    "    \n",
    "    # Multiple sequences\n",
    "    y_positions = [4, 3.2, 2.4]\n",
    "    answer_labels = ['A', 'B', 'C']\n",
    "    \n",
    "    for y, ans in zip(y_positions, answer_labels):\n",
    "        tokens4 = ['<s>', 'Context', '$', 'Question', '$', f'Ans {ans}', '<e>']\n",
    "        x = 1.5\n",
    "        for tok, col in zip(tokens4, colors3):\n",
    "            width = len(tok) * 0.1 + 0.4\n",
    "            rect = FancyBboxPatch((x, y), width, 0.55, boxstyle=\"round,pad=0.02\",\n",
    "                                  facecolor=col, edgecolor='black', linewidth=1.2)\n",
    "            ax4.add_patch(rect)\n",
    "            text_color = 'white' if col in [c_start, c_delim, c_extract] else 'black'\n",
    "            ax4.text(x + width/2, y + 0.27, tok, ha='center', va='center', \n",
    "                    fontsize=8, color=text_color, fontweight='bold')\n",
    "            x += width + 0.05\n",
    "        \n",
    "        # Score\n",
    "        ax4.text(x + 0.3, y + 0.27, f'-> Score({ans})', fontsize=9, va='center')\n",
    "    \n",
    "    ax4.text(7, 1.5, 'Create N sequences (one per answer option)', fontsize=10, \n",
    "             ha='center', fontweight='bold', color='#2c3e50')\n",
    "    ax4.text(7, 0.9, 'Softmax over scores to select best answer', fontsize=10, \n",
    "             ha='center', color='gray')\n",
    "    \n",
    "    # Legend\n",
    "    fig.text(0.5, 0.02, 'Legend:  ', fontsize=10, ha='center', fontweight='bold')\n",
    "    legend_items = [(c_start, '<s> Start'), (c_text, 'Text'), \n",
    "                    (c_delim, '$ Delimiter'), (c_extract, '<e> Extract')]\n",
    "    for i, (color, label) in enumerate(legend_items):\n",
    "        fig.text(0.35 + i * 0.1, 0.02, f'  {label}  ', fontsize=9, \n",
    "                ha='center', backgroundcolor=color,\n",
    "                color='white' if color != c_text else 'black')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "    plt.show()\n",
    "\n",
    "visualize_input_transformations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Paper Quotes for Each Task Type\n",
    "\n",
    "**Classification:**\n",
    "> *\"For text classification, we simply fine-tune our model directly. We add a special start and end token to the input.\"*\n",
    "\n",
    "**Entailment:**\n",
    "> *\"For entailment tasks, we concatenate the premise p and hypothesis h token sequences, with a delimiter token ($) in between.\"*\n",
    "\n",
    "**Similarity:**\n",
    "> *\"For similarity tasks, there is no inherent ordering of the two sentences being compared. To reflect this, we modify the input sequence to contain both possible sentence orderings (with a delimiter in between) and process each independently to produce two sequence representations $h_l^m$ which are added element-wise before being fed into the linear output layer.\"*\n",
    "\n",
    "**Multiple Choice:**\n",
    "> *\"For these tasks, we are given a context document z, a question q, and a set of possible answers $\\{a_k\\}$. We concatenate the document context and question with each possible answer, adding a delimiter token in between to get $[z; q; \\$; a_k]$. Each of these sequences are processed independently with our model and then normalized via a softmax layer to produce an output distribution over possible answers.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation: Fine-tuning Architecture\n",
    "\n",
    "### 4.1 The Classification Head\n",
    "\n",
    "From Section 3.2:\n",
    "\n",
    "> *\"The inputs are passed through our pre-trained model to obtain the final transformer block's activation $h_l^m$, which is then fed into an added linear output layer with parameters $W_y$ to predict $y$.\"*\n",
    "\n",
    "$$P(y | x^1, ..., x^m) = \\text{softmax}(h_l^m W_y)$$\n",
    "\n",
    "The architecture is remarkably simple:\n",
    "1. Pre-trained GPT (unchanged)\n",
    "2. One linear layer for classification\n",
    "\n",
    "This is why GPT is so efficient - **minimal task-specific parameters**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Model from Part II (simplified) ===\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int = 40478\n",
    "    n_positions: int = 512\n",
    "    n_embd: int = 768\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_inner: int = 3072\n",
    "    embd_pdrop: float = 0.1\n",
    "    attn_pdrop: float = 0.1\n",
    "    resid_pdrop: float = 0.1\n",
    "\n",
    "def gelu_approx(x):\n",
    "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, n_embd, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(n_embd))\n",
    "        self.beta = nn.Parameter(torch.zeros(n_embd))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return self.gamma * (x - mean) / torch.sqrt(var + self.eps) + self.beta\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = config.n_embd // config.n_head\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        mask = torch.tril(torch.ones(config.n_positions, config.n_positions))\n",
    "        self.register_buffer('mask', mask.view(1, 1, config.n_positions, config.n_positions))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        attn = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn = attn.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        attn = self.attn_dropout(F.softmax(attn, dim=-1))\n",
    "        out = (attn @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.resid_dropout(self.c_proj(out))\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, config.n_inner)\n",
    "        self.c_proj = nn.Linear(config.n_inner, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(gelu_approx(self.c_fc(x))))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.wte.weight\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids, targets=None):\n",
    "        B, T = input_ids.shape\n",
    "        x = self.drop(self.wte(input_ids) + self.wpe(torch.arange(T, device=input_ids.device)))\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss, x  # Also return hidden states\n",
    "\n",
    "config = GPTConfig()\n",
    "print(f\"Base GPT model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTForSequenceClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT with a classification head for fine-tuning.\n",
    "    \n",
    "    From paper Section 3.2:\n",
    "    \"The inputs are passed through our pre-trained model to obtain the \n",
    "    final transformer block's activation h_l^m, which is then fed into \n",
    "    an added linear output layer with parameters W_y to predict y.\"\n",
    "    \n",
    "    P(y | x^1, ..., x^m) = softmax(h_l^m * W_y)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config, num_labels: int, lm_weight: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.gpt = GPT(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.lm_weight = lm_weight  # Lambda from Equation 3\n",
    "        \n",
    "        # Classification head: single linear layer\n",
    "        # \"fed into an added linear output layer with parameters W_y\"\n",
    "        self.classifier = nn.Linear(config.n_embd, num_labels)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Initialize classifier\n",
    "        torch.nn.init.normal_(self.classifier.weight, std=0.02)\n",
    "        torch.nn.init.zeros_(self.classifier.bias)\n",
    "        \n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        classifier_params = sum(p.numel() for p in self.classifier.parameters())\n",
    "        print(f\"GPT for Classification: {n_params:,} total parameters\")\n",
    "        print(f\"  - Base GPT: {n_params - classifier_params:,}\")\n",
    "        print(f\"  - Classifier head: {classifier_params:,} (only new parameters!)\")\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: torch.Tensor,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        extract_positions: Optional[torch.Tensor] = None,\n",
    "        lm_targets: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward pass with combined loss (Equation 3).\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs, shape (batch, seq_len)\n",
    "            labels: Classification labels, shape (batch,)\n",
    "            extract_positions: Position of <e> token for each sample\n",
    "            lm_targets: Targets for auxiliary LM loss\n",
    "        \n",
    "        Returns:\n",
    "            logits: Classification logits\n",
    "            loss: Combined loss (L_3 = L_2 + lambda * L_1)\n",
    "        \"\"\"\n",
    "        batch_size = input_ids.size(0)\n",
    "        \n",
    "        # Get GPT outputs\n",
    "        lm_logits, lm_loss, hidden_states = self.gpt(input_ids, lm_targets)\n",
    "        \n",
    "        # Extract representation at <e> position (or last position)\n",
    "        if extract_positions is None:\n",
    "            # Default: use last position\n",
    "            extract_positions = torch.tensor([input_ids.size(1) - 1] * batch_size)\n",
    "        \n",
    "        # Get h_l^m for each sample\n",
    "        batch_indices = torch.arange(batch_size)\n",
    "        pooled_output = hidden_states[batch_indices, extract_positions]  # (batch, n_embd)\n",
    "        \n",
    "        # Apply dropout and classifier\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        classification_logits = self.classifier(pooled_output)  # (batch, num_labels)\n",
    "        \n",
    "        # Compute combined loss (Equation 3)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # L_2: Task-specific loss\n",
    "            task_loss = F.cross_entropy(classification_logits, labels)\n",
    "            \n",
    "            # L_3 = L_2 + lambda * L_1\n",
    "            if lm_loss is not None:\n",
    "                loss = task_loss + self.lm_weight * lm_loss\n",
    "            else:\n",
    "                loss = task_loss\n",
    "        \n",
    "        return classification_logits, loss\n",
    "\n",
    "\n",
    "# Create model for binary classification (e.g., sentiment)\n",
    "model_cls = GPTForSequenceClassification(config, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTForMultipleChoice(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT for multiple choice tasks (e.g., RACE, ROCStories).\n",
    "    \n",
    "    From paper Section 3.3:\n",
    "    \"We concatenate the document context and question with each possible answer,\n",
    "    adding a delimiter token in between to get [z; q; $; a_k]. Each of these \n",
    "    sequences are processed independently with our model and then normalized \n",
    "    via a softmax layer to produce an output distribution over possible answers.\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config, lm_weight: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.gpt = GPT(config)\n",
    "        self.lm_weight = lm_weight\n",
    "        \n",
    "        # Score head: maps hidden state to scalar score\n",
    "        self.score_head = nn.Linear(config.n_embd, 1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        torch.nn.init.normal_(self.score_head.weight, std=0.02)\n",
    "        torch.nn.init.zeros_(self.score_head.bias)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,  # (batch, num_choices, seq_len)\n",
    "        labels: Optional[torch.Tensor] = None,  # (batch,) index of correct answer\n",
    "        lm_targets: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Process each choice independently, then softmax over scores.\n",
    "        \"\"\"\n",
    "        batch_size, num_choices, seq_len = input_ids.shape\n",
    "        \n",
    "        # Flatten for processing\n",
    "        flat_input_ids = input_ids.view(-1, seq_len)  # (batch * num_choices, seq_len)\n",
    "        \n",
    "        # Get GPT outputs\n",
    "        _, lm_loss, hidden_states = self.gpt(flat_input_ids, None)\n",
    "        \n",
    "        # Extract representation at last position\n",
    "        pooled = hidden_states[:, -1, :]  # (batch * num_choices, n_embd)\n",
    "        pooled = self.dropout(pooled)\n",
    "        \n",
    "        # Get scores\n",
    "        scores = self.score_head(pooled).squeeze(-1)  # (batch * num_choices,)\n",
    "        scores = scores.view(batch_size, num_choices)  # (batch, num_choices)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            task_loss = F.cross_entropy(scores, labels)\n",
    "            loss = task_loss\n",
    "            if lm_loss is not None:\n",
    "                loss = task_loss + self.lm_weight * lm_loss\n",
    "        \n",
    "        return scores, loss\n",
    "\n",
    "\n",
    "print(\"\\nGPT for Multiple Choice:\")\n",
    "model_mc = GPTForMultipleChoice(config)\n",
    "print(f\"  Score head parameters: {sum(p.numel() for p in model_mc.score_head.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTForSimilarity(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT for similarity tasks (e.g., QQP, STS-B).\n",
    "    \n",
    "    From paper Section 3.3:\n",
    "    \"For similarity tasks, there is no inherent ordering of the two sentences \n",
    "    being compared. To reflect this, we modify the input sequence to contain \n",
    "    both possible sentence orderings (with a delimiter in between) and process \n",
    "    each independently to produce two sequence representations h_l^m which are \n",
    "    added element-wise before being fed into the linear output layer.\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config, num_labels: int = 1, lm_weight: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.gpt = GPT(config)\n",
    "        self.lm_weight = lm_weight\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        # Output head\n",
    "        self.classifier = nn.Linear(config.n_embd, num_labels)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids_ab: torch.Tensor,  # (batch, seq_len) - \"A $ B\"\n",
    "        input_ids_ba: torch.Tensor,  # (batch, seq_len) - \"B $ A\"\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Process both orderings and add representations.\n",
    "        \"\"\"\n",
    "        # Process A $ B\n",
    "        _, _, hidden_ab = self.gpt(input_ids_ab, None)\n",
    "        pooled_ab = hidden_ab[:, -1, :]  # (batch, n_embd)\n",
    "        \n",
    "        # Process B $ A\n",
    "        _, _, hidden_ba = self.gpt(input_ids_ba, None)\n",
    "        pooled_ba = hidden_ba[:, -1, :]  # (batch, n_embd)\n",
    "        \n",
    "        # Element-wise addition (key insight from paper!)\n",
    "        combined = pooled_ab + pooled_ba  # (batch, n_embd)\n",
    "        combined = self.dropout(combined)\n",
    "        \n",
    "        # Predict\n",
    "        logits = self.classifier(combined)  # (batch, num_labels)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                # Regression (e.g., STS-B)\n",
    "                loss = F.mse_loss(logits.squeeze(), labels.float())\n",
    "            else:\n",
    "                # Classification (e.g., QQP)\n",
    "                loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "print(\"\\nGPT for Similarity (processes both orderings):\")\n",
    "model_sim = GPTForSimilarity(config, num_labels=2)\n",
    "print(f\"  This makes the model symmetric to input order\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Fine-tuning Hyperparameters\n",
    "\n",
    "### 5.1 What the Paper Says\n",
    "\n",
    "From Section 4.1:\n",
    "\n",
    "> *\"For fine-tuning, we use... a learning rate of 6.25e-5, a batchsize of 32, and a linear learning rate decay schedule with warmup over 0.2% of training. We use a weight λ = 0.5 for the auxiliary language model loss. We train for 3 epochs.\"*\n",
    "\n",
    "### 5.2 Complete Hyperparameter Table\n",
    "\n",
    "| Hyperparameter | Pre-training | Fine-tuning |\n",
    "|----------------|--------------|-------------|\n",
    "| **Learning rate** | 2.5e-4 | **6.25e-5** (25x smaller!) |\n",
    "| **Batch size** | 64 | **32** |\n",
    "| **Epochs** | 100 | **3** |\n",
    "| **LR warmup** | 2000 steps | **0.2% of training** |\n",
    "| **LR schedule** | Cosine | **Linear decay** |\n",
    "| **Dropout** | 0.1 | **0.1** (unchanged) |\n",
    "| **λ (LM weight)** | N/A | **0.5** |\n",
    "\n",
    "### 5.3 Why These Choices?\n",
    "\n",
    "**Smaller learning rate (6.25e-5):**\n",
    "- Pre-trained weights are already good\n",
    "- Don't want to destroy learned features\n",
    "- Just need small adjustments for the task\n",
    "\n",
    "**Only 3 epochs:**\n",
    "- Task datasets are small (thousands of examples)\n",
    "- More epochs would lead to overfitting\n",
    "- Pre-trained features do most of the work\n",
    "\n",
    "**λ = 0.5:**\n",
    "- Balance between task and language modeling\n",
    "- Too small: lose regularization benefit\n",
    "- Too large: task learning is hampered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FineTuningConfig:\n",
    "    \"\"\"\n",
    "    Fine-tuning configuration from Section 4.1.\n",
    "    \"\"\"\n",
    "    # === From Section 4.1 ===\n",
    "    learning_rate: float = 6.25e-5      # \"learning rate of 6.25e-5\"\n",
    "    batch_size: int = 32                 # \"batchsize of 32\"\n",
    "    epochs: int = 3                      # \"train for 3 epochs\"\n",
    "    warmup_fraction: float = 0.002       # \"warmup over 0.2% of training\"\n",
    "    lm_weight: float = 0.5               # \"weight lambda = 0.5\"\n",
    "    \n",
    "    # === Standard ===\n",
    "    dropout: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "\n",
    "\n",
    "ft_config = FineTuningConfig()\n",
    "\n",
    "print(\"Fine-tuning Configuration (from paper Section 4.1)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n[Optimization]\")\n",
    "print(f\"  Learning rate:     {ft_config.learning_rate} (vs 2.5e-4 pre-training)\")\n",
    "print(f\"  Batch size:        {ft_config.batch_size}\")\n",
    "print(f\"  Epochs:            {ft_config.epochs}\")\n",
    "print(f\"  Warmup:            {ft_config.warmup_fraction*100}% of training\")\n",
    "print(f\"\\n[Auxiliary Loss]\")\n",
    "print(f\"  LM weight (lambda): {ft_config.lm_weight}\")\n",
    "print(f\"  L3 = L_task + {ft_config.lm_weight} * L_LM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Results Analysis\n",
    "\n",
    "### 6.1 Datasets\n",
    "\n",
    "GPT was evaluated on 12 datasets across 4 task categories:\n",
    "\n",
    "| Category | Datasets | Task Type |\n",
    "|----------|----------|----------|\n",
    "| **Natural Language Inference** | SNLI, MNLI, QNLI, SciTail, RTE | Entailment |\n",
    "| **Question Answering** | RACE, Story Cloze | Multiple Choice |\n",
    "| **Semantic Similarity** | QQP, STS-B, MRPC | Similarity |\n",
    "| **Classification** | CoLA, SST-2 | Single sequence |\n",
    "\n",
    "### 6.2 Main Results\n",
    "\n",
    "From Table 2 in the paper, GPT achieved **state-of-the-art on 9 out of 12 tasks**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results():\n",
    "    \"\"\"Visualize GPT's results on various benchmarks.\"\"\"\n",
    "    \n",
    "    # Results from paper Table 2\n",
    "    results = {\n",
    "        'Natural Language Inference': [\n",
    "            ('SNLI', 89.9, 'Previous SOTA: 89.3'),\n",
    "            ('MNLI-m', 82.1, 'Previous SOTA: 80.6'),\n",
    "            ('MNLI-mm', 81.4, 'Previous SOTA: 80.1'),\n",
    "            ('QNLI', 88.1, 'Previous SOTA: 82.3'),\n",
    "            ('RTE', 56.0, 'Previous: 61.7'),\n",
    "        ],\n",
    "        'Question Answering': [\n",
    "            ('RACE-m', 62.9, 'Previous SOTA: 55.7'),\n",
    "            ('RACE-h', 57.4, 'Previous SOTA: 53.3'),\n",
    "            ('Story Cloze', 86.5, 'Previous SOTA: 77.6'),\n",
    "        ],\n",
    "        'Semantic Similarity': [\n",
    "            ('QQP', 70.3, 'Acc / F1'),\n",
    "            ('STS-B', 82.0, 'Pearson / Spearman'),\n",
    "            ('MRPC', 82.3, 'Acc / F1'),\n",
    "        ],\n",
    "        'Classification': [\n",
    "            ('CoLA', 45.4, 'Matthew\\'s Corr'),\n",
    "            ('SST-2', 91.3, 'Previous SOTA: 90.2'),\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    colors = ['#3498db', '#e74c3c', '#27ae60', '#9b59b6']\n",
    "    \n",
    "    for idx, (category, data) in enumerate(results.items()):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        names = [d[0] for d in data]\n",
    "        scores = [d[1] for d in data]\n",
    "        notes = [d[2] for d in data]\n",
    "        \n",
    "        y_pos = np.arange(len(names))\n",
    "        bars = ax.barh(y_pos, scores, color=colors[idx], edgecolor='black', height=0.6)\n",
    "        \n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(names, fontsize=10)\n",
    "        ax.set_xlabel('Score', fontsize=11)\n",
    "        ax.set_title(category, fontsize=12, fontweight='bold')\n",
    "        ax.set_xlim(0, 100)\n",
    "        ax.grid(True, axis='x', alpha=0.3)\n",
    "        \n",
    "        # Add score labels\n",
    "        for bar, score in zip(bars, scores):\n",
    "            ax.text(score + 1, bar.get_y() + bar.get_height()/2, \n",
    "                   f'{score}', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('GPT Results on Downstream Tasks\\n(State-of-the-art on 9/12 datasets)', \n",
    "                 fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nKey achievements:\")\n",
    "    print(\"  - RACE (reading comprehension): +7.2% absolute improvement\")\n",
    "    print(\"  - Story Cloze: +8.9% absolute improvement\")\n",
    "    print(\"  - QNLI: +5.8% absolute improvement\")\n",
    "    print(\"  - All with same pre-trained model + minimal task-specific parameters\")\n",
    "\n",
    "visualize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Ablation Studies\n",
    "\n",
    "The paper includes important ablation studies (Table 5):\n",
    "\n",
    "**Effect of Auxiliary LM Loss:**\n",
    "\n",
    "| Setting | RACE | QQP | MRPC | CoLA | SST-2 |\n",
    "|---------|------|-----|------|------|-------|\n",
    "| **Full model** | **59.2** | **70.3** | **82.3** | **45.4** | **91.3** |\n",
    "| Without aux LM | 57.5 | 69.8 | 79.4 | 44.2 | 91.2 |\n",
    "\n",
    "The auxiliary LM loss helps on most tasks, especially smaller datasets!\n",
    "\n",
    "**Effect of Pre-training:**\n",
    "\n",
    "| Setting | RACE | QQP | MRPC | CoLA | SST-2 |\n",
    "|---------|------|-----|------|------|-------|\n",
    "| **Full model** | **59.2** | **70.3** | **82.3** | **45.4** | **91.3** |\n",
    "| No pre-training | 48.1 | 69.1 | 72.5 | 17.5 | 82.1 |\n",
    "\n",
    "Pre-training provides **massive improvements**, especially on smaller datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ablations():\n",
    "    \"\"\"Visualize ablation study results.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # === Auxiliary LM loss ablation ===\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    tasks = ['RACE', 'QQP', 'MRPC', 'CoLA', 'SST-2']\n",
    "    with_aux = [59.2, 70.3, 82.3, 45.4, 91.3]\n",
    "    without_aux = [57.5, 69.8, 79.4, 44.2, 91.2]\n",
    "    \n",
    "    x = np.arange(len(tasks))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, with_aux, width, label='With aux LM loss', \n",
    "                    color='#27ae60', edgecolor='black')\n",
    "    bars2 = ax1.bar(x + width/2, without_aux, width, label='Without aux LM loss', \n",
    "                    color='#e74c3c', edgecolor='black')\n",
    "    \n",
    "    ax1.set_ylabel('Score', fontsize=11)\n",
    "    ax1.set_title('Effect of Auxiliary LM Loss\\n(Equation 3: L3 = L2 + lambda * L1)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(tasks)\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.set_ylim(0, 100)\n",
    "    ax1.grid(True, axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add improvement annotations\n",
    "    for i, (w, wo) in enumerate(zip(with_aux, without_aux)):\n",
    "        diff = w - wo\n",
    "        if diff > 0:\n",
    "            ax1.annotate(f'+{diff:.1f}', xy=(i, max(w, wo) + 2), \n",
    "                        ha='center', fontsize=9, color='green', fontweight='bold')\n",
    "    \n",
    "    # === Pre-training ablation ===\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    with_pt = [59.2, 70.3, 82.3, 45.4, 91.3]\n",
    "    without_pt = [48.1, 69.1, 72.5, 17.5, 82.1]\n",
    "    \n",
    "    bars3 = ax2.bar(x - width/2, with_pt, width, label='With pre-training', \n",
    "                    color='#3498db', edgecolor='black')\n",
    "    bars4 = ax2.bar(x + width/2, without_pt, width, label='Without pre-training', \n",
    "                    color='#f39c12', edgecolor='black')\n",
    "    \n",
    "    ax2.set_ylabel('Score', fontsize=11)\n",
    "    ax2.set_title('Effect of Pre-training\\n(The core contribution of GPT)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(tasks)\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.set_ylim(0, 100)\n",
    "    ax2.grid(True, axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add improvement annotations\n",
    "    for i, (w, wo) in enumerate(zip(with_pt, without_pt)):\n",
    "        diff = w - wo\n",
    "        ax2.annotate(f'+{diff:.1f}', xy=(i, max(w, wo) + 2), \n",
    "                    ha='center', fontsize=9, color='blue', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nKey insights from ablations:\")\n",
    "    print(\"  1. Auxiliary LM loss helps on most tasks (especially smaller datasets)\")\n",
    "    print(\"  2. Pre-training is CRUCIAL - CoLA drops from 45.4 to 17.5 without it!\")\n",
    "    print(\"  3. Larger datasets (QQP) benefit less from pre-training\")\n",
    "\n",
    "visualize_ablations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Zero-Shot Performance\n",
    "\n",
    "From Section 5.1:\n",
    "\n",
    "> *\"We observe that GPT shows improvements over the baselines on all datasets except CoLA. This demonstrates that the model can perform a wide range of tasks with little or no supervision.\"*\n",
    "\n",
    "Even **without fine-tuning**, GPT shows reasonable performance - hinting at the emergent capabilities that would be fully realized in GPT-2 and GPT-3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Putting It All Together: Fine-tuning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_finetuning():\n",
    "    \"\"\"\n",
    "    Demonstrate the complete fine-tuning pipeline.\n",
    "    \"\"\"\n",
    "    print(\"Complete Fine-tuning Pipeline Demo\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Load pre-trained model\n",
    "    print(\"\\n1. Load pre-trained GPT model\")\n",
    "    config = GPTConfig()\n",
    "    model = GPTForSequenceClassification(config, num_labels=3)  # 3-way classification\n",
    "    \n",
    "    # 2. Prepare data\n",
    "    print(\"\\n2. Prepare input data (using input transformation)\")\n",
    "    batch_size = 4\n",
    "    seq_len = 64\n",
    "    \n",
    "    # Simulated input: <s> premise $ hypothesis <e>\n",
    "    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n",
    "    labels = torch.randint(0, 3, (batch_size,))  # entail/contradict/neutral\n",
    "    \n",
    "    # For auxiliary LM loss\n",
    "    lm_targets = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n",
    "    \n",
    "    print(f\"   Input shape: {input_ids.shape}\")\n",
    "    print(f\"   Labels shape: {labels.shape}\")\n",
    "    \n",
    "    # 3. Setup optimizer\n",
    "    print(\"\\n3. Setup optimizer (from paper: lr=6.25e-5)\")\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=6.25e-5, weight_decay=0.01)\n",
    "    \n",
    "    # 4. Training step\n",
    "    print(\"\\n4. Training step with combined loss\")\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    logits, loss = model(input_ids, labels=labels, lm_targets=lm_targets)\n",
    "    \n",
    "    print(f\"   Logits shape: {logits.shape}\")\n",
    "    print(f\"   Combined loss (L3 = L2 + 0.5*L1): {loss.item():.4f}\")\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"   Backward pass complete!\")\n",
    "    \n",
    "    # 5. Evaluation\n",
    "    print(\"\\n5. Evaluation\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits, _ = model(input_ids)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        accuracy = (preds == labels).float().mean()\n",
    "    \n",
    "    print(f\"   Predictions: {preds.tolist()}\")\n",
    "    print(f\"   True labels: {labels.tolist()}\")\n",
    "    print(f\"   Accuracy: {accuracy.item()*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Fine-tuning pipeline complete!\")\n",
    "    print(\"\\nIn practice:\")\n",
    "    print(\"  - Load actual pre-trained weights\")\n",
    "    print(\"  - Use real task data (MNLI, SNLI, etc.)\")\n",
    "    print(\"  - Train for 3 epochs\")\n",
    "    print(\"  - Evaluate on held-out test set\")\n",
    "\n",
    "demo_finetuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Summary\n",
    "\n",
    "### 8.1 Key Contributions of GPT Fine-tuning\n",
    "\n",
    "| Innovation | Description | Impact |\n",
    "|------------|-------------|--------|\n",
    "| **Input transformations** | Convert any task to sequence format | One model, many tasks |\n",
    "| **Minimal task-specific params** | Just one linear layer | Efficient transfer |\n",
    "| **Auxiliary LM loss** | L3 = L2 + λ*L1 | Better generalization |\n",
    "| **Pre-train then fine-tune** | Two-stage paradigm | Now industry standard |\n",
    "\n",
    "### 8.2 The Four Task Types\n",
    "\n",
    "| Task Type | Format | Example Datasets |\n",
    "|-----------|--------|------------------|\n",
    "| **Classification** | `<s> text <e>` | SST-2, CoLA |\n",
    "| **Entailment** | `<s> premise $ hypothesis <e>` | MNLI, SNLI, RTE |\n",
    "| **Similarity** | Both orderings, add | QQP, STS-B, MRPC |\n",
    "| **Multiple Choice** | One sequence per choice | RACE, Story Cloze |\n",
    "\n",
    "### 8.3 Fine-tuning Recipe\n",
    "\n",
    "```\n",
    "1. Start with pre-trained GPT weights\n",
    "2. Add task-specific linear head\n",
    "3. Transform inputs using appropriate format\n",
    "4. Train with L3 = L_task + 0.5 * L_LM\n",
    "5. Use lr=6.25e-5, batch=32, epochs=3\n",
    "```\n",
    "\n",
    "### 8.4 Historical Impact\n",
    "\n",
    "GPT established the **foundation** for:\n",
    "- **BERT** (Oct 2018): Same pre-train/fine-tune, bidirectional\n",
    "- **GPT-2** (Feb 2019): Larger scale, zero-shot capabilities\n",
    "- **GPT-3** (Jun 2020): 175B parameters, in-context learning\n",
    "- **ChatGPT** (Nov 2022): RLHF fine-tuning for dialogue\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Radford et al. (2018). [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "2. Wang et al. (2018). [GLUE: A Multi-Task Benchmark](https://arxiv.org/abs/1804.07461)\n",
    "3. Bowman et al. (2015). [SNLI: Stanford Natural Language Inference](https://arxiv.org/abs/1508.05326)\n",
    "4. Lai et al. (2017). [RACE: Large-scale ReAding Comprehension Dataset](https://arxiv.org/abs/1704.04683)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
