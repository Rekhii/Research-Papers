{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Research Paper | Part V\n",
    "\n",
    "## Complete Implementation from Scratch\n",
    "\n",
    "---\n",
    "\n",
    "**Paper:** [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "\n",
    "**Authors:** Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever (OpenAI, 2018)\n",
    "\n",
    "---\n",
    "\n",
    "This is the **final, comprehensive implementation** of GPT-1. Every line is documented with:\n",
    "- Paper references and quotes\n",
    "- Mathematical derivations\n",
    "- Design rationale\n",
    "- Shape annotations\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. **Configuration** - All hyperparameters from Section 4.1\n",
    "2. **Embeddings** - Token + Positional (learned)\n",
    "3. **Layer Normalization** - Pre-LN variant\n",
    "4. **GELU Activation** - Gaussian Error Linear Unit\n",
    "5. **Causal Self-Attention** - Multi-head with masking\n",
    "6. **Feed-Forward Network** - Position-wise MLP\n",
    "7. **Transformer Block** - Attention + FFN with residuals\n",
    "8. **Complete GPT Model** - Full architecture\n",
    "9. **Training** - Language modeling objective\n",
    "10. **Generation** - Autoregressive sampling\n",
    "11. **Fine-tuning** - Task-specific adaptation\n",
    "12. **Verification** - Tests and parameter counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GPT-1 Complete Implementation\n",
    "=============================\n",
    "\n",
    "This module implements the GPT-1 architecture exactly as described in:\n",
    "\"Improving Language Understanding by Generative Pre-Training\"\n",
    "Radford et al., 2018\n",
    "\n",
    "All hyperparameters and architectural choices match the paper.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Configuration\n",
    "\n",
    "### Paper Reference (Section 4.1):\n",
    "\n",
    "> *\"We trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads). For the position-wise feed-forward networks, we used 3072 dimensional inner states.\"*\n",
    "\n",
    "> *\"We trained for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens.\"*\n",
    "\n",
    "> *\"We use... residual, embedding, and attention dropouts with a rate of 0.1 for regularization.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"\n",
    "    GPT-1 Configuration\n",
    "    ===================\n",
    "    \n",
    "    All values are extracted directly from the paper, Section 4.1.\n",
    "    \n",
    "    Architecture (from \"Model specifications\"):\n",
    "    - \"12-layer decoder-only transformer\"\n",
    "    - \"768 dimensional states\"\n",
    "    - \"12 attention heads\"\n",
    "    - \"3072 dimensional inner states\" (FFN)\n",
    "    \n",
    "    Training:\n",
    "    - \"contiguous sequences of 512 tokens\"\n",
    "    - \"40,000 merges\" (BPE vocabulary)\n",
    "    - \"dropouts with a rate of 0.1\"\n",
    "    \n",
    "    Attributes:\n",
    "        vocab_size (int): Vocabulary size. Paper uses BPE with 40,000 merges\n",
    "            plus special tokens, giving ~40,478 tokens.\n",
    "        n_positions (int): Maximum sequence length. Paper: 512.\n",
    "        n_embd (int): Embedding dimension. Paper: 768.\n",
    "        n_layer (int): Number of transformer blocks. Paper: 12.\n",
    "        n_head (int): Number of attention heads. Paper: 12.\n",
    "        n_inner (int): FFN hidden dimension. Paper: 3072 (4x n_embd).\n",
    "        embd_pdrop (float): Embedding dropout probability. Paper: 0.1.\n",
    "        attn_pdrop (float): Attention dropout probability. Paper: 0.1.\n",
    "        resid_pdrop (float): Residual dropout probability. Paper: 0.1.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ========== ARCHITECTURE (Section 4.1) ==========\n",
    "    vocab_size: int = 40478      # \"40,000 merges\" + special tokens\n",
    "    n_positions: int = 512       # \"contiguous sequences of 512 tokens\"\n",
    "    n_embd: int = 768            # \"768 dimensional states\"\n",
    "    n_layer: int = 12            # \"12-layer decoder-only transformer\"\n",
    "    n_head: int = 12             # \"12 attention heads\"\n",
    "    n_inner: int = 3072          # \"3072 dimensional inner states\"\n",
    "    \n",
    "    # ========== REGULARIZATION (Section 4.1) ==========\n",
    "    embd_pdrop: float = 0.1      # \"dropouts with a rate of 0.1\"\n",
    "    attn_pdrop: float = 0.1      # Applied to attention weights\n",
    "    resid_pdrop: float = 0.1     # Applied after projections\n",
    "    \n",
    "    # ========== DERIVED VALUES ==========\n",
    "    @property\n",
    "    def head_dim(self) -> int:\n",
    "        \"\"\"Dimension per attention head: 768 / 12 = 64\"\"\"\n",
    "        assert self.n_embd % self.n_head == 0, \"n_embd must be divisible by n_head\"\n",
    "        return self.n_embd // self.n_head\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate configuration.\"\"\"\n",
    "        assert self.n_embd % self.n_head == 0, \\\n",
    "            f\"n_embd ({self.n_embd}) must be divisible by n_head ({self.n_head})\"\n",
    "        assert self.n_inner == 4 * self.n_embd, \\\n",
    "            f\"n_inner should be 4 * n_embd (standard transformer ratio)\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"\n",
    "    Training Configuration\n",
    "    ======================\n",
    "    \n",
    "    Pre-training hyperparameters from Section 4.1:\n",
    "    - \"trained for 100 epochs\"\n",
    "    - \"minibatches of 64\"\n",
    "    - \"Adam optimization scheme\"\n",
    "    - \"max learning rate of 2.5e-4\"\n",
    "    - \"increased linearly from zero over the first 2000 updates\"\n",
    "    - \"annealed to 0 using a cosine schedule\"\n",
    "    - \"weight initialization of N(0, 0.02)\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # ========== PRE-TRAINING (Section 4.1) ==========\n",
    "    batch_size: int = 64         # \"minibatches of 64\"\n",
    "    epochs: int = 100            # \"trained for 100 epochs\"\n",
    "    max_lr: float = 2.5e-4       # \"max learning rate of 2.5e-4\"\n",
    "    warmup_steps: int = 2000     # \"over the first 2000 updates\"\n",
    "    \n",
    "    # ========== OPTIMIZER ==========\n",
    "    weight_decay: float = 0.01   # Standard for AdamW\n",
    "    betas: Tuple[float, float] = (0.9, 0.999)  # Adam defaults\n",
    "    eps: float = 1e-8            # Adam default\n",
    "    \n",
    "    # ========== INITIALIZATION ==========\n",
    "    init_std: float = 0.02       # \"weight initialization of N(0, 0.02)\"\n",
    "    \n",
    "    # ========== GRADIENT CLIPPING ==========\n",
    "    max_grad_norm: float = 1.0   # Standard practice\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FineTuningConfig:\n",
    "    \"\"\"\n",
    "    Fine-tuning Configuration\n",
    "    =========================\n",
    "    \n",
    "    From Section 4.1:\n",
    "    - \"learning rate of 6.25e-5\"\n",
    "    - \"batchsize of 32\"\n",
    "    - \"train for 3 epochs\"\n",
    "    - \"warmup over 0.2% of training\"\n",
    "    - \"weight lambda = 0.5\" (auxiliary LM loss)\n",
    "    \"\"\"\n",
    "    \n",
    "    learning_rate: float = 6.25e-5   # \"learning rate of 6.25e-5\"\n",
    "    batch_size: int = 32             # \"batchsize of 32\"\n",
    "    epochs: int = 3                  # \"train for 3 epochs\"\n",
    "    warmup_fraction: float = 0.002   # \"warmup over 0.2% of training\"\n",
    "    lm_weight: float = 0.5           # \"weight lambda = 0.5\"\n",
    "\n",
    "\n",
    "# Create default configs\n",
    "config = GPTConfig()\n",
    "train_config = TrainingConfig()\n",
    "ft_config = FineTuningConfig()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GPT-1 CONFIGURATION (All values from paper Section 4.1)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n[Model Architecture]\")\n",
    "print(f\"  Layers:           {config.n_layer} (\\\"12-layer decoder-only transformer\\\")\")\n",
    "print(f\"  Hidden dim:       {config.n_embd} (\\\"768 dimensional states\\\")\")\n",
    "print(f\"  Attention heads:  {config.n_head} (\\\"12 attention heads\\\")\")\n",
    "print(f\"  Head dimension:   {config.head_dim} (768 / 12 = 64)\")\n",
    "print(f\"  FFN inner dim:    {config.n_inner} (\\\"3072 dimensional inner states\\\")\")\n",
    "print(f\"  Max sequence:     {config.n_positions} (\\\"sequences of 512 tokens\\\")\")\n",
    "print(f\"  Vocabulary:       {config.vocab_size:,} (\\\"40,000 merges\\\" + special)\")\n",
    "print(f\"\\n[Regularization]\")\n",
    "print(f\"  Dropout:          {config.embd_pdrop} (\\\"dropouts with a rate of 0.1\\\")\")\n",
    "print(f\"\\n[Pre-training]\")\n",
    "print(f\"  Batch size:       {train_config.batch_size}\")\n",
    "print(f\"  Epochs:           {train_config.epochs}\")\n",
    "print(f\"  Max LR:           {train_config.max_lr}\")\n",
    "print(f\"  Warmup steps:     {train_config.warmup_steps}\")\n",
    "print(f\"\\n[Fine-tuning]\")\n",
    "print(f\"  Learning rate:    {ft_config.learning_rate}\")\n",
    "print(f\"  Epochs:           {ft_config.epochs}\")\n",
    "print(f\"  LM weight (λ):    {ft_config.lm_weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. GELU Activation Function\n",
    "\n",
    "### Paper Reference:\n",
    "\n",
    "> *\"We used... the Gaussian Error Linear Unit (GELU) activation function.\"*\n",
    "\n",
    "### Mathematical Definition:\n",
    "\n",
    "**Exact form:**\n",
    "$$\\text{GELU}(x) = x \\cdot \\Phi(x) = x \\cdot \\frac{1}{2}\\left[1 + \\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right]$$\n",
    "\n",
    "**Approximation (used in practice):**\n",
    "$$\\text{GELU}(x) \\approx 0.5x\\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}}(x + 0.044715x^3)\\right]\\right)$$\n",
    "\n",
    "### Why GELU over ReLU?\n",
    "\n",
    "- **ReLU**: Hard threshold at 0, gradient is 0 for x < 0 (\"dead neurons\")\n",
    "- **GELU**: Smooth, probabilistic gating, non-zero gradients everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Gaussian Error Linear Unit (GELU) Activation\n",
    "    =============================================\n",
    "    \n",
    "    Paper: \"We used... the Gaussian Error Linear Unit (GELU) activation function.\"\n",
    "    \n",
    "    GELU was introduced by Hendrycks & Gimpel (2016) and provides a smooth,\n",
    "    probabilistic alternative to ReLU.\n",
    "    \n",
    "    Mathematical formulation:\n",
    "        GELU(x) = x * Phi(x)\n",
    "        \n",
    "    where Phi(x) is the CDF of the standard normal distribution.\n",
    "    \n",
    "    This implementation uses the tanh approximation:\n",
    "        GELU(x) ≈ 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))\n",
    "    \n",
    "    The approximation is faster and numerically stable while being\n",
    "    very close to the exact value (max error < 0.001).\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor of any shape\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of same shape with GELU applied element-wise\n",
    "        \n",
    "    Example:\n",
    "        >>> x = torch.tensor([-1.0, 0.0, 1.0])\n",
    "        >>> gelu(x)\n",
    "        tensor([-0.1588,  0.0000,  0.8413])\n",
    "    \"\"\"\n",
    "    # Constants for the approximation\n",
    "    # sqrt(2/pi) ≈ 0.7978845608\n",
    "    # The coefficient 0.044715 was empirically determined\n",
    "    \n",
    "    return 0.5 * x * (\n",
    "        1.0 + torch.tanh(\n",
    "            math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3))\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# Verify GELU implementation\n",
    "print(\"GELU Activation Function\")\n",
    "print(\"=\" * 50)\n",
    "test_values = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "print(f\"Input:  {test_values.tolist()}\")\n",
    "print(f\"Output: {[f'{v:.4f}' for v in gelu(test_values).tolist()]}\")\n",
    "print(f\"\\nCompare with PyTorch's GELU:\")\n",
    "print(f\"PyTorch: {[f'{v:.4f}' for v in F.gelu(test_values).tolist()]}\")\n",
    "print(f\"Match: {torch.allclose(gelu(test_values), F.gelu(test_values), atol=1e-4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Layer Normalization\n",
    "\n",
    "### Why Layer Norm?\n",
    "\n",
    "Deep networks suffer from **internal covariate shift** - the distribution of layer inputs changes during training. Layer Normalization stabilizes training by normalizing across the feature dimension.\n",
    "\n",
    "### Mathematical Definition:\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "Where:\n",
    "- $\\mu = \\frac{1}{d}\\sum_{i=1}^{d} x_i$ (mean across features)\n",
    "- $\\sigma^2 = \\frac{1}{d}\\sum_{i=1}^{d} (x_i - \\mu)^2$ (variance across features)\n",
    "- $\\gamma, \\beta \\in \\mathbb{R}^d$ are learned scale and shift parameters\n",
    "- $\\epsilon$ is a small constant for numerical stability\n",
    "\n",
    "### Pre-LN vs Post-LN:\n",
    "\n",
    "GPT uses **Pre-LN** (LayerNorm before sublayer):\n",
    "```\n",
    "x = x + Sublayer(LayerNorm(x))  # Pre-LN (GPT)\n",
    "x = LayerNorm(x + Sublayer(x))  # Post-LN (original Transformer)\n",
    "```\n",
    "\n",
    "Pre-LN provides more stable gradients during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer Normalization\n",
    "    ===================\n",
    "    \n",
    "    Normalizes inputs across the feature dimension (last dimension).\n",
    "    Used extensively throughout GPT for training stability.\n",
    "    \n",
    "    From Ba et al. (2016): \"Layer Normalization\"\n",
    "    \n",
    "    Mathematical formulation:\n",
    "        y = gamma * (x - mean) / sqrt(var + eps) + beta\n",
    "        \n",
    "    where mean and var are computed across the last dimension.\n",
    "    \n",
    "    GPT uses Pre-LayerNorm (LN before attention/FFN) rather than\n",
    "    Post-LayerNorm (LN after residual). This provides:\n",
    "    - More stable gradients\n",
    "    - Better training without careful warmup\n",
    "    - Cleaner residual path\n",
    "    \n",
    "    Args:\n",
    "        n_embd (int): Feature dimension (768 for GPT-1)\n",
    "        eps (float): Small constant for numerical stability\n",
    "        \n",
    "    Shape:\n",
    "        Input: (batch, seq_len, n_embd)\n",
    "        Output: (batch, seq_len, n_embd)\n",
    "        \n",
    "    Parameters:\n",
    "        gamma: Learned scale, shape (n_embd,), initialized to 1\n",
    "        beta: Learned shift, shape (n_embd,), initialized to 0\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        \n",
    "        # Learnable parameters\n",
    "        # gamma (scale): initialized to 1, allows network to learn optimal scale\n",
    "        # beta (shift): initialized to 0, allows network to learn optimal bias\n",
    "        self.gamma = nn.Parameter(torch.ones(n_embd))\n",
    "        self.beta = nn.Parameter(torch.zeros(n_embd))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply layer normalization.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor, shape (batch, seq_len, n_embd)\n",
    "            \n",
    "        Returns:\n",
    "            Normalized tensor, shape (batch, seq_len, n_embd)\n",
    "        \"\"\"\n",
    "        # Compute mean across last dimension (features)\n",
    "        # Shape: (batch, seq_len, 1)\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Compute variance across last dimension\n",
    "        # unbiased=False uses N instead of N-1 in denominator (matches paper)\n",
    "        # Shape: (batch, seq_len, 1)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        \n",
    "        # Normalize: (x - mean) / sqrt(var + eps)\n",
    "        # Shape: (batch, seq_len, n_embd)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        \n",
    "        # Scale and shift with learned parameters\n",
    "        # gamma and beta broadcast across batch and seq_len\n",
    "        return self.gamma * x_norm + self.beta\n",
    "\n",
    "\n",
    "# Test LayerNorm\n",
    "print(\"Layer Normalization\")\n",
    "print(\"=\" * 50)\n",
    "ln = LayerNorm(config.n_embd)\n",
    "x = torch.randn(2, 5, config.n_embd) * 10 + 5  # Unnormalized\n",
    "out = ln(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"\\nBefore LayerNorm (sample position):\")\n",
    "print(f\"  Mean: {x[0, 0].mean().item():.4f}\")\n",
    "print(f\"  Std:  {x[0, 0].std().item():.4f}\")\n",
    "print(f\"\\nAfter LayerNorm (sample position):\")\n",
    "print(f\"  Mean: {out[0, 0].mean().item():.6f} (≈0)\")\n",
    "print(f\"  Std:  {out[0, 0].std().item():.6f} (≈1)\")\n",
    "print(f\"\\nParameters: {sum(p.numel() for p in ln.parameters()):,} (gamma + beta)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Causal Self-Attention\n",
    "\n",
    "### Paper Reference:\n",
    "\n",
    "> *\"This model applies a multi-headed self-attention operation over the input context tokens\"*\n",
    "\n",
    "> *\"We trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads).\"*\n",
    "\n",
    "### Mathematical Formulation:\n",
    "\n",
    "**Standard Attention:**\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "**Causal (Masked) Attention:**\n",
    "$$\\text{CausalAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M\\right)V$$\n",
    "\n",
    "Where mask $M$ is:\n",
    "$$M_{ij} = \\begin{cases} 0 & \\text{if } j \\leq i \\text{ (can attend)} \\\\ -\\infty & \\text{if } j > i \\text{ (cannot attend)} \\end{cases}$$\n",
    "\n",
    "**Multi-Head Attention:**\n",
    "- Split into 12 heads, each with dimension 64\n",
    "- Process in parallel, concatenate, project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Causal Self-Attention\n",
    "    =================================\n",
    "    \n",
    "    Paper: \"12 attention heads\" with \"768 dimensional states\"\n",
    "    This gives 64 dimensions per head (768 / 12 = 64).\n",
    "    \n",
    "    The \"masked\" in \"masked self-attention heads\" refers to causal masking:\n",
    "    each position can only attend to itself and previous positions.\n",
    "    This is required for autoregressive language modeling.\n",
    "    \n",
    "    Architecture:\n",
    "        1. Project input to Q, K, V (combined for efficiency)\n",
    "        2. Split into 12 heads\n",
    "        3. Compute attention scores: QK^T / sqrt(d_k)\n",
    "        4. Apply causal mask (future = -inf)\n",
    "        5. Softmax to get attention weights\n",
    "        6. Apply attention to values\n",
    "        7. Concatenate heads and project output\n",
    "    \n",
    "    Args:\n",
    "        config: GPTConfig with n_embd, n_head, dropout settings\n",
    "        \n",
    "    Shape:\n",
    "        Input: (batch, seq_len, n_embd)\n",
    "        Output: (batch, seq_len, n_embd)\n",
    "        \n",
    "    Parameters:\n",
    "        c_attn: Combined Q/K/V projection, (n_embd, 3 * n_embd)\n",
    "        c_proj: Output projection, (n_embd, n_embd)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store configuration\n",
    "        self.n_head = config.n_head        # 12 heads\n",
    "        self.n_embd = config.n_embd        # 768 dimensions\n",
    "        self.head_dim = config.head_dim    # 64 dimensions per head\n",
    "        \n",
    "        # Scaling factor: 1/sqrt(d_k) for stable gradients\n",
    "        # Without scaling, dot products grow with dimension, causing\n",
    "        # softmax to have extremely small gradients\n",
    "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # === PROJECTIONS ===\n",
    "        # Combined Q, K, V projection for efficiency\n",
    "        # Instead of 3 separate (768, 768) matrices, use one (768, 2304)\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        \n",
    "        # Output projection: combines all heads back to n_embd\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        \n",
    "        # === DROPOUT ===\n",
    "        # Paper: \"dropouts with a rate of 0.1\"\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)   # On attention weights\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop) # On output\n",
    "        \n",
    "        # === CAUSAL MASK ===\n",
    "        # Lower triangular matrix: position i can attend to positions 0...i\n",
    "        # Register as buffer (not a parameter, but saved with model)\n",
    "        mask = torch.tril(torch.ones(config.n_positions, config.n_positions))\n",
    "        self.register_buffer(\n",
    "            'mask', \n",
    "            mask.view(1, 1, config.n_positions, config.n_positions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for causal self-attention.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor, shape (batch, seq_len, n_embd)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor, shape (batch, seq_len, n_embd)\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape  # batch, seq_len, n_embd\n",
    "        \n",
    "        # === STEP 1: Project to Q, K, V ===\n",
    "        # x @ W_qkv + b_qkv\n",
    "        # Shape: (B, T, C) @ (C, 3C) -> (B, T, 3C)\n",
    "        qkv = self.c_attn(x)\n",
    "        \n",
    "        # Split into Q, K, V\n",
    "        # Each has shape (B, T, C)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        \n",
    "        # === STEP 2: Reshape for multi-head attention ===\n",
    "        # (B, T, C) -> (B, T, n_head, head_dim) -> (B, n_head, T, head_dim)\n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        # Now: q, k, v all have shape (B, n_head, T, head_dim)\n",
    "        \n",
    "        # === STEP 3: Compute attention scores ===\n",
    "        # QK^T / sqrt(d_k)\n",
    "        # (B, n_head, T, head_dim) @ (B, n_head, head_dim, T) -> (B, n_head, T, T)\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        # === STEP 4: Apply causal mask ===\n",
    "        # Where mask is 0, set attention score to -inf\n",
    "        # After softmax, -inf becomes 0 (no attention to future)\n",
    "        attn_scores = attn_scores.masked_fill(\n",
    "            self.mask[:, :, :T, :T] == 0, \n",
    "            float('-inf')\n",
    "        )\n",
    "        \n",
    "        # === STEP 5: Softmax to get attention weights ===\n",
    "        # Shape: (B, n_head, T, T)\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Apply dropout to attention weights\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "        \n",
    "        # === STEP 6: Apply attention to values ===\n",
    "        # (B, n_head, T, T) @ (B, n_head, T, head_dim) -> (B, n_head, T, head_dim)\n",
    "        out = attn_weights @ v\n",
    "        \n",
    "        # === STEP 7: Concatenate heads and project ===\n",
    "        # (B, n_head, T, head_dim) -> (B, T, n_head, head_dim) -> (B, T, C)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Output projection with dropout\n",
    "        out = self.resid_dropout(self.c_proj(out))\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# Test Causal Self-Attention\n",
    "print(\"Causal Self-Attention\")\n",
    "print(\"=\" * 50)\n",
    "attn = CausalSelfAttention(config)\n",
    "x = torch.randn(2, 10, config.n_embd)\n",
    "out = attn(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"\\nParameter breakdown:\")\n",
    "print(f\"  c_attn (Q/K/V combined): {config.n_embd} x {3*config.n_embd} + {3*config.n_embd} = {config.n_embd * 3 * config.n_embd + 3 * config.n_embd:,}\")\n",
    "print(f\"  c_proj (output):         {config.n_embd} x {config.n_embd} + {config.n_embd} = {config.n_embd * config.n_embd + config.n_embd:,}\")\n",
    "print(f\"  Total: {sum(p.numel() for p in attn.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Feed-Forward Network (MLP)\n",
    "\n",
    "### Paper Reference:\n",
    "\n",
    "> *\"For the position-wise feed-forward networks, we used 3072 dimensional inner states.\"*\n",
    "\n",
    "### Mathematical Formulation:\n",
    "\n",
    "$$\\text{FFN}(x) = \\text{GELU}(xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "Where:\n",
    "- $W_1 \\in \\mathbb{R}^{768 \\times 3072}$ (expand)\n",
    "- $W_2 \\in \\mathbb{R}^{3072 \\times 768}$ (contract)\n",
    "\n",
    "### Why 4x Expansion?\n",
    "\n",
    "The 4x expansion ratio (768 → 3072 → 768) is standard in transformers:\n",
    "- More \"working space\" for computation\n",
    "- Attention routes information; FFN processes it\n",
    "- Research shows FFN neurons encode specific features/concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network\n",
    "    ===================================\n",
    "    \n",
    "    Paper: \"3072 dimensional inner states\"\n",
    "    \n",
    "    This is a simple two-layer MLP applied independently to each position:\n",
    "        1. Linear: 768 -> 3072 (expand)\n",
    "        2. GELU activation\n",
    "        3. Linear: 3072 -> 768 (contract)\n",
    "        4. Dropout\n",
    "    \n",
    "    The 4x expansion ratio (768 * 4 = 3072) is standard in transformers.\n",
    "    This provides more \"working space\" for the network to process information.\n",
    "    \n",
    "    Research has shown that individual neurons in the FFN often encode\n",
    "    specific concepts or features (\"knowledge neurons\").\n",
    "    \n",
    "    Args:\n",
    "        config: GPTConfig with n_embd, n_inner, dropout settings\n",
    "        \n",
    "    Shape:\n",
    "        Input: (batch, seq_len, n_embd)\n",
    "        Output: (batch, seq_len, n_embd)\n",
    "        \n",
    "    Parameters:\n",
    "        c_fc: First linear layer, (n_embd, n_inner) = (768, 3072)\n",
    "        c_proj: Second linear layer, (n_inner, n_embd) = (3072, 768)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First linear layer: expand 768 -> 3072\n",
    "        # \"fc\" = \"fully connected\"\n",
    "        self.c_fc = nn.Linear(config.n_embd, config.n_inner)\n",
    "        \n",
    "        # Second linear layer: contract 3072 -> 768\n",
    "        # \"proj\" = \"projection\"\n",
    "        self.c_proj = nn.Linear(config.n_inner, config.n_embd)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for FFN.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor, shape (batch, seq_len, n_embd)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor, shape (batch, seq_len, n_embd)\n",
    "        \"\"\"\n",
    "        # Expand: (B, T, 768) -> (B, T, 3072)\n",
    "        x = self.c_fc(x)\n",
    "        \n",
    "        # GELU activation\n",
    "        x = gelu(x)\n",
    "        \n",
    "        # Contract: (B, T, 3072) -> (B, T, 768)\n",
    "        x = self.c_proj(x)\n",
    "        \n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Test MLP\n",
    "print(\"Feed-Forward Network (MLP)\")\n",
    "print(\"=\" * 50)\n",
    "mlp = MLP(config)\n",
    "x = torch.randn(2, 10, config.n_embd)\n",
    "out = mlp(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"\\nParameter breakdown:\")\n",
    "print(f\"  c_fc:   {config.n_embd} x {config.n_inner} + {config.n_inner} = {config.n_embd * config.n_inner + config.n_inner:,}\")\n",
    "print(f\"  c_proj: {config.n_inner} x {config.n_embd} + {config.n_embd} = {config.n_inner * config.n_embd + config.n_embd:,}\")\n",
    "print(f\"  Total: {sum(p.numel() for p in mlp.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Transformer Block\n",
    "\n",
    "### Architecture:\n",
    "\n",
    "Each block consists of:\n",
    "1. **Layer Norm** → **Multi-Head Attention** → **Residual Add**\n",
    "2. **Layer Norm** → **Feed-Forward** → **Residual Add**\n",
    "\n",
    "```\n",
    "x = x + Attention(LayerNorm(x))  # Pre-LN attention\n",
    "x = x + FFN(LayerNorm(x))        # Pre-LN FFN\n",
    "```\n",
    "\n",
    "### Residual Connections:\n",
    "\n",
    "Residual connections (He et al., 2016) allow gradients to flow directly:\n",
    "- Enables training much deeper networks\n",
    "- Each layer learns a \"residual\" or \"delta\" to add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Decoder Block\n",
    "    =========================\n",
    "    \n",
    "    A single transformer block with:\n",
    "        1. Layer Norm + Multi-Head Causal Self-Attention + Residual\n",
    "        2. Layer Norm + Feed-Forward Network + Residual\n",
    "    \n",
    "    GPT uses Pre-LayerNorm (LN before sublayers) for better stability:\n",
    "        x = x + Attention(LayerNorm(x))\n",
    "        x = x + FFN(LayerNorm(x))\n",
    "    \n",
    "    vs the original Transformer's Post-LayerNorm:\n",
    "        x = LayerNorm(x + Attention(x))\n",
    "        x = LayerNorm(x + FFN(x))\n",
    "    \n",
    "    Pre-LN provides:\n",
    "    - Cleaner gradient flow through residual path\n",
    "    - More stable training\n",
    "    - Less sensitivity to hyperparameters\n",
    "    \n",
    "    Args:\n",
    "        config: GPTConfig with all hyperparameters\n",
    "        \n",
    "    Shape:\n",
    "        Input: (batch, seq_len, n_embd)\n",
    "        Output: (batch, seq_len, n_embd)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Layer Norm before attention\n",
    "        self.ln_1 = LayerNorm(config.n_embd)\n",
    "        \n",
    "        # Multi-head causal self-attention\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        \n",
    "        # Layer Norm before FFN\n",
    "        self.ln_2 = LayerNorm(config.n_embd)\n",
    "        \n",
    "        # Position-wise feed-forward network\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer block.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor, shape (batch, seq_len, n_embd)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor, shape (batch, seq_len, n_embd)\n",
    "        \"\"\"\n",
    "        # === Attention sub-block (Pre-LN) ===\n",
    "        # 1. Apply LayerNorm\n",
    "        # 2. Apply attention\n",
    "        # 3. Add residual connection\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        \n",
    "        # === FFN sub-block (Pre-LN) ===\n",
    "        # 1. Apply LayerNorm\n",
    "        # 2. Apply FFN\n",
    "        # 3. Add residual connection\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Test Block\n",
    "print(\"Transformer Block\")\n",
    "print(\"=\" * 50)\n",
    "block = Block(config)\n",
    "x = torch.randn(2, 10, config.n_embd)\n",
    "out = block(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"\\nParameter breakdown:\")\n",
    "print(f\"  ln_1:      {sum(p.numel() for p in block.ln_1.parameters()):,}\")\n",
    "print(f\"  attention: {sum(p.numel() for p in block.attn.parameters()):,}\")\n",
    "print(f\"  ln_2:      {sum(p.numel() for p in block.ln_2.parameters()):,}\")\n",
    "print(f\"  mlp:       {sum(p.numel() for p in block.mlp.parameters()):,}\")\n",
    "print(f\"  Total:     {sum(p.numel() for p in block.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Complete GPT Model\n",
    "\n",
    "### Paper Reference (Section 3.1):\n",
    "\n",
    "> *\"We use a multi-layer Transformer decoder for the language model, which is a variant of the transformer. This model applies a multi-headed self-attention operation over the input context tokens followed by position-wise feedforward layers to produce an output distribution over target tokens.\"*\n",
    "\n",
    "### Complete Architecture:\n",
    "\n",
    "```\n",
    "Input tokens\n",
    "    ↓\n",
    "Token Embedding (40,478 x 768) + Position Embedding (512 x 768)\n",
    "    ↓\n",
    "Dropout\n",
    "    ↓\n",
    "Transformer Block × 12\n",
    "    ↓\n",
    "Final Layer Norm\n",
    "    ↓\n",
    "Language Model Head (tied with token embedding)\n",
    "    ↓\n",
    "Output logits (vocabulary probabilities)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT-1 Language Model\n",
    "    ====================\n",
    "    \n",
    "    Complete implementation of \"Improving Language Understanding by Generative Pre-Training\"\n",
    "    (Radford et al., 2018)\n",
    "    \n",
    "    Architecture (from Section 4.1):\n",
    "    - \"12-layer decoder-only transformer\"\n",
    "    - \"768 dimensional states\"\n",
    "    - \"12 attention heads\"\n",
    "    - \"3072 dimensional inner states\" (FFN)\n",
    "    - \"contiguous sequences of 512 tokens\"\n",
    "    \n",
    "    Components:\n",
    "        1. Token Embedding: vocab_size -> n_embd\n",
    "        2. Position Embedding: n_positions -> n_embd (LEARNED, not sinusoidal)\n",
    "        3. 12 Transformer Blocks (attention + FFN with residuals)\n",
    "        4. Final Layer Norm\n",
    "        5. Language Model Head (weight-tied with token embedding)\n",
    "    \n",
    "    Training Objective (Equation 1):\n",
    "        L1(U) = sum_i log P(u_i | u_{i-k}, ..., u_{i-1})\n",
    "    \n",
    "    This is standard causal language modeling: predict the next token\n",
    "    given all previous tokens.\n",
    "    \n",
    "    Args:\n",
    "        config: GPTConfig with all hyperparameters\n",
    "        \n",
    "    Inputs:\n",
    "        input_ids: Token IDs, shape (batch, seq_len)\n",
    "        targets: Target token IDs for loss, shape (batch, seq_len), optional\n",
    "        \n",
    "    Outputs:\n",
    "        logits: Vocabulary logits, shape (batch, seq_len, vocab_size)\n",
    "        loss: Cross-entropy loss if targets provided, else None\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # ========== EMBEDDINGS ==========\n",
    "        # Token embeddings: map token IDs to vectors\n",
    "        # Shape: (vocab_size, n_embd) = (40478, 768)\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        \n",
    "        # Position embeddings: LEARNED (not sinusoidal like original Transformer)\n",
    "        # Shape: (n_positions, n_embd) = (512, 768)\n",
    "        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n",
    "        \n",
    "        # Embedding dropout\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        \n",
    "        # ========== TRANSFORMER BLOCKS ==========\n",
    "        # 12 identical decoder blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(config) for _ in range(config.n_layer)\n",
    "        ])\n",
    "        \n",
    "        # ========== OUTPUT ==========\n",
    "        # Final layer normalization\n",
    "        self.ln_f = LayerNorm(config.n_embd)\n",
    "        \n",
    "        # Language model head: project to vocabulary\n",
    "        # Note: bias=False because we'll tie weights with embedding\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # ========== WEIGHT TYING ==========\n",
    "        # Share weights between token embedding and LM head\n",
    "        # This is standard practice that:\n",
    "        # - Reduces parameters\n",
    "        # - Improves performance\n",
    "        # - Makes semantic sense (similar tokens have similar embeddings)\n",
    "        self.lm_head.weight = self.wte.weight\n",
    "        \n",
    "        # ========== INITIALIZATION ==========\n",
    "        # Paper: \"weight initialization of N(0, 0.02)\"\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # Report parameter count\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"GPT-1 Model initialized with {n_params:,} parameters\")\n",
    "    \n",
    "    def _init_weights(self, module: nn.Module):\n",
    "        \"\"\"\n",
    "        Initialize weights.\n",
    "        \n",
    "        Paper: \"weight initialization of N(0, 0.02) was sufficient\"\n",
    "        \n",
    "        This simple initialization works because LayerNorm helps\n",
    "        stabilize the forward pass.\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: torch.Tensor,\n",
    "        targets: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Forward pass for language modeling.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs, shape (batch, seq_len)\n",
    "            targets: Target token IDs for loss computation, shape (batch, seq_len)\n",
    "            \n",
    "        Returns:\n",
    "            logits: Vocabulary logits, shape (batch, seq_len, vocab_size)\n",
    "            loss: Cross-entropy loss if targets provided, else None\n",
    "        \"\"\"\n",
    "        B, T = input_ids.shape\n",
    "        assert T <= self.config.n_positions, \\\n",
    "            f\"Sequence length {T} exceeds maximum {self.config.n_positions}\"\n",
    "        \n",
    "        # ========== EMBEDDINGS ==========\n",
    "        # Token embeddings: (B, T) -> (B, T, n_embd)\n",
    "        tok_emb = self.wte(input_ids)\n",
    "        \n",
    "        # Position embeddings: (T,) -> (T, n_embd) -> broadcast to (B, T, n_embd)\n",
    "        positions = torch.arange(T, device=input_ids.device)\n",
    "        pos_emb = self.wpe(positions)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        x = self.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        # ========== TRANSFORMER BLOCKS ==========\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # ========== OUTPUT ==========\n",
    "        # Final layer norm\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        # (B, T, n_embd) -> (B, T, vocab_size)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        # ========== LOSS ==========\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # Reshape for cross-entropy\n",
    "            # logits: (B * T, vocab_size)\n",
    "            # targets: (B * T,)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1)\n",
    "            )\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def get_num_params(self, non_embedding: bool = True) -> int:\n",
    "        \"\"\"\n",
    "        Count parameters.\n",
    "        \n",
    "        Args:\n",
    "            non_embedding: If True, exclude embedding parameters\n",
    "            \n",
    "        Returns:\n",
    "            Number of parameters\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "\n",
    "# Create model\n",
    "print(\"=\" * 70)\n",
    "print(\"CREATING GPT-1 MODEL\")\n",
    "print(\"=\" * 70)\n",
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_parameter_count(model: GPT, config: GPTConfig):\n",
    "    \"\"\"\n",
    "    Detailed breakdown of model parameters.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"DETAILED PARAMETER COUNT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Embeddings\n",
    "    tok_emb = config.vocab_size * config.n_embd\n",
    "    pos_emb = config.n_positions * config.n_embd\n",
    "    print(f\"\\n[EMBEDDINGS]\")\n",
    "    print(f\"  Token embedding:    {config.vocab_size:,} x {config.n_embd} = {tok_emb:,}\")\n",
    "    print(f\"  Position embedding: {config.n_positions} x {config.n_embd} = {pos_emb:,}\")\n",
    "    print(f\"  Subtotal: {tok_emb + pos_emb:,}\")\n",
    "    \n",
    "    # Per block\n",
    "    print(f\"\\n[PER TRANSFORMER BLOCK]\")\n",
    "    \n",
    "    # Attention\n",
    "    c_attn = config.n_embd * (3 * config.n_embd) + (3 * config.n_embd)  # W + b\n",
    "    c_proj = config.n_embd * config.n_embd + config.n_embd\n",
    "    attn_total = c_attn + c_proj\n",
    "    print(f\"  Attention:\")\n",
    "    print(f\"    c_attn (Q/K/V): {config.n_embd} x {3*config.n_embd} + {3*config.n_embd} = {c_attn:,}\")\n",
    "    print(f\"    c_proj:         {config.n_embd} x {config.n_embd} + {config.n_embd} = {c_proj:,}\")\n",
    "    \n",
    "    # FFN\n",
    "    c_fc = config.n_embd * config.n_inner + config.n_inner\n",
    "    c_proj_ffn = config.n_inner * config.n_embd + config.n_embd\n",
    "    ffn_total = c_fc + c_proj_ffn\n",
    "    print(f\"  FFN:\")\n",
    "    print(f\"    c_fc:           {config.n_embd} x {config.n_inner} + {config.n_inner} = {c_fc:,}\")\n",
    "    print(f\"    c_proj:         {config.n_inner} x {config.n_embd} + {config.n_embd} = {c_proj_ffn:,}\")\n",
    "    \n",
    "    # LayerNorms\n",
    "    ln_params = 2 * config.n_embd  # gamma + beta\n",
    "    ln_total = 2 * ln_params  # Two LayerNorms per block\n",
    "    print(f\"  LayerNorm:        2 x (2 x {config.n_embd}) = {ln_total:,}\")\n",
    "    \n",
    "    block_total = attn_total + ffn_total + ln_total\n",
    "    print(f\"  Block total:      {block_total:,}\")\n",
    "    \n",
    "    # All blocks\n",
    "    all_blocks = block_total * config.n_layer\n",
    "    print(f\"\\n[ALL BLOCKS]\")\n",
    "    print(f\"  {config.n_layer} blocks x {block_total:,} = {all_blocks:,}\")\n",
    "    \n",
    "    # Output\n",
    "    ln_f = 2 * config.n_embd\n",
    "    print(f\"\\n[OUTPUT]\")\n",
    "    print(f\"  Final LayerNorm:  {ln_f:,}\")\n",
    "    print(f\"  LM Head:          (tied with token embedding, 0 additional)\")\n",
    "    \n",
    "    # Total\n",
    "    total = tok_emb + pos_emb + all_blocks + ln_f\n",
    "    print(f\"\\n[TOTAL]\")\n",
    "    print(f\"  Embeddings:       {tok_emb + pos_emb:,}\")\n",
    "    print(f\"  Blocks:           {all_blocks:,}\")\n",
    "    print(f\"  Output:           {ln_f:,}\")\n",
    "    print(f\"  \" + \"-\" * 40)\n",
    "    print(f\"  TOTAL:            {total:,}\")\n",
    "    \n",
    "    # Actual count\n",
    "    actual = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\n  Actual (PyTorch): {actual:,}\")\n",
    "    print(f\"  Paper reports:    ~117M parameters\")\n",
    "\n",
    "detailed_parameter_count(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Training\n",
    "\n",
    "### Paper Reference (Section 3.1):\n",
    "\n",
    "> *\"Given an unsupervised corpus of tokens U = {u1, ..., un}, we use a standard language modeling objective to maximize the following likelihood:\"*\n",
    "\n",
    "$$L_1(\\mathcal{U}) = \\sum_i \\log P(u_i | u_{i-k}, ..., u_{i-1}; \\Theta)$$\n",
    "\n",
    "### Learning Rate Schedule:\n",
    "\n",
    "> *\"The learning rate was increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    GPT Trainer\n",
    "    ===========\n",
    "    \n",
    "    Implements the training procedure from Section 4.1:\n",
    "    - Adam optimizer with max LR 2.5e-4\n",
    "    - Linear warmup for 2000 steps\n",
    "    - Cosine annealing to 0\n",
    "    - Gradient clipping\n",
    "    \n",
    "    The language modeling objective (Equation 1):\n",
    "        L1(U) = sum_i log P(u_i | u_{i-k}, ..., u_{i-1})\n",
    "    \n",
    "    is implemented as cross-entropy loss between model predictions\n",
    "    and the next token at each position.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model: GPT, \n",
    "        train_config: TrainingConfig,\n",
    "        device: torch.device = torch.device('cpu')\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.config = train_config\n",
    "        self.device = device\n",
    "        \n",
    "        # Move model to device\n",
    "        self.model.to(device)\n",
    "        \n",
    "        # Optimizer: \"Adam optimization scheme\"\n",
    "        self.optimizer = AdamW(\n",
    "            model.parameters(),\n",
    "            lr=train_config.max_lr,\n",
    "            betas=train_config.betas,\n",
    "            eps=train_config.eps,\n",
    "            weight_decay=train_config.weight_decay\n",
    "        )\n",
    "        \n",
    "        # Training state\n",
    "        self.step = 0\n",
    "        self.losses = []\n",
    "    \n",
    "    def get_lr(self, step: int, total_steps: int) -> float:\n",
    "        \"\"\"\n",
    "        Get learning rate for current step.\n",
    "        \n",
    "        Paper: \"The learning rate was increased linearly from zero over\n",
    "        the first 2000 updates and annealed to 0 using a cosine schedule.\"\n",
    "        \n",
    "        Args:\n",
    "            step: Current training step\n",
    "            total_steps: Total number of training steps\n",
    "            \n",
    "        Returns:\n",
    "            Learning rate for this step\n",
    "        \"\"\"\n",
    "        warmup = self.config.warmup_steps\n",
    "        max_lr = self.config.max_lr\n",
    "        \n",
    "        if step < warmup:\n",
    "            # Linear warmup: 0 -> max_lr\n",
    "            return max_lr * step / warmup\n",
    "        else:\n",
    "            # Cosine annealing: max_lr -> 0\n",
    "            progress = (step - warmup) / (total_steps - warmup)\n",
    "            return max_lr * 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    \n",
    "    def train_step(\n",
    "        self, \n",
    "        input_ids: torch.Tensor, \n",
    "        targets: torch.Tensor,\n",
    "        total_steps: int\n",
    "    ) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Single training step.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Input token IDs, shape (batch, seq_len)\n",
    "            targets: Target token IDs, shape (batch, seq_len)\n",
    "            total_steps: Total steps for LR schedule\n",
    "            \n",
    "        Returns:\n",
    "            loss: Training loss\n",
    "            lr: Current learning rate\n",
    "        \"\"\"\n",
    "        # Update learning rate\n",
    "        lr = self.get_lr(self.step, total_steps)\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        # Move data to device\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        targets = targets.to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        self.model.train()\n",
    "        logits, loss = self.model(input_ids, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (common practice for stability)\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            self.model.parameters(), \n",
    "            self.config.max_grad_norm\n",
    "        )\n",
    "        \n",
    "        # Update weights\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update state\n",
    "        self.step += 1\n",
    "        self.losses.append(loss.item())\n",
    "        \n",
    "        return loss.item(), lr\n",
    "\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(model, train_config, device)\n",
    "print(f\"\\nTrainer initialized on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_training(model: GPT, trainer: Trainer, num_steps: int = 100):\n",
    "    \"\"\"\n",
    "    Demonstrate training on synthetic data.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TRAINING DEMO\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Running {num_steps} steps on synthetic data...\")\n",
    "    \n",
    "    # Reset trainer\n",
    "    trainer.step = 0\n",
    "    trainer.losses = []\n",
    "    \n",
    "    batch_size = 8\n",
    "    seq_len = 64\n",
    "    \n",
    "    losses = []\n",
    "    lrs = []\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Generate synthetic batch\n",
    "        input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n",
    "        targets = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n",
    "        \n",
    "        loss, lr = trainer.train_step(input_ids, targets, num_steps)\n",
    "        losses.append(loss)\n",
    "        lrs.append(lr)\n",
    "        \n",
    "        if (step + 1) % 25 == 0:\n",
    "            print(f\"  Step {step+1:4d} | Loss: {loss:.4f} | LR: {lr:.2e}\")\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    \n",
    "    axes[0].plot(losses, 'b-', linewidth=1.5, alpha=0.7)\n",
    "    axes[0].set_xlabel('Step')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training Loss')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].plot(lrs, 'r-', linewidth=2)\n",
    "    axes[1].set_xlabel('Step')\n",
    "    axes[1].set_ylabel('Learning Rate')\n",
    "    axes[1].set_title('Learning Rate Schedule')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nFinal loss: {losses[-1]:.4f}\")\n",
    "    print(f\"Expected random: {math.log(config.vocab_size):.4f}\")\n",
    "\n",
    "demo_training(model, trainer, num_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Text Generation\n",
    "\n",
    "### Autoregressive Generation:\n",
    "\n",
    "Once trained, GPT generates text by:\n",
    "1. Start with a prompt\n",
    "2. Predict the next token distribution\n",
    "3. Sample from the distribution\n",
    "4. Append sampled token to sequence\n",
    "5. Repeat until done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model: GPT,\n",
    "    input_ids: torch.Tensor,\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: Optional[int] = None,\n",
    "    top_p: Optional[float] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Autoregressive Text Generation\n",
    "    ==============================\n",
    "    \n",
    "    Generate text by repeatedly predicting and sampling the next token.\n",
    "    \n",
    "    Sampling strategies:\n",
    "    - Temperature: Scale logits before softmax\n",
    "        - T < 1: More confident (peaked distribution)\n",
    "        - T > 1: More random (uniform distribution)\n",
    "    - Top-k: Only consider the k highest probability tokens\n",
    "    - Top-p (nucleus): Only consider tokens whose cumulative probability <= p\n",
    "    \n",
    "    Args:\n",
    "        model: Trained GPT model\n",
    "        input_ids: Starting token IDs, shape (batch, seq_len)\n",
    "        max_new_tokens: Number of tokens to generate\n",
    "        temperature: Sampling temperature (1.0 = no change)\n",
    "        top_k: If set, only sample from top k tokens\n",
    "        top_p: If set, use nucleus sampling\n",
    "        \n",
    "    Returns:\n",
    "        Generated token IDs including input, shape (batch, seq_len + max_new_tokens)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    generated = input_ids.clone()\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop context to max sequence length if needed\n",
    "        idx_cond = generated\n",
    "        if generated.size(1) > model.config.n_positions:\n",
    "            idx_cond = generated[:, -model.config.n_positions:]\n",
    "        \n",
    "        # Get predictions\n",
    "        logits, _ = model(idx_cond)\n",
    "        \n",
    "        # Get logits for the last position only\n",
    "        logits = logits[:, -1, :]  # (batch, vocab_size)\n",
    "        \n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # Apply top-k filtering\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = float('-inf')\n",
    "        \n",
    "        # Apply top-p (nucleus) filtering\n",
    "        if top_p is not None:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            \n",
    "            # Remove tokens with cumulative probability above threshold\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            \n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(\n",
    "                1, sorted_indices, sorted_indices_to_remove\n",
    "            )\n",
    "            logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Append to generated sequence\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "\n",
    "# Demo generation\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GENERATION DEMO\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Note: Model is untrained, so outputs are random.\")\n",
    "\n",
    "prompt = torch.randint(0, 100, (1, 5)).to(device)\n",
    "\n",
    "settings = [\n",
    "    (\"Greedy (T=0.1)\", {\"temperature\": 0.1}),\n",
    "    (\"Normal (T=1.0)\", {\"temperature\": 1.0}),\n",
    "    (\"Creative (T=1.5)\", {\"temperature\": 1.5}),\n",
    "    (\"Top-k (k=10)\", {\"top_k\": 10}),\n",
    "    (\"Top-p (p=0.9)\", {\"top_p\": 0.9}),\n",
    "]\n",
    "\n",
    "for name, kwargs in settings:\n",
    "    output = generate(model, prompt, max_new_tokens=10, **kwargs)\n",
    "    generated_tokens = output[0, 5:].tolist()\n",
    "    print(f\"\\n{name}: {generated_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Fine-tuning for Classification\n",
    "\n",
    "### Paper Reference (Section 3.2):\n",
    "\n",
    "> *\"The inputs are passed through our pre-trained model to obtain the final transformer block's activation $h_l^m$, which is then fed into an added linear output layer with parameters $W_y$ to predict $y$.\"*\n",
    "\n",
    "$$P(y | x^1, ..., x^m) = \\text{softmax}(h_l^m W_y)$$\n",
    "\n",
    "### Combined Objective (Equation 3):\n",
    "\n",
    "> *\"We additionally found that including language modeling as an auxiliary objective to the fine-tuning helped learning.\"*\n",
    "\n",
    "$$L_3(C) = L_2(C) + \\lambda \\cdot L_1(C)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTForSequenceClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT with Classification Head\n",
    "    ============================\n",
    "    \n",
    "    For fine-tuning on classification tasks (sentiment, NLI, etc.).\n",
    "    \n",
    "    Paper (Section 3.2):\n",
    "    \"The inputs are passed through our pre-trained model to obtain the\n",
    "    final transformer block's activation h_l^m, which is then fed into\n",
    "    an added linear output layer with parameters W_y to predict y.\"\n",
    "    \n",
    "    Combined objective (Equation 3):\n",
    "        L3(C) = L2(C) + lambda * L1(C)\n",
    "        \n",
    "    where:\n",
    "        L2 = task-specific classification loss\n",
    "        L1 = auxiliary language modeling loss\n",
    "        lambda = 0.5 (from paper)\n",
    "    \n",
    "    Args:\n",
    "        config: GPTConfig\n",
    "        num_labels: Number of classification labels\n",
    "        lm_weight: Weight for auxiliary LM loss (lambda = 0.5)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        config: GPTConfig, \n",
    "        num_labels: int,\n",
    "        lm_weight: float = 0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pre-trained GPT backbone\n",
    "        self.gpt = GPT(config)\n",
    "        \n",
    "        # Task configuration\n",
    "        self.num_labels = num_labels\n",
    "        self.lm_weight = lm_weight  # lambda from Equation 3\n",
    "        \n",
    "        # Classification head: single linear layer\n",
    "        # \"added linear output layer with parameters W_y\"\n",
    "        self.classifier = nn.Linear(config.n_embd, num_labels)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Initialize classifier\n",
    "        nn.init.normal_(self.classifier.weight, std=0.02)\n",
    "        nn.init.zeros_(self.classifier.bias)\n",
    "        \n",
    "        # Count parameters\n",
    "        total = sum(p.numel() for p in self.parameters())\n",
    "        classifier_params = sum(p.numel() for p in self.classifier.parameters())\n",
    "        print(f\"\\nClassification Model:\")\n",
    "        print(f\"  Total parameters:      {total:,}\")\n",
    "        print(f\"  New classifier params: {classifier_params:,}\")\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        lm_targets: Optional[torch.Tensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Forward pass with combined loss.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs, shape (batch, seq_len)\n",
    "            labels: Classification labels, shape (batch,)\n",
    "            lm_targets: Targets for auxiliary LM loss, shape (batch, seq_len)\n",
    "            \n",
    "        Returns:\n",
    "            logits: Classification logits, shape (batch, num_labels)\n",
    "            loss: Combined loss L3 = L2 + lambda * L1\n",
    "        \"\"\"\n",
    "        batch_size = input_ids.size(0)\n",
    "        \n",
    "        # Get GPT outputs\n",
    "        # We need hidden states, so we modify to return them\n",
    "        logits_lm, loss_lm = self.gpt(input_ids, lm_targets)\n",
    "        \n",
    "        # Get hidden states from last position\n",
    "        # We need to recompute without the LM head\n",
    "        B, T = input_ids.shape\n",
    "        tok_emb = self.gpt.wte(input_ids)\n",
    "        pos_emb = self.gpt.wpe(torch.arange(T, device=input_ids.device))\n",
    "        x = self.gpt.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        for block in self.gpt.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.gpt.ln_f(x)\n",
    "        \n",
    "        # Extract representation at last position: h_l^m\n",
    "        # \"final transformer block's activation h_l^m\"\n",
    "        pooled = x[:, -1, :]  # (batch, n_embd)\n",
    "        pooled = self.dropout(pooled)\n",
    "        \n",
    "        # Classification: softmax(h_l^m * W_y)\n",
    "        classification_logits = self.classifier(pooled)  # (batch, num_labels)\n",
    "        \n",
    "        # Compute combined loss (Equation 3)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # L2: Task-specific classification loss\n",
    "            loss_task = F.cross_entropy(classification_logits, labels)\n",
    "            \n",
    "            # L3 = L2 + lambda * L1\n",
    "            if loss_lm is not None:\n",
    "                loss = loss_task + self.lm_weight * loss_lm\n",
    "            else:\n",
    "                loss = loss_task\n",
    "        \n",
    "        return classification_logits, loss\n",
    "\n",
    "\n",
    "# Demo\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINE-TUNING DEMO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create classification model (e.g., for sentiment: positive/negative)\n",
    "model_cls = GPTForSequenceClassification(config, num_labels=2)\n",
    "model_cls.to(device)\n",
    "\n",
    "# Simulate fine-tuning step\n",
    "batch_size = 4\n",
    "seq_len = 64\n",
    "\n",
    "input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len)).to(device)\n",
    "labels = torch.randint(0, 2, (batch_size,)).to(device)\n",
    "lm_targets = torch.randint(0, config.vocab_size, (batch_size, seq_len)).to(device)\n",
    "\n",
    "# Forward pass\n",
    "logits, loss = model_cls(input_ids, labels=labels, lm_targets=lm_targets)\n",
    "\n",
    "print(f\"\\nForward pass:\")\n",
    "print(f\"  Input shape:  {input_ids.shape}\")\n",
    "print(f\"  Labels shape: {labels.shape}\")\n",
    "print(f\"  Logits shape: {logits.shape}\")\n",
    "print(f\"  Combined loss (L3 = L2 + 0.5*L1): {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Verification and Testing\n",
    "\n",
    "Let's verify our implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_verification_tests(model: GPT, config: GPTConfig):\n",
    "    \"\"\"\n",
    "    Run verification tests on the model.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"VERIFICATION TESTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    all_passed = True\n",
    "    \n",
    "    # Test 1: Output shape\n",
    "    print(\"\\n[Test 1] Output shape\")\n",
    "    x = torch.randint(0, config.vocab_size, (2, 32)).to(device)\n",
    "    logits, _ = model(x)\n",
    "    expected_shape = (2, 32, config.vocab_size)\n",
    "    passed = logits.shape == expected_shape\n",
    "    print(f\"  Expected: {expected_shape}\")\n",
    "    print(f\"  Got:      {tuple(logits.shape)}\")\n",
    "    print(f\"  PASSED: {passed}\")\n",
    "    all_passed &= passed\n",
    "    \n",
    "    # Test 2: Causal masking\n",
    "    print(\"\\n[Test 2] Causal masking (no future leakage)\")\n",
    "    x1 = torch.tensor([[1, 2, 3, 4, 5]]).to(device)\n",
    "    x2 = torch.tensor([[1, 2, 3, 9, 9]]).to(device)  # Different future\n",
    "    logits1, _ = model(x1)\n",
    "    logits2, _ = model(x2)\n",
    "    # Positions 0, 1, 2 should be identical\n",
    "    diff = (logits1[0, :3] - logits2[0, :3]).abs().max().item()\n",
    "    passed = diff < 1e-5\n",
    "    print(f\"  Max diff at positions 0-2: {diff:.8f}\")\n",
    "    print(f\"  PASSED: {passed} (should be ~0)\")\n",
    "    all_passed &= passed\n",
    "    \n",
    "    # Test 3: Loss computation\n",
    "    print(\"\\n[Test 3] Loss computation\")\n",
    "    x = torch.randint(0, config.vocab_size, (4, 64)).to(device)\n",
    "    targets = torch.randint(0, config.vocab_size, (4, 64)).to(device)\n",
    "    _, loss = model(x, targets)\n",
    "    expected_loss = math.log(config.vocab_size)  # Random baseline\n",
    "    passed = abs(loss.item() - expected_loss) < 1.0  # Within 1 of expected\n",
    "    print(f\"  Loss: {loss.item():.4f}\")\n",
    "    print(f\"  Expected (random): {expected_loss:.4f}\")\n",
    "    print(f\"  PASSED: {passed}\")\n",
    "    all_passed &= passed\n",
    "    \n",
    "    # Test 4: Gradient flow\n",
    "    print(\"\\n[Test 4] Gradient flow\")\n",
    "    model.zero_grad()\n",
    "    x = torch.randint(0, config.vocab_size, (2, 32)).to(device)\n",
    "    targets = torch.randint(0, config.vocab_size, (2, 32)).to(device)\n",
    "    _, loss = model(x, targets)\n",
    "    loss.backward()\n",
    "    \n",
    "    has_grad = all(\n",
    "        p.grad is not None and p.grad.abs().sum() > 0 \n",
    "        for p in model.parameters() \n",
    "        if p.requires_grad\n",
    "    )\n",
    "    print(f\"  All parameters have gradients: {has_grad}\")\n",
    "    print(f\"  PASSED: {has_grad}\")\n",
    "    all_passed &= has_grad\n",
    "    \n",
    "    # Test 5: Parameter count\n",
    "    print(\"\\n[Test 5] Parameter count\")\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    # GPT-1 is approximately 117M parameters\n",
    "    passed = 110_000_000 < n_params < 125_000_000\n",
    "    print(f\"  Parameters: {n_params:,}\")\n",
    "    print(f\"  Expected: ~117M\")\n",
    "    print(f\"  PASSED: {passed}\")\n",
    "    all_passed &= passed\n",
    "    \n",
    "    # Test 6: Generation\n",
    "    print(\"\\n[Test 6] Generation\")\n",
    "    prompt = torch.randint(0, 100, (1, 5)).to(device)\n",
    "    generated = generate(model, prompt, max_new_tokens=10)\n",
    "    passed = generated.shape == (1, 15)\n",
    "    print(f\"  Prompt length: 5\")\n",
    "    print(f\"  Generated length: {generated.shape[1]}\")\n",
    "    print(f\"  PASSED: {passed}\")\n",
    "    all_passed &= passed\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"ALL TESTS PASSED: {all_passed}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "run_verification_tests(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Summary: Complete GPT-1 Implementation\n",
    "\n",
    "### What We Built\n",
    "\n",
    "| Component | Implementation | Paper Reference |\n",
    "|-----------|---------------|----------------|\n",
    "| **Configuration** | `GPTConfig` | Section 4.1 |\n",
    "| **GELU** | `gelu()` | \"Gaussian Error Linear Unit\" |\n",
    "| **Layer Norm** | `LayerNorm` | Pre-LN variant |\n",
    "| **Attention** | `CausalSelfAttention` | \"masked self-attention heads\" |\n",
    "| **FFN** | `MLP` | \"3072 dimensional inner states\" |\n",
    "| **Block** | `Block` | Pre-LN residual connections |\n",
    "| **Model** | `GPT` | \"12-layer decoder-only transformer\" |\n",
    "| **Training** | `Trainer` | Section 4.1 hyperparameters |\n",
    "| **Generation** | `generate()` | Autoregressive sampling |\n",
    "| **Fine-tuning** | `GPTForSequenceClassification` | Equation 3 |\n",
    "\n",
    "### Architecture Specifications\n",
    "\n",
    "| Parameter | Value | Source |\n",
    "|-----------|-------|--------|\n",
    "| Layers | 12 | \"12-layer decoder-only transformer\" |\n",
    "| Hidden dim | 768 | \"768 dimensional states\" |\n",
    "| Heads | 12 | \"12 attention heads\" |\n",
    "| Head dim | 64 | 768 / 12 |\n",
    "| FFN dim | 3072 | \"3072 dimensional inner states\" |\n",
    "| Max seq | 512 | \"sequences of 512 tokens\" |\n",
    "| Vocab | 40,478 | \"40,000 merges\" |\n",
    "| Dropout | 0.1 | \"dropouts with a rate of 0.1\" |\n",
    "| Parameters | ~117M | Paper reports |\n",
    "\n",
    "### Key Equations\n",
    "\n",
    "**Pre-training (Equation 1):**\n",
    "$$L_1(\\mathcal{U}) = \\sum_i \\log P(u_i | u_{i-k}, ..., u_{i-1}; \\Theta)$$\n",
    "\n",
    "**Fine-tuning (Equation 3):**\n",
    "$$L_3(C) = L_2(C) + \\lambda \\cdot L_1(C)$$\n",
    "\n",
    "### Historical Impact\n",
    "\n",
    "GPT-1 established the **pre-train then fine-tune** paradigm that now dominates NLP:\n",
    "\n",
    "- **GPT-2** (2019): Scaled up, demonstrated zero-shot capabilities\n",
    "- **GPT-3** (2020): 175B parameters, in-context learning\n",
    "- **BERT** (2018): Bidirectional variant, same paradigm\n",
    "- **ChatGPT** (2022): RLHF fine-tuning for dialogue\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Radford et al. (2018). [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "2. Vaswani et al. (2017). [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "3. Hendrycks & Gimpel (2016). [GELUs](https://arxiv.org/abs/1606.08415)\n",
    "4. Ba et al. (2016). [Layer Normalization](https://arxiv.org/abs/1607.06450)\n",
    "5. Sennrich et al. (2016). [BPE](https://arxiv.org/abs/1508.07909)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
