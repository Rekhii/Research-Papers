{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Research Paper | Part II\n",
    "\n",
    "## The Architecture: Building the Decoder-Only Transformer\n",
    "\n",
    "---\n",
    "\n",
    "**Paper:** [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "\n",
    "**Authors:** Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever (OpenAI, 2018)\n",
    "\n",
    "---\n",
    "\n",
    "In Part I, we understood the motivation behind GPT: leverage massive unlabeled text through unsupervised pre-training, then fine-tune on specific tasks. Now we dive deep into the architecture that makes this possible.\n",
    "\n",
    "This notebook provides:\n",
    "- **Detailed explanations** of every architectural decision\n",
    "- **Direct quotes** from the paper with analysis\n",
    "- **Historical context** comparing to prior work\n",
    "- **Complete implementations** with accurate visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Rectangle, Circle\n",
    "import numpy as np\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The Big Picture: Why This Architecture?\n",
    "\n",
    "### 1.1 The Core Insight from the Paper\n",
    "\n",
    "From Section 3.1:\n",
    "\n",
    "> *\"We use a multi-layer Transformer decoder for the language model, which is a variant of the transformer. This model applies a multi-headed self-attention operation over the input context tokens followed by position-wise feedforward layers to produce an output distribution over target tokens.\"*\n",
    "\n",
    "This single paragraph tells us the entire architecture. Let's break it down:\n",
    "\n",
    "| Phrase | Meaning |\n",
    "|--------|--------|\n",
    "| \"multi-layer\" | Stack multiple blocks (12 in GPT) |\n",
    "| \"Transformer decoder\" | Not encoder-decoder, just decoder |\n",
    "| \"multi-headed self-attention\" | Parallel attention mechanisms |\n",
    "| \"position-wise feedforward\" | Same MLP applied to each position |\n",
    "| \"output distribution over target tokens\" | Predict next token probabilities |\n",
    "\n",
    "### 1.2 Why Decoder-Only? The Three Options in 2018\n",
    "\n",
    "When GPT was being developed, researchers had three main architectural options:\n",
    "\n",
    "**Option 1: RNN/LSTM** (the incumbent)\n",
    "- Pros: Well-understood, proven on many tasks\n",
    "- Cons: Sequential processing (slow), vanishing gradients, limited context\n",
    "\n",
    "**Option 2: Transformer Encoder** (what BERT would use 4 months later)\n",
    "- Pros: Bidirectional context, parallel processing\n",
    "- Cons: Can't do autoregressive generation (sees future tokens)\n",
    "\n",
    "**Option 3: Transformer Decoder** (GPT's choice)\n",
    "- Pros: Parallel processing, natural for language modeling, can generate text\n",
    "- Cons: Only sees left context (not bidirectional)\n",
    "\n",
    "### 1.3 The Key Equation\n",
    "\n",
    "From the paper's Equation 1:\n",
    "\n",
    "$$L_1(\\mathcal{U}) = \\sum_i \\log P(u_i | u_{i-k}, ..., u_{i-1}; \\Theta)$$\n",
    "\n",
    "This is the **language modeling objective**: predict each token given only the *previous* tokens. This fundamentally requires a decoder architecture that can only look backwards.\n",
    "\n",
    "### 1.4 Historical Context: June 2018\n",
    "\n",
    "GPT was published in a pivotal moment:\n",
    "\n",
    "| Date | Model | Architecture | Key Innovation |\n",
    "|------|-------|-------------|----------------|\n",
    "| 2013 | Word2Vec | Static embeddings | Word vectors |\n",
    "| Feb 2018 | ELMo | Bidirectional LSTM | Contextualized embeddings |\n",
    "| **Jun 2018** | **GPT** | **Transformer Decoder** | **Pre-train + fine-tune paradigm** |\n",
    "| Oct 2018 | BERT | Transformer Encoder | Bidirectional pre-training |\n",
    "\n",
    "GPT established that **Transformers could be pre-trained on raw text and transfer to diverse tasks** - a paradigm that now dominates NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Architecture Specifications: The Exact Numbers\n",
    "\n",
    "### 2.1 What the Paper Says\n",
    "\n",
    "From Section 4.1 (Model specifications):\n",
    "\n",
    "> *\"We trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads). For the position-wise feed-forward networks, we used 3072 dimensional inner states.\"*\n",
    "\n",
    "And on training:\n",
    "\n",
    "> *\"We trained for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens.\"*\n",
    "\n",
    "And on regularization:\n",
    "\n",
    "> *\"We use... residual, embedding, and attention dropouts with a rate of 0.1 for regularization.\"*\n",
    "\n",
    "### 2.2 Complete Specification Table\n",
    "\n",
    "| Parameter | Value | Paper Quote / Derivation |\n",
    "|-----------|-------|-------------------------|\n",
    "| Layers | 12 | \"12-layer decoder-only transformer\" |\n",
    "| Hidden dimension | 768 | \"768 dimensional states\" |\n",
    "| Attention heads | 12 | \"12 attention heads\" |\n",
    "| Head dimension | 64 | 768 / 12 = 64 |\n",
    "| FFN inner dim | 3072 | \"3072 dimensional inner states\" |\n",
    "| Max sequence | 512 | \"contiguous sequences of 512 tokens\" |\n",
    "| Vocab size | ~40,478 | \"40,000 merges\" (BPE) + special tokens |\n",
    "| Dropout | 0.1 | \"dropouts with a rate of 0.1\" |\n",
    "\n",
    "### 2.3 Why These Numbers?\n",
    "\n",
    "**768 hidden dimension**: \n",
    "- Divisible by 12 (heads) giving 64 per head\n",
    "- 64 is a power of 2 (GPU-efficient)\n",
    "- Larger than original Transformer's 512, giving more capacity\n",
    "\n",
    "**12 layers**:\n",
    "- Double the original Transformer (6 layers)\n",
    "- More depth = more abstraction levels\n",
    "- Same as BERT-base (published 4 months later)\n",
    "\n",
    "**3072 FFN inner dimension**:\n",
    "- Exactly 4x the hidden size (768 * 4)\n",
    "- Standard ratio from original Transformer\n",
    "- Provides \"memory\" for learned patterns\n",
    "\n",
    "**512 context length**:\n",
    "- Memory scales as O(n^2) with sequence length\n",
    "- 512 is long enough for most documents\n",
    "- Longer than most prior work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"\n",
    "    GPT-1 configuration - every value matches the paper exactly.\n",
    "    \n",
    "    References:\n",
    "    - Section 4.1 for architecture specs\n",
    "    - Section 4.1 for regularization\n",
    "    \"\"\"\n",
    "    # === From Section 4.1 ===\n",
    "    vocab_size: int = 40478       # \"40,000 merges\" + special tokens\n",
    "    n_positions: int = 512        # \"contiguous sequences of 512 tokens\"\n",
    "    n_embd: int = 768             # \"768 dimensional states\"\n",
    "    n_layer: int = 12             # \"12-layer decoder-only transformer\"\n",
    "    n_head: int = 12              # \"12 attention heads\"\n",
    "    n_inner: int = 3072           # \"3072 dimensional inner states\"\n",
    "    \n",
    "    # === Regularization ===\n",
    "    embd_pdrop: float = 0.1       # \"dropouts with a rate of 0.1\"\n",
    "    attn_pdrop: float = 0.1       \n",
    "    resid_pdrop: float = 0.1      \n",
    "    \n",
    "    @property\n",
    "    def head_dim(self) -> int:\n",
    "        \"\"\"Each head has dimension 768/12 = 64\"\"\"\n",
    "        return self.n_embd // self.n_head\n",
    "\n",
    "\n",
    "config = GPTConfig()\n",
    "\n",
    "print(\"GPT-1 Configuration (All values from paper Section 4.1)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n[Architecture]\")\n",
    "print(f\"  Vocabulary:        {config.vocab_size:,} tokens\")\n",
    "print(f\"  Max sequence:      {config.n_positions} tokens\")\n",
    "print(f\"  Hidden dimension:  {config.n_embd}\")\n",
    "print(f\"  Layers:            {config.n_layer}\")\n",
    "print(f\"  Attention heads:   {config.n_head}\")\n",
    "print(f\"  Head dimension:    {config.head_dim} (= {config.n_embd}/{config.n_head})\")\n",
    "print(f\"  FFN inner dim:     {config.n_inner} (= {config.n_embd} * 4)\")\n",
    "print(f\"\\n[Regularization]\")\n",
    "print(f\"  Dropout rate:      {config.embd_pdrop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Complete Architecture Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_gpt_architecture():\n",
    "    \"\"\"Complete GPT architecture with paper annotations.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 18))\n",
    "    ax.set_xlim(0, 14)\n",
    "    ax.set_ylim(0, 18)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Colors\n",
    "    c_emb = '#3498db'   # Blue - embeddings\n",
    "    c_attn = '#e74c3c'  # Red - attention\n",
    "    c_ffn = '#2ecc71'   # Green - FFN\n",
    "    c_norm = '#f39c12'  # Orange - layer norm\n",
    "    c_out = '#9b59b6'   # Purple - output\n",
    "    \n",
    "    # Title\n",
    "    ax.text(7, 17.5, 'GPT Architecture', fontsize=20, fontweight='bold', ha='center')\n",
    "    ax.text(7, 17, '\"12-layer decoder-only transformer\" - Section 4.1', \n",
    "            fontsize=11, ha='center', style='italic', color='gray')\n",
    "    \n",
    "    # === INPUT ===\n",
    "    ax.text(7, 1.2, 'Input: \"The cat sat on the mat\"', fontsize=11, ha='center', fontweight='bold')\n",
    "    tokens = ['<s>', 'The', 'cat', 'sat', 'on', '...']\n",
    "    for i, tok in enumerate(tokens):\n",
    "        x = 2.5 + i * 1.8\n",
    "        rect = FancyBboxPatch((x-0.5, 0.3), 1.0, 0.6, boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor='white', edgecolor='black', linewidth=1.5)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x, 0.6, tok, ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    ax.annotate('', xy=(7, 2.0), xytext=(7, 1.4),\n",
    "                arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "    \n",
    "    # === EMBEDDINGS ===\n",
    "    rect_te = FancyBboxPatch((3.5, 2.1), 3, 1, boxstyle=\"round,pad=0.03\",\n",
    "                             facecolor=c_emb, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect_te)\n",
    "    ax.text(5, 2.6, 'Token Embedding', ha='center', va='center', fontsize=11,\n",
    "            fontweight='bold', color='white')\n",
    "    \n",
    "    rect_pe = FancyBboxPatch((7.5, 2.1), 3, 1, boxstyle=\"round,pad=0.03\",\n",
    "                             facecolor=c_emb, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect_pe)\n",
    "    ax.text(9, 2.6, 'Position Embedding', ha='center', va='center', fontsize=11,\n",
    "            fontweight='bold', color='white')\n",
    "    \n",
    "    ax.text(6.75, 2.6, '+', fontsize=20, ha='center', va='center', fontweight='bold')\n",
    "    ax.text(12, 2.6, '40,478 x 768\\n(learned)', fontsize=9, ha='left', \n",
    "            va='center', color='gray', style='italic')\n",
    "    \n",
    "    ax.annotate('', xy=(7, 3.6), xytext=(7, 3.2),\n",
    "                arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "    \n",
    "    # Dropout\n",
    "    rect_drop = FancyBboxPatch((5.5, 3.7), 3, 0.5, boxstyle=\"round,pad=0.02\",\n",
    "                               facecolor='#ecf0f1', edgecolor='black', linewidth=1)\n",
    "    ax.add_patch(rect_drop)\n",
    "    ax.text(7, 3.95, 'Dropout (p=0.1)', ha='center', va='center', fontsize=9)\n",
    "    \n",
    "    # === TRANSFORMER BLOCK ===\n",
    "    block_y = 4.5\n",
    "    block_h = 7.5\n",
    "    \n",
    "    rect_block = FancyBboxPatch((2, block_y), 10, block_h, boxstyle=\"round,pad=0.05\",\n",
    "                                facecolor='#f8f9fa', edgecolor='#2c3e50', linewidth=2.5)\n",
    "    ax.add_patch(rect_block)\n",
    "    ax.text(0.8, block_y + block_h/2, 'Transformer\\nDecoder\\nBlock\\n\\nx 12', fontsize=11,\n",
    "            ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    ax.annotate('', xy=(7, block_y + 0.4), xytext=(7, 4.3),\n",
    "                arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "    \n",
    "    # Layer Norm 1\n",
    "    ln1_y = block_y + 0.6\n",
    "    rect_ln1 = FancyBboxPatch((4, ln1_y), 6, 0.7, boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor=c_norm, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect_ln1)\n",
    "    ax.text(7, ln1_y + 0.35, 'Layer Normalization', ha='center', va='center',\n",
    "            fontsize=10, fontweight='bold', color='white')\n",
    "    \n",
    "    ax.annotate('', xy=(7, ln1_y + 1.1), xytext=(7, ln1_y + 0.75),\n",
    "                arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "    \n",
    "    # Attention\n",
    "    attn_y = ln1_y + 1.2\n",
    "    rect_attn = FancyBboxPatch((3.5, attn_y), 7, 1.6, boxstyle=\"round,pad=0.03\",\n",
    "                               facecolor=c_attn, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect_attn)\n",
    "    ax.text(7, attn_y + 1.0, 'Masked Multi-Head', ha='center', va='center',\n",
    "            fontsize=12, fontweight='bold', color='white')\n",
    "    ax.text(7, attn_y + 0.5, 'Self-Attention', ha='center', va='center',\n",
    "            fontsize=12, fontweight='bold', color='white')\n",
    "    ax.text(11.5, attn_y + 1.0, '12 heads', fontsize=9, ha='left', color='gray', style='italic')\n",
    "    ax.text(11.5, attn_y + 0.6, '64 dim/head', fontsize=9, ha='left', color='gray', style='italic')\n",
    "    \n",
    "    # Residual 1\n",
    "    res_x = 2.8\n",
    "    ax.plot([res_x, res_x], [block_y + 0.4, attn_y + 2.0], 'k-', linewidth=2)\n",
    "    ax.plot([res_x, 6.6], [attn_y + 2.0, attn_y + 2.0], 'k-', linewidth=2)\n",
    "    \n",
    "    ax.annotate('', xy=(7, attn_y + 2.15), xytext=(7, attn_y + 1.7),\n",
    "                arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "    circle1 = Circle((7, attn_y + 2.2), 0.25, facecolor='white', edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(circle1)\n",
    "    ax.text(7, attn_y + 2.2, '+', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Layer Norm 2\n",
    "    ln2_y = attn_y + 2.6\n",
    "    rect_ln2 = FancyBboxPatch((4, ln2_y), 6, 0.7, boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor=c_norm, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect_ln2)\n",
    "    ax.text(7, ln2_y + 0.35, 'Layer Normalization', ha='center', va='center',\n",
    "            fontsize=10, fontweight='bold', color='white')\n",
    "    \n",
    "    ax.annotate('', xy=(7, ln2_y + 1.0), xytext=(7, ln2_y + 0.75),\n",
    "                arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "    \n",
    "    # FFN\n",
    "    ffn_y = ln2_y + 1.1\n",
    "    rect_ffn = FancyBboxPatch((3.5, ffn_y), 7, 1.6, boxstyle=\"round,pad=0.03\",\n",
    "                              facecolor=c_ffn, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect_ffn)\n",
    "    ax.text(7, ffn_y + 1.0, 'Position-wise', ha='center', va='center',\n",
    "            fontsize=12, fontweight='bold', color='white')\n",
    "    ax.text(7, ffn_y + 0.5, 'Feed-Forward Network', ha='center', va='center',\n",
    "            fontsize=12, fontweight='bold', color='white')\n",
    "    ax.text(11.5, ffn_y + 1.0, '768 -> 3072 -> 768', fontsize=9, ha='left', color='gray', style='italic')\n",
    "    ax.text(11.5, ffn_y + 0.6, 'GELU activation', fontsize=9, ha='left', color='gray', style='italic')\n",
    "    \n",
    "    # Residual 2\n",
    "    ax.plot([res_x, res_x], [attn_y + 2.45, ffn_y + 1.9], 'k-', linewidth=2)\n",
    "    ax.plot([res_x, 6.6], [ffn_y + 1.9, ffn_y + 1.9], 'k-', linewidth=2)\n",
    "    \n",
    "    ax.annotate('', xy=(7, ffn_y + 2.05), xytext=(7, ffn_y + 1.7),\n",
    "                arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "    circle2 = Circle((7, ffn_y + 2.1), 0.25, facecolor='white', edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(circle2)\n",
    "    ax.text(7, ffn_y + 2.1, '+', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # === OUTPUT ===\n",
    "    out_y = block_y + block_h + 0.3\n",
    "    ax.annotate('', xy=(7, out_y + 0.3), xytext=(7, out_y),\n",
    "                arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "    \n",
    "    rect_ln_f = FancyBboxPatch((4, out_y + 0.4), 6, 0.7, boxstyle=\"round,pad=0.02\",\n",
    "                               facecolor=c_norm, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect_ln_f)\n",
    "    ax.text(7, out_y + 0.75, 'Layer Normalization', ha='center', va='center',\n",
    "            fontsize=10, fontweight='bold', color='white')\n",
    "    \n",
    "    ax.annotate('', xy=(7, out_y + 1.5), xytext=(7, out_y + 1.15),\n",
    "                arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "    \n",
    "    rect_head = FancyBboxPatch((4, out_y + 1.6), 6, 0.8, boxstyle=\"round,pad=0.03\",\n",
    "                               facecolor=c_out, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect_head)\n",
    "    ax.text(7, out_y + 2.0, 'Linear (768 -> 40,478)', ha='center', va='center',\n",
    "            fontsize=10, fontweight='bold', color='white')\n",
    "    ax.text(11.5, out_y + 2.0, 'Weight tied with\\ntoken embedding', fontsize=9, \n",
    "            ha='left', color='gray', style='italic')\n",
    "    \n",
    "    ax.annotate('', xy=(7, out_y + 2.8), xytext=(7, out_y + 2.45),\n",
    "                arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "    \n",
    "    rect_soft = FancyBboxPatch((5, out_y + 2.9), 4, 0.6, boxstyle=\"round,pad=0.02\",\n",
    "                               facecolor='#ecf0f1', edgecolor='black', linewidth=1.5)\n",
    "    ax.add_patch(rect_soft)\n",
    "    ax.text(7, out_y + 3.2, 'Softmax', ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    ax.annotate('', xy=(7, out_y + 3.9), xytext=(7, out_y + 3.55),\n",
    "                arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "    ax.text(7, out_y + 4.2, 'P(next token | context)', fontsize=12, ha='center', \n",
    "            va='center', fontweight='bold')\n",
    "    \n",
    "    # Legend\n",
    "    legend_items = [(c_emb, 'Embeddings'), (c_norm, 'Layer Norm'),\n",
    "                    (c_attn, 'Attention'), (c_ffn, 'Feed-Forward'), (c_out, 'Output')]\n",
    "    for i, (color, label) in enumerate(legend_items):\n",
    "        x = 1.5 + i * 2.4\n",
    "        rect = Rectangle((x, 16.3), 0.4, 0.4, facecolor=color, edgecolor='black')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x + 0.55, 16.5, label, fontsize=9, va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "draw_gpt_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. GELU Activation: Why Not ReLU?\n",
    "\n",
    "### 3.1 The Paper's Choice\n",
    "\n",
    "The original Transformer (2017) used ReLU. GPT explicitly chose **GELU**:\n",
    "\n",
    "> *\"We used... the Gaussian Error Linear Unit (GELU) activation function.\"*\n",
    "\n",
    "### 3.2 What is GELU?\n",
    "\n",
    "GELU (Gaussian Error Linear Unit) was introduced by Hendrycks & Gimpel in 2016. The key idea:\n",
    "\n",
    "**ReLU asks**: \"Is this value positive?\" (binary gate)\n",
    "$$\\text{ReLU}(x) = \\max(0, x) = x \\cdot \\mathbf{1}_{x > 0}$$\n",
    "\n",
    "**GELU asks**: \"How likely is this value to be larger than others?\" (probabilistic gate)\n",
    "$$\\text{GELU}(x) = x \\cdot \\Phi(x)$$\n",
    "\n",
    "Where $\\Phi(x)$ is the CDF of the standard normal distribution.\n",
    "\n",
    "### 3.3 The Approximation Used in Practice\n",
    "\n",
    "Computing the exact CDF is expensive. GPT uses this approximation:\n",
    "\n",
    "$$\\text{GELU}(x) \\approx 0.5x\\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}}(x + 0.044715x^3)\\right]\\right)$$\n",
    "\n",
    "### 3.4 Why GELU Works Better\n",
    "\n",
    "| Property | ReLU | GELU |\n",
    "|----------|------|------|\n",
    "| For x < 0 | Always 0 (\"dead\") | Small negative values |\n",
    "| At x = 0 | Not differentiable | Smooth |\n",
    "| Gradient | Discontinuous | Continuous everywhere |\n",
    "| Interpretation | Hard gate | Soft, probabilistic gate |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu_approx(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"GELU activation (tanh approximation as used in GPT).\"\"\"\n",
    "    return 0.5 * x * (1.0 + torch.tanh(\n",
    "        math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3))\n",
    "    ))\n",
    "\n",
    "\n",
    "# Visualization\n",
    "x = torch.linspace(-4, 4, 300)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: GELU vs ReLU\n",
    "ax1 = axes[0]\n",
    "ax1.plot(x.numpy(), gelu_approx(x).numpy(), 'b-', linewidth=3, label='GELU (GPT)')\n",
    "ax1.plot(x.numpy(), F.relu(x).numpy(), 'r--', linewidth=2.5, label='ReLU (Original Transformer)')\n",
    "ax1.axhline(y=0, color='gray', linewidth=0.8, linestyle=':')\n",
    "ax1.axvline(x=0, color='gray', linewidth=0.8, linestyle=':')\n",
    "ax1.fill_between(x.numpy(), 0, gelu_approx(x).numpy(), where=(x.numpy() < 0), \n",
    "                 alpha=0.3, color='blue', label='GELU allows small negatives')\n",
    "ax1.set_xlabel('Input (x)', fontsize=12)\n",
    "ax1.set_ylabel('Output', fontsize=12)\n",
    "ax1.set_title('GELU vs ReLU Activation Functions', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(-4, 4)\n",
    "ax1.set_ylim(-0.5, 4)\n",
    "\n",
    "# Plot 2: The \"gate\" interpretation\n",
    "ax2 = axes[1]\n",
    "gate = 0.5 * (1 + torch.tanh(math.sqrt(2/math.pi) * (x + 0.044715 * x**3)))\n",
    "relu_gate = (x > 0).float()\n",
    "\n",
    "ax2.plot(x.numpy(), gate.numpy(), 'b-', linewidth=3, label='GELU gate (smooth)')\n",
    "ax2.plot(x.numpy(), relu_gate.numpy(), 'r--', linewidth=2.5, label='ReLU gate (hard)')\n",
    "ax2.axhline(y=0.5, color='gray', linewidth=0.8, linestyle=':')\n",
    "ax2.axvline(x=0, color='gray', linewidth=0.8, linestyle=':')\n",
    "ax2.set_xlabel('Input (x)', fontsize=12)\n",
    "ax2.set_ylabel('Gate Value', fontsize=12)\n",
    "ax2.set_title('The \"Gating\" Function\\nGELU(x) = x * gate(x)', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(-4, 4)\n",
    "ax2.set_ylim(-0.1, 1.1)\n",
    "\n",
    "ax2.annotate('Smooth\\ntransition', xy=(0, 0.5), xytext=(1.5, 0.3),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue', lw=2),\n",
    "            fontsize=11, color='blue', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Why GELU became standard in modern LLMs:\")\n",
    "print(\"  1. Smooth gradients -> better optimization\")\n",
    "print(\"  2. Non-zero for negative inputs -> no 'dead neurons'\")\n",
    "print(\"  3. Now used in GPT-2, GPT-3, GPT-4, BERT, and most transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Layer Normalization: Pre-LN vs Post-LN\n",
    "\n",
    "### 4.1 Why Layer Norm?\n",
    "\n",
    "Deep networks suffer from **internal covariate shift** - layer inputs change distribution during training. Normalization stabilizes this.\n",
    "\n",
    "**Batch Norm** (commonly used in vision): Normalizes across batch dimension\n",
    "- Problem: Batch statistics vary, doesn't work for variable-length sequences\n",
    "\n",
    "**Layer Norm** (used in GPT): Normalizes across feature dimension\n",
    "- Each position normalized independently\n",
    "- Works with any batch size, any sequence length\n",
    "\n",
    "### 4.2 The Math\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "Where:\n",
    "- $\\mu = \\frac{1}{d}\\sum_{i=1}^{d} x_i$ (mean across 768 features)\n",
    "- $\\sigma^2 = \\frac{1}{d}\\sum_{i=1}^{d} (x_i - \\mu)^2$ (variance)\n",
    "- $\\gamma, \\beta$ are learnable parameters\n",
    "\n",
    "### 4.3 Pre-LN vs Post-LN (Critical Choice!)\n",
    "\n",
    "**Post-LN** (Original Transformer 2017):\n",
    "```\n",
    "x = LayerNorm(x + Sublayer(x))\n",
    "```\n",
    "\n",
    "**Pre-LN** (GPT and most modern transformers):\n",
    "```\n",
    "x = x + Sublayer(LayerNorm(x))\n",
    "```\n",
    "\n",
    "**Why Pre-LN is better:**\n",
    "1. Residual path stays \"clean\" (gradients flow directly)\n",
    "2. More stable training without careful warmup\n",
    "3. Enables training much deeper networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"Layer Normalization as used in GPT (normalizes across features).\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(n_embd))   # Scale\n",
    "        self.beta = nn.Parameter(torch.zeros(n_embd))   # Shift\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.gamma * x_norm + self.beta\n",
    "\n",
    "\n",
    "# Test\n",
    "ln = LayerNorm(config.n_embd)\n",
    "x = torch.randn(2, 5, config.n_embd) * 10 + 5  # Unnormalized\n",
    "out = ln(x)\n",
    "\n",
    "print(\"LayerNorm Test\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Input:  mean={x[0,0].mean():.2f}, std={x[0,0].std():.2f}\")\n",
    "print(f\"Output: mean={out[0,0].mean():.4f}, std={out[0,0].std():.4f}\")\n",
    "print(f\"\\nAfter LayerNorm: mean -> 0, std -> 1 (as expected)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Causal Self-Attention: The Heart of GPT\n",
    "\n",
    "### 5.1 What the Paper Says\n",
    "\n",
    "> *\"This model applies a **multi-headed self-attention** operation over the input context tokens...\"*\n",
    "\n",
    "> *\"We trained a 12-layer decoder-only transformer with **masked self-attention heads**\"*\n",
    "\n",
    "The word **\"masked\"** is crucial - it means causal masking.\n",
    "\n",
    "### 5.2 Why Causal Masking?\n",
    "\n",
    "The training objective (Equation 1 from the paper):\n",
    "\n",
    "$$L_1(\\mathcal{U}) = \\sum_i \\log P(u_i | u_{i-k}, ..., u_{i-1}; \\Theta)$$\n",
    "\n",
    "To predict token $u_i$, we can only use tokens $u_1, ..., u_{i-1}$. If position $i$ could see position $i+1$, it would be **cheating**.\n",
    "\n",
    "### 5.3 The Attention Equation\n",
    "\n",
    "Standard attention:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Causal attention adds a mask:\n",
    "$$\\text{CausalAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M\\right)V$$\n",
    "\n",
    "Where:\n",
    "$$M_{ij} = \\begin{cases} 0 & \\text{if } j \\leq i \\\\ -\\infty & \\text{if } j > i \\end{cases}$$\n",
    "\n",
    "### 5.4 Multi-Head Attention\n",
    "\n",
    "From the paper: **\"12 attention heads\"** with **\"768 dimensional states\"**\n",
    "\n",
    "This means:\n",
    "- 12 parallel attention operations\n",
    "- Each head has dimension 768/12 = 64\n",
    "- Each head can learn different patterns (syntax, semantics, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head causal self-attention.\n",
    "    \n",
    "    From paper: \"12 attention heads\" with \"768 dimensional states\"\n",
    "    -> 64 dimensions per head\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = config.n_embd // config.n_head\n",
    "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Q, K, V projections combined\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # Output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # Dropout\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        # Causal mask\n",
    "        mask = torch.tril(torch.ones(config.n_positions, config.n_positions))\n",
    "        self.register_buffer('mask', mask.view(1, 1, config.n_positions, config.n_positions))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        \n",
    "        # Reshape for multi-head\n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Attention scores\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        attn_weights = F.softmax(attn, dim=-1)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "        \n",
    "        # Apply to values\n",
    "        out = attn_weights @ v\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        out = self.resid_dropout(self.c_proj(out))\n",
    "        \n",
    "        return out, attn_weights\n",
    "\n",
    "\n",
    "# Test\n",
    "attn = CausalSelfAttention(config)\n",
    "x = torch.randn(2, 10, config.n_embd)\n",
    "out, weights = attn(x)\n",
    "\n",
    "print(\"Causal Self-Attention Test\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Input:            {x.shape}\")\n",
    "print(f\"Output:           {out.shape}\")\n",
    "print(f\"Attention shape:  {weights.shape} (batch, heads, queries, keys)\")\n",
    "print(f\"Parameters:       {sum(p.numel() for p in attn.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_causal_attention():\n",
    "    \"\"\"Visualize the causal attention mechanism.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    \n",
    "    tokens = ['<s>', 'The', 'cat', 'sat', 'on']\n",
    "    n = len(tokens)\n",
    "    \n",
    "    # Generate example scores\n",
    "    np.random.seed(42)\n",
    "    scores = np.random.randn(n, n) * 0.5\n",
    "    \n",
    "    # Plot 1: Raw scores\n",
    "    ax1 = axes[0]\n",
    "    im1 = ax1.imshow(scores, cmap='RdBu_r', vmin=-2, vmax=2)\n",
    "    ax1.set_title('Step 1: Raw Attention Scores\\n(QK^T / sqrt(d_k))', fontsize=12, fontweight='bold')\n",
    "    ax1.set_xticks(range(n))\n",
    "    ax1.set_xticklabels(tokens, fontsize=10)\n",
    "    ax1.set_yticks(range(n))\n",
    "    ax1.set_yticklabels(tokens, fontsize=10)\n",
    "    ax1.set_xlabel('Key (attending TO)', fontsize=10)\n",
    "    ax1.set_ylabel('Query (attending FROM)', fontsize=10)\n",
    "    plt.colorbar(im1, ax=ax1, shrink=0.8)\n",
    "    \n",
    "    # Plot 2: Causal mask\n",
    "    ax2 = axes[1]\n",
    "    mask = np.tril(np.ones((n, n)))\n",
    "    scores_masked = np.where(mask == 1, scores, -10)\n",
    "    im2 = ax2.imshow(scores_masked, cmap='RdBu_r', vmin=-3, vmax=2)\n",
    "    ax2.set_title('Step 2: Apply Causal Mask\\n(future = -inf)', fontsize=12, fontweight='bold')\n",
    "    ax2.set_xticks(range(n))\n",
    "    ax2.set_xticklabels(tokens, fontsize=10)\n",
    "    ax2.set_yticks(range(n))\n",
    "    ax2.set_yticklabels(tokens, fontsize=10)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if j > i:\n",
    "                ax2.text(j, i, 'X', ha='center', va='center', fontsize=12, \n",
    "                        color='red', fontweight='bold')\n",
    "    plt.colorbar(im2, ax=ax2, shrink=0.8)\n",
    "    \n",
    "    # Plot 3: After softmax\n",
    "    ax3 = axes[2]\n",
    "    scores_inf = np.where(mask == 1, scores, -np.inf)\n",
    "    attn_weights = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        row = scores_inf[i, :i+1]\n",
    "        exp_row = np.exp(row - np.max(row))\n",
    "        attn_weights[i, :i+1] = exp_row / exp_row.sum()\n",
    "    \n",
    "    im3 = ax3.imshow(attn_weights, cmap='Blues', vmin=0, vmax=1)\n",
    "    ax3.set_title('Step 3: Softmax\\n(each row sums to 1)', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xticks(range(n))\n",
    "    ax3.set_xticklabels(tokens, fontsize=10)\n",
    "    ax3.set_yticks(range(n))\n",
    "    ax3.set_yticklabels(tokens, fontsize=10)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            val = attn_weights[i, j]\n",
    "            if val > 0.01:\n",
    "                color = 'white' if val > 0.5 else 'black'\n",
    "                ax3.text(j, i, f'{val:.2f}', ha='center', va='center', fontsize=9, color=color)\n",
    "    plt.colorbar(im3, ax=ax3, shrink=0.8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Key insight: Each position can ONLY attend to itself and previous positions.\")\n",
    "    print(\"  - '<s>' (pos 0): 100% self-attention\")\n",
    "    print(\"  - 'on'  (pos 4): Can attend to all 5 tokens\")\n",
    "\n",
    "visualize_causal_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Feed-Forward Network: The \"Memory\"\n",
    "\n",
    "### 6.1 From the Paper\n",
    "\n",
    "> *\"For the position-wise feed-forward networks, we used 3072 dimensional inner states.\"*\n",
    "\n",
    "### 6.2 Architecture\n",
    "\n",
    "$$\\text{FFN}(x) = \\text{GELU}(xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "- $W_1 \\in \\mathbb{R}^{768 \\times 3072}$ (expand)\n",
    "- $W_2 \\in \\mathbb{R}^{3072 \\times 768}$ (contract)\n",
    "\n",
    "### 6.3 Why FFN Matters\n",
    "\n",
    "Think of attention and FFN as doing different jobs:\n",
    "- **Attention**: Routing information between positions\n",
    "- **FFN**: Processing information at each position\n",
    "\n",
    "The 3072-dimensional hidden layer acts like a \"memory\" that stores learned patterns. Research has shown individual neurons often correspond to specific concepts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Position-wise FFN. From paper: '3072 dimensional inner states'\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, config.n_inner)    # 768 -> 3072\n",
    "        self.c_proj = nn.Linear(config.n_inner, config.n_embd)  # 3072 -> 768\n",
    "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.c_fc(x)\n",
    "        x = gelu_approx(x)\n",
    "        x = self.c_proj(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "mlp = MLP(config)\n",
    "print(\"FFN Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Architecture: 768 -> 3072 -> 768 (4x expansion)\")\n",
    "print(f\"Parameters:   {sum(p.numel() for p in mlp.parameters()):,}\")\n",
    "print(f\"  W1: 768 x 3072 + 3072 = {768*3072 + 3072:,}\")\n",
    "print(f\"  W2: 3072 x 768 + 768 = {3072*768 + 768:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Complete GPT Model\n",
    "\n",
    "Now we assemble all components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block with Pre-LayerNorm.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        attn_out, attn_weights = self.attn(self.ln_1(x))\n",
    "        x = x + attn_out  # Residual\n",
    "        x = x + self.mlp(self.ln_2(x))  # Residual\n",
    "        return x, attn_weights\n",
    "\n",
    "\n",
    "class GPTEmbeddings(nn.Module):\n",
    "    \"\"\"Token + Position embeddings (both learned).\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "    \n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        B, T = input_ids.shape\n",
    "        tok_emb = self.wte(input_ids)\n",
    "        pos_emb = self.wpe(torch.arange(T, device=input_ids.device))\n",
    "        return self.drop(tok_emb + pos_emb)\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"Complete GPT model matching the paper.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.embeddings = GPTEmbeddings(config)\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying (important!)\n",
    "        self.lm_head.weight = self.embeddings.wte.weight\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        print(f\"GPT: {sum(p.numel() for p in self.parameters()):,} parameters\")\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids: torch.Tensor, targets: Optional[torch.Tensor] = None):\n",
    "        x = self.embeddings(input_ids)\n",
    "        for block in self.blocks:\n",
    "            x, _ = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_analysis(config):\n",
    "    \"\"\"Detailed parameter breakdown.\"\"\"\n",
    "    print(\"\\nGPT-1 Parameter Breakdown\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Embeddings\n",
    "    tok_emb = config.vocab_size * config.n_embd\n",
    "    pos_emb = config.n_positions * config.n_embd\n",
    "    print(f\"\\nEMBEDDINGS\")\n",
    "    print(f\"  Token:    {config.vocab_size:,} x {config.n_embd} = {tok_emb:,}\")\n",
    "    print(f\"  Position: {config.n_positions} x {config.n_embd} = {pos_emb:,}\")\n",
    "    \n",
    "    # Per block\n",
    "    attn = config.n_embd * (3 * config.n_embd) + (3 * config.n_embd) + \\\n",
    "           config.n_embd * config.n_embd + config.n_embd\n",
    "    ffn = config.n_embd * config.n_inner + config.n_inner + \\\n",
    "          config.n_inner * config.n_embd + config.n_embd\n",
    "    ln = 4 * config.n_embd  # 2 LayerNorms\n",
    "    block = attn + ffn + ln\n",
    "    \n",
    "    print(f\"\\nPER BLOCK\")\n",
    "    print(f\"  Attention:  {attn:,}\")\n",
    "    print(f\"  FFN:        {ffn:,}\")\n",
    "    print(f\"  LayerNorm:  {ln:,}\")\n",
    "    print(f\"  Total:      {block:,}\")\n",
    "    \n",
    "    all_blocks = block * config.n_layer\n",
    "    final_ln = 2 * config.n_embd\n",
    "    total = tok_emb + pos_emb + all_blocks + final_ln\n",
    "    \n",
    "    print(f\"\\nTOTAL\")\n",
    "    print(f\"  12 Blocks:    {all_blocks:,}\")\n",
    "    print(f\"  Final LN:     {final_ln:,}\")\n",
    "    print(f\"  LM Head:      (tied, 0 additional)\")\n",
    "    print(f\"  \" + \"=\"*40)\n",
    "    print(f\"  TOTAL:        {total:,}\")\n",
    "    print(f\"\\n  Paper reports: ~117M parameters\")\n",
    "\n",
    "parameter_analysis(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "model.eval()\n",
    "x = torch.randint(0, config.vocab_size, (2, 64))\n",
    "y = torch.randint(0, config.vocab_size, (2, 64))\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits, loss = model(x, y)\n",
    "\n",
    "print(\"\\nForward Pass Test\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Input:  {x.shape}\")\n",
    "print(f\"Logits: {logits.shape}\")\n",
    "print(f\"Loss:   {loss.item():.4f}\")\n",
    "print(f\"Expected (random): {math.log(config.vocab_size):.4f}\")\n",
    "print(f\"\\nModel working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Summary\n",
    "\n",
    "### Key Architectural Choices (All from Paper)\n",
    "\n",
    "| Choice | GPT Specification | Why |\n",
    "|--------|------------------|-----|\n",
    "| Architecture | Decoder-only | Language modeling is autoregressive |\n",
    "| Depth | 12 layers | Balance of capacity and efficiency |\n",
    "| Hidden dim | 768 | Divisible by 12, GPU-efficient |\n",
    "| Attention heads | 12 | Multiple parallel attention patterns |\n",
    "| FFN inner | 3072 (4x) | Standard expansion ratio |\n",
    "| Activation | GELU | Smooth, better than ReLU |\n",
    "| Normalization | Pre-LN | Better gradient flow |\n",
    "| Positions | Learned | Flexibility |\n",
    "| Output | Tied weights | Reduces parameters |\n",
    "\n",
    "### Differences from Original Transformer (2017)\n",
    "\n",
    "| Aspect | Original | GPT |\n",
    "|--------|----------|-----|\n",
    "| Structure | Encoder-Decoder | Decoder-only |\n",
    "| Position | Sinusoidal | Learned |\n",
    "| Activation | ReLU | GELU |\n",
    "| LayerNorm | Post-LN | Pre-LN |\n",
    "| Weight tying | No | Yes |\n",
    "\n",
    "### What's Next\n",
    "\n",
    "**Part III**: Pre-training - Training objective, learning rate schedule, text generation\n",
    "\n",
    "**Part IV**: Fine-tuning - Task-specific input transformations, auxiliary loss\n",
    "\n",
    "**Part V**: Complete implementation and evaluation\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Radford et al. (2018). [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "2. Vaswani et al. (2017). [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "3. Hendrycks & Gimpel (2016). [GELUs](https://arxiv.org/abs/1606.08415)\n",
    "4. Ba et al. (2016). [Layer Normalization](https://arxiv.org/abs/1607.06450)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
