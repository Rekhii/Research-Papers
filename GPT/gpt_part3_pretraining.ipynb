{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Research Paper | Part III\n",
    "\n",
    "## Pre-training: Language Modeling, Training, and Generation\n",
    "\n",
    "---\n",
    "\n",
    "**Paper:** [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "\n",
    "**Authors:** Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever (OpenAI, 2018)\n",
    "\n",
    "---\n",
    "\n",
    "In Part II, we built the complete GPT architecture. Now we dive into **how it's trained**:\n",
    "\n",
    "1. The **language modeling objective** - what GPT learns\n",
    "2. The **training procedure** - optimization details from the paper\n",
    "3. **Text generation** - how to sample from the trained model\n",
    "\n",
    "This is where the magic happens - turning 117M random parameters into a model that understands language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Rectangle, Circle, FancyArrowPatch\n",
    "import numpy as np\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The Language Modeling Objective\n",
    "\n",
    "### 1.1 What the Paper Says\n",
    "\n",
    "From Section 3.1 (Unsupervised pre-training):\n",
    "\n",
    "> *\"Given an unsupervised corpus of tokens $\\mathcal{U} = \\{u_1, ..., u_n\\}$, we use a standard language modeling objective to maximize the following likelihood:\"*\n",
    "\n",
    "$$L_1(\\mathcal{U}) = \\sum_i \\log P(u_i | u_{i-k}, ..., u_{i-1}; \\Theta)$$\n",
    "\n",
    "This is **the core of GPT** - predict each token given the previous tokens.\n",
    "\n",
    "### 1.2 Breaking Down the Equation\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|--------|\n",
    "| $\\mathcal{U} = \\{u_1, ..., u_n\\}$ | The training corpus (sequence of tokens) |\n",
    "| $u_i$ | The token at position $i$ |\n",
    "| $k$ | Context window size (512 in GPT) |\n",
    "| $u_{i-k}, ..., u_{i-1}$ | The $k$ tokens before position $i$ |\n",
    "| $\\Theta$ | Model parameters (~117M) |\n",
    "| $P(u_i | ...)$ | Probability of token $u_i$ given context |\n",
    "\n",
    "### 1.3 The Intuition\n",
    "\n",
    "Consider the sentence: \"The cat sat on the mat\"\n",
    "\n",
    "GPT learns to predict:\n",
    "- P(\"cat\" | \"The\") should be high\n",
    "- P(\"sat\" | \"The cat\") should be high\n",
    "- P(\"on\" | \"The cat sat\") should be high\n",
    "- P(\"banana\" | \"The cat sat\") should be low\n",
    "\n",
    "By learning these patterns across billions of tokens, GPT develops an understanding of language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_language_modeling():\n",
    "    \"\"\"Visualize the language modeling objective.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    ax.set_xlim(0, 16)\n",
    "    ax.set_ylim(0, 8)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Title\n",
    "    ax.text(8, 7.5, 'Language Modeling Objective', fontsize=16, fontweight='bold', ha='center')\n",
    "    ax.text(8, 7, r'$L_1(\\mathcal{U}) = \\sum_i \\log P(u_i | u_{i-k}, ..., u_{i-1}; \\Theta)$', \n",
    "            fontsize=14, ha='center', style='italic')\n",
    "    \n",
    "    # Input sequence\n",
    "    tokens = ['<s>', 'The', 'cat', 'sat', 'on', 'the', 'mat']\n",
    "    colors = ['#3498db', '#3498db', '#3498db', '#3498db', '#3498db', '#3498db', '#3498db']\n",
    "    \n",
    "    ax.text(1, 5.8, 'Input tokens (context):', fontsize=11, fontweight='bold')\n",
    "    for i, (tok, col) in enumerate(zip(tokens[:-1], colors[:-1])):\n",
    "        x = 1.5 + i * 2\n",
    "        rect = FancyBboxPatch((x-0.5, 5), 1.2, 0.6, boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor=col, edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x + 0.1, 5.3, tok, ha='center', va='center', fontsize=11, color='white', fontweight='bold')\n",
    "    \n",
    "    # Arrows down\n",
    "    for i in range(6):\n",
    "        x = 1.6 + i * 2\n",
    "        ax.annotate('', xy=(x, 4.3), xytext=(x, 4.9),\n",
    "                    arrowprops=dict(arrowstyle='->', color='black', lw=1.5))\n",
    "    \n",
    "    # GPT box\n",
    "    rect_gpt = FancyBboxPatch((1, 3.5), 12, 0.7, boxstyle=\"round,pad=0.03\",\n",
    "                              facecolor='#2c3e50', edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect_gpt)\n",
    "    ax.text(7, 3.85, 'GPT Model (12 layers, 768 dim, 12 heads)', \n",
    "            ha='center', va='center', fontsize=12, color='white', fontweight='bold')\n",
    "    \n",
    "    # Arrows down\n",
    "    for i in range(6):\n",
    "        x = 1.6 + i * 2\n",
    "        ax.annotate('', xy=(x, 2.8), xytext=(x, 3.4),\n",
    "                    arrowprops=dict(arrowstyle='->', color='black', lw=1.5))\n",
    "    \n",
    "    # Predictions\n",
    "    ax.text(1, 2.3, 'Predict next token:', fontsize=11, fontweight='bold')\n",
    "    predictions = ['The', 'cat', 'sat', 'on', 'the', 'mat']\n",
    "    for i, pred in enumerate(predictions):\n",
    "        x = 1.5 + i * 2\n",
    "        rect = FancyBboxPatch((x-0.5, 1.5), 1.2, 0.6, boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor='#e74c3c', edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x + 0.1, 1.8, pred, ha='center', va='center', fontsize=11, color='white', fontweight='bold')\n",
    "    \n",
    "    # Loss computation\n",
    "    ax.text(8, 0.8, 'Loss = -log P(\"The\"|\"<s>\") - log P(\"cat\"|\"<s> The\") - log P(\"sat\"|\"<s> The cat\") - ...', \n",
    "            fontsize=10, ha='center', style='italic')\n",
    "    ax.text(8, 0.3, 'Minimize this loss = Maximize probability of correct next tokens', \n",
    "            fontsize=11, ha='center', fontweight='bold', color='#27ae60')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_language_modeling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 The Loss Function: Cross-Entropy\n",
    "\n",
    "The paper's objective translates to **cross-entropy loss**:\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{N}\\sum_{i=1}^{N} \\log P(u_i | u_1, ..., u_{i-1})$$\n",
    "\n",
    "In PyTorch, this is `F.cross_entropy(logits, targets)`.\n",
    "\n",
    "### 1.5 Teacher Forcing\n",
    "\n",
    "During training, we use **teacher forcing**:\n",
    "- Feed the **correct** previous tokens (not model predictions)\n",
    "- This allows parallel computation of all positions\n",
    "- The causal mask ensures position $i$ only sees positions $< i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_teacher_forcing():\n",
    "    \"\"\"Show how teacher forcing enables parallel training.\"\"\"\n",
    "    \n",
    "    print(\"Teacher Forcing in GPT Training\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nInput sequence:  ['<s>', 'The', 'cat', 'sat', 'on']\")\n",
    "    print(\"Target sequence: ['The', 'cat', 'sat', 'on', 'the']\")\n",
    "    print()\n",
    "    print(\"At each position, GPT predicts the NEXT token:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Position':<10} {'Input (context)':<25} {'Target (predict)'}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    contexts = [\n",
    "        (\"0\", \"'<s>'\", \"'The'\"),\n",
    "        (\"1\", \"'<s>', 'The'\", \"'cat'\"),\n",
    "        (\"2\", \"'<s>', 'The', 'cat'\", \"'sat'\"),\n",
    "        (\"3\", \"'<s>', 'The', 'cat', 'sat'\", \"'on'\"),\n",
    "        (\"4\", \"'<s>', 'The', 'cat', 'sat', 'on'\", \"'the'\"),\n",
    "    ]\n",
    "    \n",
    "    for pos, ctx, tgt in contexts:\n",
    "        print(f\"{pos:<10} {ctx:<25} {tgt}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Key insight: ALL positions computed in PARALLEL (not sequential)\")\n",
    "    print(\"The causal mask ensures each position only sees past tokens.\")\n",
    "\n",
    "demonstrate_teacher_forcing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Training Procedure\n",
    "\n",
    "### 2.1 Dataset: BooksCorpus\n",
    "\n",
    "From Section 4.1:\n",
    "\n",
    "> *\"For pre-training the model, we use the BooksCorpus dataset. It contains over 7,000 unique unpublished books from a variety of genres including Adventure, Fantasy, and Romance.\"*\n",
    "\n",
    "Why BooksCorpus?\n",
    "- **Long contiguous text**: Books have coherent narratives spanning many pages\n",
    "- **Diverse topics**: Fiction covers many domains and writing styles\n",
    "- **~1 billion words**: Enough data to train 117M parameters\n",
    "\n",
    "### 2.2 Tokenization: BPE\n",
    "\n",
    "From Section 4.1:\n",
    "\n",
    "> *\"We used a bytepair encoding (BPE) vocabulary with 40,000 merges.\"*\n",
    "\n",
    "BPE (Byte Pair Encoding) is a subword tokenization method:\n",
    "- Starts with character-level vocabulary\n",
    "- Iteratively merges most frequent pairs\n",
    "- Balances vocabulary size with handling rare words\n",
    "\n",
    "### 2.3 Training Hyperparameters\n",
    "\n",
    "From Section 4.1:\n",
    "\n",
    "> *\"We trained for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens. Since layernorm is used extensively throughout the model, a simple weight initialization of N(0, 0.02) was sufficient.\"*\n",
    "\n",
    "> *\"We used the Adam optimization scheme with a max learning rate of 2.5e-4. The learning rate was increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule.\"*\n",
    "\n",
    "Let's extract all the details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"Model configuration from paper.\"\"\"\n",
    "    vocab_size: int = 40478\n",
    "    n_positions: int = 512\n",
    "    n_embd: int = 768\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_inner: int = 3072\n",
    "    embd_pdrop: float = 0.1\n",
    "    attn_pdrop: float = 0.1\n",
    "    resid_pdrop: float = 0.1\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class TrainingConfig:\n",
    "    \"\"\"\n",
    "    Training configuration - all values from paper Section 4.1.\n",
    "    \"\"\"\n",
    "    # === Batch and Sequence ===\n",
    "    batch_size: int = 64              # \"minibatches of 64\"\n",
    "    seq_length: int = 512             # \"contiguous sequences of 512 tokens\"\n",
    "    \n",
    "    # === Training Duration ===\n",
    "    epochs: int = 100                 # \"trained for 100 epochs\"\n",
    "    \n",
    "    # === Optimizer ===\n",
    "    optimizer: str = \"Adam\"           # \"Adam optimization scheme\"\n",
    "    max_lr: float = 2.5e-4            # \"max learning rate of 2.5e-4\"\n",
    "    \n",
    "    # === Learning Rate Schedule ===\n",
    "    warmup_steps: int = 2000          # \"increased linearly from zero over the first 2000 updates\"\n",
    "    schedule: str = \"cosine\"          # \"annealed to 0 using a cosine schedule\"\n",
    "    \n",
    "    # === Regularization ===\n",
    "    weight_decay: float = 0.01        # Standard for Adam/AdamW\n",
    "    \n",
    "    # === Initialization ===\n",
    "    init_std: float = 0.02            # \"weight initialization of N(0, 0.02)\"\n",
    "\n",
    "\n",
    "train_config = TrainingConfig()\n",
    "\n",
    "print(\"GPT Training Configuration (from paper Section 4.1)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n[Data]\")\n",
    "print(f\"  Dataset:        BooksCorpus (~1B words)\")\n",
    "print(f\"  Tokenization:   BPE with 40,000 merges\")\n",
    "print(f\"  Batch size:     {train_config.batch_size}\")\n",
    "print(f\"  Sequence length:{train_config.seq_length}\")\n",
    "print(f\"\\n[Training]\")\n",
    "print(f\"  Epochs:         {train_config.epochs}\")\n",
    "print(f\"  Optimizer:      {train_config.optimizer}\")\n",
    "print(f\"  Max LR:         {train_config.max_lr}\")\n",
    "print(f\"\\n[LR Schedule]\")\n",
    "print(f\"  Warmup steps:   {train_config.warmup_steps}\")\n",
    "print(f\"  Schedule:       Linear warmup + {train_config.schedule} decay\")\n",
    "print(f\"\\n[Initialization]\")\n",
    "print(f\"  Weight init:    N(0, {train_config.init_std})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Learning Rate Schedule\n",
    "\n",
    "The paper specifies a **warmup + cosine annealing** schedule:\n",
    "\n",
    "1. **Warmup phase** (steps 0 to 2000): Linear increase from 0 to max_lr\n",
    "2. **Cosine decay** (steps 2000 to end): Cosine annealing from max_lr to 0\n",
    "\n",
    "$$\\text{lr}(t) = \\begin{cases} \n",
    "\\text{max\\_lr} \\cdot \\frac{t}{\\text{warmup}} & \\text{if } t < \\text{warmup} \\\\\n",
    "\\text{max\\_lr} \\cdot \\frac{1}{2}\\left(1 + \\cos\\left(\\pi \\cdot \\frac{t - \\text{warmup}}{T - \\text{warmup}}\\right)\\right) & \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_scheduler(optimizer, warmup_steps: int, total_steps: int):\n",
    "    \"\"\"\n",
    "    Create the learning rate scheduler from the paper:\n",
    "    - Linear warmup for first 2000 steps\n",
    "    - Cosine annealing to 0 after warmup\n",
    "    \"\"\"\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            # Linear warmup\n",
    "            return step / warmup_steps\n",
    "        else:\n",
    "            # Cosine annealing\n",
    "            progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "            return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    \n",
    "    return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "def visualize_lr_schedule():\n",
    "    \"\"\"Visualize the learning rate schedule from the paper.\"\"\"\n",
    "    \n",
    "    # Simulate training\n",
    "    total_steps = 100000  # Approximate steps for 100 epochs\n",
    "    warmup_steps = 2000\n",
    "    max_lr = 2.5e-4\n",
    "    \n",
    "    # Create dummy optimizer\n",
    "    dummy_param = torch.nn.Parameter(torch.zeros(1))\n",
    "    optimizer = AdamW([dummy_param], lr=max_lr)\n",
    "    scheduler = get_lr_scheduler(optimizer, warmup_steps, total_steps)\n",
    "    \n",
    "    # Collect LR values\n",
    "    lrs = []\n",
    "    steps = list(range(0, total_steps, 100))\n",
    "    for step in steps:\n",
    "        scheduler.last_epoch = step - 1\n",
    "        lr = scheduler.get_last_lr()[0]\n",
    "        lrs.append(lr)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Full schedule\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(steps, lrs, 'b-', linewidth=2)\n",
    "    ax1.axvline(x=warmup_steps, color='r', linestyle='--', linewidth=1.5, label='End of warmup')\n",
    "    ax1.axhline(y=max_lr, color='g', linestyle=':', linewidth=1.5, label=f'Max LR = {max_lr}')\n",
    "    ax1.fill_between([0, warmup_steps], 0, max_lr, alpha=0.2, color='red', label='Warmup phase')\n",
    "    ax1.set_xlabel('Training Step', fontsize=12)\n",
    "    ax1.set_ylabel('Learning Rate', fontsize=12)\n",
    "    ax1.set_title('GPT Learning Rate Schedule\\n\"Linear warmup + cosine annealing\"', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(0, total_steps)\n",
    "    ax1.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    \n",
    "    # Zoom on warmup\n",
    "    ax2 = axes[1]\n",
    "    warmup_steps_plot = [s for s in steps if s <= 5000]\n",
    "    warmup_lrs = lrs[:len(warmup_steps_plot)]\n",
    "    ax2.plot(warmup_steps_plot, warmup_lrs, 'b-', linewidth=2)\n",
    "    ax2.axvline(x=warmup_steps, color='r', linestyle='--', linewidth=1.5, label='End of warmup (step 2000)')\n",
    "    ax2.axhline(y=max_lr, color='g', linestyle=':', linewidth=1.5)\n",
    "    ax2.fill_between([0, warmup_steps], 0, max_lr, alpha=0.2, color='red')\n",
    "    ax2.set_xlabel('Training Step', fontsize=12)\n",
    "    ax2.set_ylabel('Learning Rate', fontsize=12)\n",
    "    ax2.set_title('Zoom: Warmup Phase\\n\"Increased linearly from zero over first 2000 updates\"', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xlim(0, 5000)\n",
    "    ax2.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Why this schedule?\")\n",
    "    print(\"  - Warmup: Prevents early training instability (large gradients at start)\")\n",
    "    print(\"  - Cosine: Smooth decay allows fine-tuning towards end of training\")\n",
    "    print(\"  - Now standard in most transformer training\")\n",
    "\n",
    "visualize_lr_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 The Adam Optimizer\n",
    "\n",
    "From the paper:\n",
    "\n",
    "> *\"We used the Adam optimization scheme\"*\n",
    "\n",
    "Adam (Adaptive Moment Estimation) maintains:\n",
    "- First moment estimate (momentum)\n",
    "- Second moment estimate (adaptive learning rates)\n",
    "\n",
    "The update rule:\n",
    "\n",
    "$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$$\n",
    "$$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$$\n",
    "$$\\theta_t = \\theta_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$\n",
    "\n",
    "Standard hyperparameters:\n",
    "- $\\beta_1 = 0.9$\n",
    "- $\\beta_2 = 0.999$\n",
    "- $\\epsilon = 10^{-8}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Implementation: Complete Training Loop\n",
    "\n",
    "Let's implement the full training procedure. First, we need our model from Part II:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Model components from Part II ===\n",
    "\n",
    "config = GPTConfig()\n",
    "\n",
    "def gelu_approx(x):\n",
    "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, n_embd, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(n_embd))\n",
    "        self.beta = nn.Parameter(torch.zeros(n_embd))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        return self.gamma * (x - mean) / torch.sqrt(var + self.eps) + self.beta\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = config.n_embd // config.n_head\n",
    "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
    "        \n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        \n",
    "        mask = torch.tril(torch.ones(config.n_positions, config.n_positions))\n",
    "        self.register_buffer('mask', mask.view(1, 1, config.n_positions, config.n_positions))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        \n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        \n",
    "        out = (attn @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.resid_dropout(self.c_proj(out))\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, config.n_inner)\n",
    "        self.c_proj = nn.Linear(config.n_inner, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(gelu_approx(self.c_fc(x))))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.wte.weight  # Weight tying\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"GPT initialized: {n_params:,} parameters\")\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids, targets=None):\n",
    "        B, T = input_ids.shape\n",
    "        \n",
    "        tok_emb = self.wte(input_ids)\n",
    "        pos_emb = self.wpe(torch.arange(T, device=input_ids.device))\n",
    "        x = self.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    GPT Trainer implementing the paper's training procedure.\n",
    "    \n",
    "    From Section 4.1:\n",
    "    - Adam optimizer with max LR 2.5e-4\n",
    "    - Linear warmup for 2000 steps\n",
    "    - Cosine annealing to 0\n",
    "    - Batch size 64, sequence length 512\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_config):\n",
    "        self.model = model\n",
    "        self.config = train_config\n",
    "        \n",
    "        # Optimizer (from paper: \"Adam optimization scheme\")\n",
    "        self.optimizer = AdamW(\n",
    "            model.parameters(),\n",
    "            lr=train_config.max_lr,\n",
    "            weight_decay=train_config.weight_decay,\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "        \n",
    "        self.step = 0\n",
    "        self.losses = []\n",
    "    \n",
    "    def get_lr(self, step, total_steps):\n",
    "        \"\"\"Get learning rate for current step.\"\"\"\n",
    "        warmup = self.config.warmup_steps\n",
    "        max_lr = self.config.max_lr\n",
    "        \n",
    "        if step < warmup:\n",
    "            # Linear warmup\n",
    "            return max_lr * step / warmup\n",
    "        else:\n",
    "            # Cosine annealing\n",
    "            progress = (step - warmup) / (total_steps - warmup)\n",
    "            return max_lr * 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    \n",
    "    def train_step(self, input_ids, targets, total_steps):\n",
    "        \"\"\"Single training step.\"\"\"\n",
    "        # Update learning rate\n",
    "        lr = self.get_lr(self.step, total_steps)\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        # Forward pass\n",
    "        self.model.train()\n",
    "        logits, loss = self.model(input_ids, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (common practice)\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.step += 1\n",
    "        self.losses.append(loss.item())\n",
    "        \n",
    "        return loss.item(), lr\n",
    "\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(model, train_config)\n",
    "print(\"\\nTrainer initialized with paper's hyperparameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_training(model, trainer, num_steps=100):\n",
    "    \"\"\"\n",
    "    Demonstrate training on synthetic data.\n",
    "    In practice, you'd use BooksCorpus.\n",
    "    \"\"\"\n",
    "    print(\"Training Demo (synthetic data)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Running {num_steps} steps to demonstrate training loop...\")\n",
    "    print()\n",
    "    \n",
    "    total_steps = num_steps\n",
    "    batch_size = 8  # Smaller for demo\n",
    "    seq_len = 64    # Smaller for demo\n",
    "    \n",
    "    losses = []\n",
    "    lrs = []\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Generate synthetic batch\n",
    "        input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n",
    "        # For language modeling, targets are inputs shifted by 1\n",
    "        targets = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n",
    "        \n",
    "        loss, lr = trainer.train_step(input_ids, targets, total_steps)\n",
    "        losses.append(loss)\n",
    "        lrs.append(lr)\n",
    "        \n",
    "        if (step + 1) % 20 == 0:\n",
    "            print(f\"Step {step+1:4d} | Loss: {loss:.4f} | LR: {lr:.2e}\")\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(losses, 'b-', linewidth=1.5, alpha=0.7)\n",
    "    ax1.set_xlabel('Step', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.set_title('Training Loss', fontsize=13, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(lrs, 'r-', linewidth=2)\n",
    "    ax2.set_xlabel('Step', fontsize=12)\n",
    "    ax2.set_ylabel('Learning Rate', fontsize=12)\n",
    "    ax2.set_title('Learning Rate Schedule', fontsize=13, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nFinal loss: {losses[-1]:.4f}\")\n",
    "    print(f\"Expected random loss: {math.log(config.vocab_size):.4f}\")\n",
    "    print(\"\\nNote: With real data and more steps, loss would decrease significantly.\")\n",
    "\n",
    "demo_training(model, trainer, num_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Text Generation\n",
    "\n",
    "### 4.1 Autoregressive Generation\n",
    "\n",
    "Once trained, GPT generates text **autoregressively**:\n",
    "1. Start with a prompt\n",
    "2. Predict the next token\n",
    "3. Append the predicted token to the sequence\n",
    "4. Repeat until done\n",
    "\n",
    "### 4.2 Sampling Strategies\n",
    "\n",
    "The model outputs a probability distribution over the vocabulary. How do we select the next token?\n",
    "\n",
    "| Strategy | Description | Pros | Cons |\n",
    "|----------|-------------|------|------|\n",
    "| **Greedy** | Always pick highest prob | Fast, deterministic | Repetitive, boring |\n",
    "| **Temperature** | Scale logits before softmax | Controls randomness | Can be incoherent |\n",
    "| **Top-k** | Sample from k highest probs | Balances diversity | Fixed k may not work for all distributions |\n",
    "| **Top-p (nucleus)** | Sample from smallest set summing to p | Adaptive | Slightly more compute |\n",
    "\n",
    "### 4.3 Temperature Scaling\n",
    "\n",
    "$$P(x_i) = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}$$\n",
    "\n",
    "Where $T$ is temperature:\n",
    "- $T = 1$: Original distribution\n",
    "- $T < 1$: More peaked (more confident)\n",
    "- $T > 1$: More uniform (more random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sampling_strategies():\n",
    "    \"\"\"Visualize different sampling strategies.\"\"\"\n",
    "    \n",
    "    # Simulated logits for 10 tokens\n",
    "    np.random.seed(42)\n",
    "    logits = np.array([2.5, 1.8, 1.2, 0.8, 0.5, 0.3, 0.1, -0.2, -0.5, -1.0])\n",
    "    token_names = ['the', 'a', 'cat', 'dog', 'sat', 'ran', 'big', 'on', 'in', 'of']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Original probabilities\n",
    "    probs = np.exp(logits) / np.exp(logits).sum()\n",
    "    \n",
    "    ax1 = axes[0, 0]\n",
    "    bars = ax1.bar(token_names, probs, color='#3498db', edgecolor='black')\n",
    "    bars[0].set_color('#e74c3c')  # Highest\n",
    "    ax1.set_ylabel('Probability', fontsize=11)\n",
    "    ax1.set_title('Original Distribution (T=1)\\nGreedy selects \"the\" (highest)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylim(0, 0.5)\n",
    "    for i, p in enumerate(probs):\n",
    "        ax1.text(i, p + 0.01, f'{p:.2f}', ha='center', fontsize=9)\n",
    "    \n",
    "    # Low temperature\n",
    "    T_low = 0.5\n",
    "    probs_low = np.exp(logits / T_low) / np.exp(logits / T_low).sum()\n",
    "    \n",
    "    ax2 = axes[0, 1]\n",
    "    bars = ax2.bar(token_names, probs_low, color='#3498db', edgecolor='black')\n",
    "    bars[0].set_color('#e74c3c')\n",
    "    ax2.set_ylabel('Probability', fontsize=11)\n",
    "    ax2.set_title(f'Low Temperature (T={T_low})\\nMore confident, less diverse', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylim(0, 0.8)\n",
    "    for i, p in enumerate(probs_low):\n",
    "        if p > 0.01:\n",
    "            ax2.text(i, p + 0.01, f'{p:.2f}', ha='center', fontsize=9)\n",
    "    \n",
    "    # High temperature\n",
    "    T_high = 2.0\n",
    "    probs_high = np.exp(logits / T_high) / np.exp(logits / T_high).sum()\n",
    "    \n",
    "    ax3 = axes[1, 0]\n",
    "    bars = ax3.bar(token_names, probs_high, color='#3498db', edgecolor='black')\n",
    "    ax3.set_ylabel('Probability', fontsize=11)\n",
    "    ax3.set_title(f'High Temperature (T={T_high})\\nMore uniform, more random', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylim(0, 0.3)\n",
    "    for i, p in enumerate(probs_high):\n",
    "        ax3.text(i, p + 0.005, f'{p:.2f}', ha='center', fontsize=9)\n",
    "    \n",
    "    # Top-k sampling\n",
    "    k = 3\n",
    "    top_k_probs = probs.copy()\n",
    "    top_k_probs[k:] = 0\n",
    "    top_k_probs = top_k_probs / top_k_probs.sum()  # Renormalize\n",
    "    \n",
    "    ax4 = axes[1, 1]\n",
    "    colors = ['#2ecc71' if i < k else '#ecf0f1' for i in range(len(probs))]\n",
    "    bars = ax4.bar(token_names, top_k_probs, color=colors, edgecolor='black')\n",
    "    ax4.set_ylabel('Probability', fontsize=11)\n",
    "    ax4.set_title(f'Top-k Sampling (k={k})\\nOnly sample from top {k} tokens', fontsize=12, fontweight='bold')\n",
    "    ax4.set_ylim(0, 0.6)\n",
    "    for i, p in enumerate(top_k_probs):\n",
    "        if p > 0.01:\n",
    "            ax4.text(i, p + 0.01, f'{p:.2f}', ha='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_sampling_strategies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model,\n",
    "    input_ids: torch.Tensor,\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: Optional[int] = None,\n",
    "    top_p: Optional[float] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Autoregressive text generation.\n",
    "    \n",
    "    Args:\n",
    "        model: The GPT model\n",
    "        input_ids: Starting tokens, shape (batch, seq_len)\n",
    "        max_new_tokens: How many tokens to generate\n",
    "        temperature: Sampling temperature (1.0 = normal)\n",
    "        top_k: If set, only sample from top k tokens\n",
    "        top_p: If set, use nucleus sampling\n",
    "    \n",
    "    Returns:\n",
    "        Generated token IDs including the input\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    generated = input_ids.clone()\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop to max sequence length if needed\n",
    "        idx_cond = generated if generated.size(1) <= model.config.n_positions else generated[:, -model.config.n_positions:]\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, _ = model(idx_cond)\n",
    "        \n",
    "        # Get logits for the last position\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        # Apply top-k filtering\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = float('-inf')\n",
    "        \n",
    "        # Apply top-p (nucleus) filtering\n",
    "        if top_p is not None:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            \n",
    "            # Remove tokens with cumulative probability above threshold\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            \n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "            logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        # Sample\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Append\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "\n",
    "print(\"Generation function defined.\")\n",
    "print(\"\\nSupported sampling methods:\")\n",
    "print(\"  - temperature: Controls randomness\")\n",
    "print(\"  - top_k: Sample from top k tokens\")\n",
    "print(\"  - top_p: Nucleus sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_generation(model):\n",
    "    \"\"\"\n",
    "    Demonstrate generation with different settings.\n",
    "    Note: Model is untrained, so outputs will be random.\n",
    "    \"\"\"\n",
    "    print(\"Generation Demo (untrained model)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Note: Model is untrained, outputs will be random tokens.\")\n",
    "    print(\"With a trained model, this would produce coherent text.\")\n",
    "    print()\n",
    "    \n",
    "    # Start with a \"prompt\" (just random tokens for demo)\n",
    "    prompt = torch.randint(0, 100, (1, 5))  # Use low token IDs\n",
    "    \n",
    "    settings = [\n",
    "        (\"Greedy (temperature=0.001)\", {\"temperature\": 0.001}),\n",
    "        (\"Normal (temperature=1.0)\", {\"temperature\": 1.0}),\n",
    "        (\"Creative (temperature=1.5)\", {\"temperature\": 1.5}),\n",
    "        (\"Top-k (k=10)\", {\"top_k\": 10}),\n",
    "        (\"Top-p (p=0.9)\", {\"top_p\": 0.9}),\n",
    "    ]\n",
    "    \n",
    "    for name, kwargs in settings:\n",
    "        output = generate(model, prompt, max_new_tokens=10, **kwargs)\n",
    "        generated_tokens = output[0, 5:].tolist()  # Only new tokens\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Token IDs: {generated_tokens}\")\n",
    "        print()\n",
    "\n",
    "demo_generation(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Visualizing the Generation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_generation_process():\n",
    "    \"\"\"Visualize step-by-step autoregressive generation.\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "    ax.set_xlim(0, 16)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    ax.text(8, 9.5, 'Autoregressive Generation: Step by Step', \n",
    "            fontsize=16, fontweight='bold', ha='center')\n",
    "    \n",
    "    # Define the steps\n",
    "    steps = [\n",
    "        (\"Step 1: Start with prompt\", ['The', 'cat'], None, None),\n",
    "        (\"Step 2: Predict next token\", ['The', 'cat'], 'sat', ['sat: 0.35', 'ran: 0.20', 'is: 0.15', '...']),\n",
    "        (\"Step 3: Append and continue\", ['The', 'cat', 'sat'], 'on', ['on: 0.40', 'down: 0.25', '...']),\n",
    "        (\"Step 4: Continue...\", ['The', 'cat', 'sat', 'on'], 'the', ['the: 0.50', 'a: 0.20', '...']),\n",
    "        (\"Step 5: Final\", ['The', 'cat', 'sat', 'on', 'the', 'mat'], None, None),\n",
    "    ]\n",
    "    \n",
    "    for i, (title, tokens, next_tok, probs) in enumerate(steps):\n",
    "        y = 8 - i * 1.6\n",
    "        \n",
    "        # Step title\n",
    "        ax.text(0.5, y + 0.3, title, fontsize=11, fontweight='bold')\n",
    "        \n",
    "        # Tokens\n",
    "        for j, tok in enumerate(tokens):\n",
    "            x = 1 + j * 1.3\n",
    "            color = '#3498db' if tok != next_tok else '#2ecc71'\n",
    "            rect = FancyBboxPatch((x-0.4, y-0.3), 1.0, 0.5, boxstyle=\"round,pad=0.02\",\n",
    "                                  facecolor=color, edgecolor='black', linewidth=1.5)\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x + 0.1, y - 0.05, tok, ha='center', va='center', fontsize=10, \n",
    "                   color='white', fontweight='bold')\n",
    "        \n",
    "        # Next token prediction\n",
    "        if next_tok:\n",
    "            x = 1 + len(tokens) * 1.3\n",
    "            rect = FancyBboxPatch((x-0.4, y-0.3), 1.0, 0.5, boxstyle=\"round,pad=0.02\",\n",
    "                                  facecolor='#e74c3c', edgecolor='black', linewidth=1.5)\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x + 0.1, y - 0.05, next_tok, ha='center', va='center', fontsize=10, \n",
    "                   color='white', fontweight='bold')\n",
    "            \n",
    "            # Arrow\n",
    "            ax.annotate('', xy=(x - 0.5, y - 0.05), xytext=(x - 0.8, y - 0.05),\n",
    "                        arrowprops=dict(arrowstyle='->', color='#e74c3c', lw=2))\n",
    "        \n",
    "        # Probability distribution\n",
    "        if probs:\n",
    "            prob_text = '  '.join(probs)\n",
    "            ax.text(10, y - 0.05, f'P(next): {prob_text}', fontsize=9, \n",
    "                   style='italic', color='gray')\n",
    "    \n",
    "    # Legend\n",
    "    ax.text(1, 0.5, 'Legend:', fontsize=11, fontweight='bold')\n",
    "    for i, (color, label) in enumerate([('#3498db', 'Context'), ('#e74c3c', 'Predicted'), ('#2ecc71', 'Just added')]):\n",
    "        x = 3 + i * 3\n",
    "        rect = Rectangle((x, 0.3), 0.5, 0.4, facecolor=color, edgecolor='black')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x + 0.7, 0.5, label, fontsize=10, va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_generation_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Perplexity: Measuring Language Model Quality\n",
    "\n",
    "### 5.1 What is Perplexity?\n",
    "\n",
    "Perplexity is the standard metric for language models. It measures how \"surprised\" the model is by the test data:\n",
    "\n",
    "$$\\text{PPL} = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^{N} \\log P(u_i | u_1, ..., u_{i-1})\\right) = \\exp(\\mathcal{L})$$\n",
    "\n",
    "Where $\\mathcal{L}$ is the cross-entropy loss.\n",
    "\n",
    "### 5.2 Intuition\n",
    "\n",
    "- **Lower perplexity = Better model**\n",
    "- Perplexity of $k$ means the model is \"as confused as if choosing uniformly among $k$ options\"\n",
    "- Random baseline with vocabulary $V$: $\\text{PPL} = V$ (40,478 for GPT)\n",
    "\n",
    "### 5.3 GPT's Results\n",
    "\n",
    "From the paper (Table 1):\n",
    "\n",
    "| Model | BooksCorpus Perplexity |\n",
    "|-------|----------------------|\n",
    "| GPT | **18.4** |\n",
    "\n",
    "This means GPT is \"as confused as choosing among ~18 equally likely words\" - remarkably good given 40,000+ vocabulary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, input_ids, targets):\n",
    "    \"\"\"\n",
    "    Compute perplexity.\n",
    "    \n",
    "    PPL = exp(cross_entropy_loss)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, loss = model(input_ids, targets)\n",
    "    perplexity = torch.exp(loss)\n",
    "    return perplexity.item(), loss.item()\n",
    "\n",
    "\n",
    "# Demo\n",
    "test_input = torch.randint(0, config.vocab_size, (4, 128))\n",
    "test_target = torch.randint(0, config.vocab_size, (4, 128))\n",
    "\n",
    "ppl, loss = compute_perplexity(model, test_input, test_target)\n",
    "\n",
    "print(\"Perplexity Demo\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Loss:       {loss:.4f}\")\n",
    "print(f\"Perplexity: {ppl:.2f}\")\n",
    "print(f\"\\nFor reference:\")\n",
    "print(f\"  Random baseline PPL:  {config.vocab_size} (vocab size)\")\n",
    "print(f\"  GPT paper result:     18.4 (on BooksCorpus)\")\n",
    "print(f\"\\nOur untrained model:    {ppl:.2f} (close to vocab size as expected)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Summary\n",
    "\n",
    "### 6.1 Key Training Details (All from Paper Section 4.1)\n",
    "\n",
    "| Aspect | Value | Paper Quote |\n",
    "|--------|-------|-------------|\n",
    "| **Dataset** | BooksCorpus | \"over 7,000 unique unpublished books\" |\n",
    "| **Tokenization** | BPE | \"40,000 merges\" |\n",
    "| **Batch size** | 64 | \"minibatches of 64\" |\n",
    "| **Sequence length** | 512 | \"contiguous sequences of 512 tokens\" |\n",
    "| **Epochs** | 100 | \"trained for 100 epochs\" |\n",
    "| **Optimizer** | Adam | \"Adam optimization scheme\" |\n",
    "| **Max LR** | 2.5e-4 | \"max learning rate of 2.5e-4\" |\n",
    "| **Warmup** | 2000 steps | \"increased linearly... over first 2000 updates\" |\n",
    "| **Schedule** | Cosine | \"annealed to 0 using a cosine schedule\" |\n",
    "| **Init** | N(0, 0.02) | \"weight initialization of N(0, 0.02)\" |\n",
    "\n",
    "### 6.2 The Language Modeling Objective\n",
    "\n",
    "$$L_1(\\mathcal{U}) = \\sum_i \\log P(u_i | u_{i-k}, ..., u_{i-1}; \\Theta)$$\n",
    "\n",
    "- Predict each token given previous context\n",
    "- Cross-entropy loss in practice\n",
    "- Teacher forcing enables parallel training\n",
    "\n",
    "### 6.3 Generation\n",
    "\n",
    "- Autoregressive: predict one token, append, repeat\n",
    "- Sampling strategies: greedy, temperature, top-k, top-p\n",
    "- Temperature controls diversity vs. coherence tradeoff\n",
    "\n",
    "### 6.4 What's Next\n",
    "\n",
    "**Part IV**: Fine-tuning\n",
    "- Task-specific input transformations\n",
    "- The auxiliary loss trick\n",
    "- Results on downstream tasks\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Radford et al. (2018). [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "2. Kingma & Ba (2014). [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)\n",
    "3. Sennrich et al. (2016). [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909) (BPE)\n",
    "4. Holtzman et al. (2019). [The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751) (Top-p sampling)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
